{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c913dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from itertools import product\n",
    "from scripts.utilities.data_calculator import get_mean_st\n",
    "# from scripts.customized_net.models import BasicBlock, WideResNet_v1, ResNet\n",
    "from scripts.customized_net.nervous_system import brain\n",
    "from scripts.utilities.train_val import train\n",
    "from scripts.utilities.get_dataset import ImageNetDownSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b663548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('book_name = \"' + IPython.notebook.notebook_name+'\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('book_name = \"' + IPython.notebook.notebook_name+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "307aa7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50_deep_resnet_ziyuan-seed1.ipynb\n",
      "Experiment: 1\n"
     ]
    }
   ],
   "source": [
    "print(book_name)\n",
    "\n",
    "experiment_id = int(book_name[book_name.rfind('seed')+4:len(book_name)-6:1]) # Experiment 1 - ResNet replicate\n",
    "print(\"Experiment:\",experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47eb1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np seed max 2**32 - 1\n",
    "# https://numpy.org/doc/stable/reference/random/legacy.html\n",
    "seed_factor = ((2**32 - 1)/60)\n",
    "\n",
    "def set_seeds(seed, experiment_id=None):\n",
    "    if not seed:\n",
    "        seed = 10\n",
    "        \n",
    "    seed = int(seed * experiment_id)\n",
    "\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fd5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e64c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "valid_size = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "valid_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "dir_path = 'data/Imagenet32_train'\n",
    "\n",
    "train_data = ImageNetDownSample(dir_path, train=True, transform=train_transform)\n",
    "valid_data = ImageNetDownSample(dir_path, train=True, transform=valid_transform)\n",
    "test_data = ImageNetDownSample(dir_path, train=False, transform=test_transform)\n",
    "\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler,\n",
    "                          num_workers=num_workers, worker_init_fn=seed_worker)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, sampler=valid_sampler,\n",
    "                          num_workers=num_workers, worker_init_fn=seed_worker)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, \n",
    "                          worker_init_fn=seed_worker, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed62c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_loader.sampler),len(valid_loader.sampler),len(test_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f119d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (data, target) in enumerate(test_loader):\n",
    "#     if i == 1: print(torch.min(data), torch.max(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63154c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean, train_std = get_mean_st(train_loader)\n",
    "# valid_mean, valid_std = get_mean_st(valid_loader)\n",
    "# test_mean, test_std = get_mean_st(test_loader)\n",
    "# print(train_mean, train_std)\n",
    "# print(valid_mean, valid_std)\n",
    "# print(test_mean, test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3634cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4811, 0.4575, 0.4079]) tensor([0.2604, 0.2532, 0.2682])\n",
      "tensor([0.4813, 0.4575, 0.4077]) tensor([0.2607, 0.2534, 0.2682])\n",
      "tensor([0.4730, 0.4502, 0.4011]) tensor([0.2579, 0.2511, 0.2654])\n"
     ]
    }
   ],
   "source": [
    "# tensor([0.4811, 0.4575, 0.4079]) tensor([0.2604, 0.2532, 0.2682])\n",
    "# tensor([0.4813, 0.4575, 0.4077]) tensor([0.2607, 0.2534, 0.2682])\n",
    "# tensor([0.4730, 0.4502, 0.4011]) tensor([0.2579, 0.2511, 0.2654])\n",
    "train_mean, train_std = torch.tensor([0.4811, 0.4575, 0.4079]), torch.tensor([0.2604, 0.2532, 0.2682])\n",
    "valid_mean, valid_std = torch.tensor([0.4813, 0.4575, 0.4077]), torch.tensor([0.2607, 0.2534, 0.2682])\n",
    "test_mean, test_std   = torch.tensor([0.4730, 0.4502, 0.4011]), torch.tensor([0.2579, 0.2511, 0.2654])\n",
    "print(train_mean, train_std)\n",
    "print(valid_mean, valid_std)\n",
    "print(test_mean, test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063a4cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop((32, 32), padding = 4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(valid_mean, valid_std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomCrop((32, 32), padding = 4),\n",
    "    #transforms.RandomRotation(10),\n",
    "    #transforms.ColorJitter(),\n",
    "    #transforms.RandomAffine(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010a3e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  143165576  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_workers = 0\n",
    "valid_size = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "dir_path = 'data/Imagenet32_train'\n",
    "\n",
    "train_data = ImageNetDownSample(dir_path, train=True, transform=train_transform)\n",
    "valid_data = ImageNetDownSample(dir_path, train=True, transform=valid_transform)\n",
    "test_data = ImageNetDownSample(dir_path, train=False, transform=test_transform)\n",
    "\n",
    "##################\n",
    "#### set seed ####\n",
    "##################\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, \n",
    "                               num_workers=num_workers, worker_init_fn=seed_worker)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, sampler=valid_sampler, \n",
    "                               num_workers=num_workers, worker_init_fn=seed_worker)\n",
    "\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, shuffle=True)\n",
    "\n",
    "classes, class_counts = np.unique(train_loader.dataset.train_labels, return_counts=True)\n",
    "class_num = len(classes)\n",
    "class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb87a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "tensor(-1.8340) tensor(2.2566)\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate(test_loader):\n",
    "    if i == 1: \n",
    "        print(data.shape)\n",
    "        print(torch.min(data), torch.max(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c445c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  143165576  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    #img = transforms.Normalize((-train_mean/train_std).tolist(),(1.0/train_std).tolist())(img)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "##################\n",
    "#### set seed ####\n",
    "##################\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "image_iter = iter(train_loader)\n",
    "images, _ = image_iter.next()\n",
    "imshow(torchvision.utils.make_grid(images[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da092f2",
   "metadata": {},
   "source": [
    "- 258458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b446c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nerve Cord 1 - Downsizing Images and Remaining Image Quality\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "   \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        # self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # self.bn1 = norm_layer(self.inplanes)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.stemblock = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\",nn.Conv2d(3, self.inplanes, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False)),\n",
    "            (\"bn1\",norm_layer(self.inplanes)),\n",
    "            (\"relu\",nn.ReLU(inplace=True)),\n",
    "            #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        ]))\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.stemblock(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd7eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [50, 1001]\n",
    "# n = int((layers[0] - 2) / 9)\n",
    "# print(\"n:\",n)\n",
    "# print(\"Total layers:\",9*n+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78f4e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  71582788  ]\n",
      "DeepNet parameters: 25549352\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#### set seed ####\n",
    "##################\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "deep_net = ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "#num_ftrs = deep_net.fc.in_features\n",
    "#deep_net.fc = nn.Linear(num_ftrs, len(classes))\n",
    "deep_net_total_params = sum(p.numel() for p in deep_net.parameters())\n",
    "print(\"DeepNet parameters:\",deep_net_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4f7bf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total convolution layers: 48\n",
      "Total ResNet layers: 50\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 128, 32, 32])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 512, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 512, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 512, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 128, 16, 16])\n",
      "torch.Size([2, 512, 16, 16])\n",
      "torch.Size([2, 256, 16, 16])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 256, 8, 8])\n",
      "torch.Size([2, 1024, 8, 8])\n",
      "torch.Size([2, 512, 8, 8])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 2048, 4, 4])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 2048, 4, 4])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 2048, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(deep_net.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter+=1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter+=1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "\n",
    "outputs = []\n",
    "names = []\n",
    "image = torch.randn(2, 64, 32, 32)\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(\"Total ResNet layers:\",len(outputs)+2)\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    #pass\n",
    "    print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "061ea252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Layer 1 --- layer1.0.conv1\n",
      "Conv Layer 2 --- layer1.0.conv2\n",
      "Conv Layer 3 --- layer1.0.conv3\n",
      "Conv Layer 4 --- layer1.1.conv1\n",
      "Conv Layer 5 --- layer1.1.conv2\n",
      "Conv Layer 6 --- layer1.1.conv3\n",
      "Conv Layer 7 --- layer1.2.conv1\n",
      "Conv Layer 8 --- layer1.2.conv2\n",
      "Conv Layer 9 --- layer1.2.conv3\n",
      "Conv Layer 10 --- layer2.0.conv1\n",
      "Conv Layer 11 --- layer2.0.conv2\n",
      "Conv Layer 12 --- layer2.0.conv3\n",
      "Conv Layer 13 --- layer2.1.conv1\n",
      "Conv Layer 14 --- layer2.1.conv2\n",
      "Conv Layer 15 --- layer2.1.conv3\n",
      "Conv Layer 16 --- layer2.2.conv1\n",
      "Conv Layer 17 --- layer2.2.conv2\n",
      "Conv Layer 18 --- layer2.2.conv3\n",
      "Conv Layer 19 --- layer2.3.conv1\n",
      "Conv Layer 20 --- layer2.3.conv2\n",
      "Conv Layer 21 --- layer2.3.conv3\n",
      "Conv Layer 22 --- layer3.0.conv1\n",
      "Conv Layer 23 --- layer3.0.conv2\n",
      "Conv Layer 24 --- layer3.0.conv3\n",
      "Conv Layer 25 --- layer3.1.conv1\n",
      "Conv Layer 26 --- layer3.1.conv2\n",
      "Conv Layer 27 --- layer3.1.conv3\n",
      "Conv Layer 28 --- layer3.2.conv1\n",
      "Conv Layer 29 --- layer3.2.conv2\n",
      "Conv Layer 30 --- layer3.2.conv3\n",
      "Conv Layer 31 --- layer3.3.conv1\n",
      "Conv Layer 32 --- layer3.3.conv2\n",
      "Conv Layer 33 --- layer3.3.conv3\n",
      "Conv Layer 34 --- layer3.4.conv1\n",
      "Conv Layer 35 --- layer3.4.conv2\n",
      "Conv Layer 36 --- layer3.4.conv3\n",
      "Conv Layer 37 --- layer3.5.conv1\n",
      "Conv Layer 38 --- layer3.5.conv2\n",
      "Conv Layer 39 --- layer3.5.conv3\n",
      "Conv Layer 40 --- layer4.0.conv1\n",
      "Conv Layer 41 --- layer4.0.conv2\n",
      "Conv Layer 42 --- layer4.0.conv3\n",
      "Conv Layer 43 --- layer4.1.conv1\n",
      "Conv Layer 44 --- layer4.1.conv2\n",
      "Conv Layer 45 --- layer4.1.conv3\n",
      "Conv Layer 46 --- layer4.2.conv1\n",
      "Conv Layer 47 --- layer4.2.conv2\n",
      "Conv Layer 48 --- layer4.2.conv3\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for i, j in enumerate(deep_net.state_dict()):\n",
    "    if ('layer') in j:\n",
    "        if ('conv') in j:\n",
    "            l+=1 \n",
    "            print(\"Conv Layer\",l,\"---\",j.replace(\".weight\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68934bb8",
   "metadata": {},
   "source": [
    "from pprint import pprint\n",
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(wide_net.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter+=1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter+=1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "pprint(conv_layers)\n",
    "\n",
    "outputs = []\n",
    "names = []\n",
    "image = torch.randn(2, 16, 32, 32)\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(len(outputs))\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ab1e4",
   "metadata": {},
   "source": [
    "n = 0\n",
    "for name, param in wide_net.named_parameters():\n",
    "    if \"conv\" in name or \"fc.w\" in name or \"fc1.\" in name:\n",
    "        n+=1\n",
    "        print(f\"Layer {n}:\",name, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7976ae",
   "metadata": {},
   "source": [
    "n = 0\n",
    "for name, param in wide_net.named_parameters():\n",
    "    if \"conv\" in name or \"fc.w\" in name or \"fc1.\" in name:\n",
    "        n+=1\n",
    "        print(f\"Layer {n}:\",name, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f9ed9",
   "metadata": {},
   "source": [
    "wide_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e1f13",
   "metadata": {},
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(wide_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f624490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (stemblock): Sequential(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e66dee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_ids = [d for d in range(torch.cuda.device_count())]\n",
    "\n",
    "device = f'cuda:{device_ids[0]}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "deep_net = deep_net.to(device)\n",
    "\n",
    "#if device == 'cuda:0':\n",
    "#    deep_net = nn.DataParallel(deep_net, device_ids)\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89f5eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4 # 1e-4 # 5e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "##################\n",
    "#### set seed ####\n",
    "##################\n",
    "#set_seeds(seed_factor, experiment_id)\n",
    "deep_optimizer = optim.SGD(deep_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay) # 1e-4\n",
    "deep_scheduler = optim.lr_scheduler.CosineAnnealingLR(deep_optimizer, T_max=100, eta_min=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29a3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_name(model):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_name = model.module.__class__.__name__\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3783fff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-897e724d7b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%20\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pretrained/model_cifar_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnb_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdeep_net_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnet_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep_net_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deep_net' is not defined"
     ]
    }
   ],
   "source": [
    "nb_name = book_name\n",
    "nb_name = os.path.splitext(nb_name.replace(\" \",\"\").replace(\"-\",\"_\").replace(\"%20\",\"\").lower())[0]\n",
    "weights_path = 'pretrained/model_cifar_'+nb_name\n",
    "deep_net_weights = weights_path+\"_\"+net_name(deep_net)+'.pt'\n",
    "weights_path, deep_net_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b3de86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained exists\n",
      "start_epoch: 0\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(Path(weights_path).parent) == False:\n",
    "    os.makedirs(Path(weights_path).parent)\n",
    "else:\n",
    "    print(Path(weights_path).parent, \"exists\")\n",
    "\n",
    "load_model = ''\n",
    "if load_model == \"deep\":\n",
    "    checkpoint = torch.load(deep_net_weights)\n",
    "    deep_net.load_state_dict(checkpoint['net'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "elif load_model == \"wide\":\n",
    "    checkpoint = torch.load(wide_net_weights)\n",
    "    deep_net.load_state_dict(checkpoint['net'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "print('start_epoch: %s' % start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d0dec89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_name(deep_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f4550e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153051, 128116, 50000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.sampler),len(valid_loader.sampler),len(test_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03028dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.02736979166667, 10.0090625)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1153051/128/300, 128116/128/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1064e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "best_acc_cord1 = 0\n",
    "best_acc_cord2 = 0\n",
    "model_list = []\n",
    "\n",
    "def net_name(model):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_name = model.module.__class__.__name__\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "    return model_name\n",
    "        \n",
    "def train(epoch, model, pre_trained_weights, train_loader, valid_loader, optimizer, criterion, scheduler, device):\n",
    "\n",
    "    global best_acc_cord1, best_acc_cord2, model_list\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_name = net_name(model)\n",
    "    \n",
    "    if model_name not in model_name:\n",
    "        model_name.append(model_name)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch+1} on {model_name}')\n",
    "\n",
    "    criterion.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_name = model.module.__class__.__name__\n",
    "    else:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "    train_loss, train_loss_list = 0, []\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    print(\"------Train------\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        # print(outputs.max(1))\n",
    "        # print(torch.max(outputs, 1))\n",
    "        \n",
    "        _, predicted = outputs.max(1) # _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        train_total += targets.size(0)\n",
    "        train_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # print(predicted.eq(targets).sum().item())\n",
    "        # print((predicted == targets).sum().item())\n",
    "\n",
    "        if batch_idx % 300 == 299:    # print every 30 mini-batches\n",
    "            train_acc = train_correct/train_total\n",
    "            print('[%d, %5d] Train Loss: %.6f |  Train Acc: %.3f%% (%d/%d)' %\n",
    "                  (epoch + 1, batch_idx + 1, train_loss, 100.*train_acc, train_correct, train_total))\n",
    "    \n",
    "    # train_loss /= len(train_loader.sampler)\n",
    "\n",
    "    train_acc = 100.*train_correct/train_total\n",
    "\n",
    "    print('Epoch Train Loss: %.6f' % train_loss)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss, valid_loss_list = 0, []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    print(\"------Validation------\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valid_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            valid_loss_list.append(loss.item())\n",
    "            \n",
    "            # print(outputs.max(1))\n",
    "            # print(torch.max(outputs, 1))\n",
    "\n",
    "            _, predicted = outputs.max(1) # _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # print(predicted.eq(targets).sum().item())\n",
    "            # print((predicted == targets).sum().item())\n",
    "            val_acc = val_correct/val_total\n",
    "            if batch_idx % 100 == 99:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] Val Loss: %.6f |  Val Acc: %.3f%% (%d/%d)' %\n",
    "                      (epoch + 1, batch_idx + 1, val_loss, 100.*val_acc, val_correct, val_total))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    # val_loss /= len(valid_loader.sampler)\n",
    "    val_acc = 100.*val_correct/val_total\n",
    "    \n",
    "    if model_name not in model_list:\n",
    "        model_list.append(model_name)\n",
    "\n",
    "    state = {\n",
    "        'net': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "\n",
    "    if len(model_list) == 1:\n",
    "        print(\"Single-Model Mode\")\n",
    "        if val_acc > best_acc_cord1:\n",
    "            if model_list.index(model_name) == 0:\n",
    "                print(f'Validation accuracy have improved from {best_acc_cord1:.4f} to {val_acc:.4f},',\n",
    "                      f'saving the new {model_list[0]} model...')\n",
    "                best_acc_cord1 = val_acc\n",
    "                torch.save(state, pre_trained_weights)\n",
    "        print('Epoch Valid Loss: %.6f' % val_loss,\n",
    "              f\"| Best Global Accuracy {max(best_acc_cord1,best_acc_cord2):.4f}\",\n",
    "              f\"| Best {model_list[0]} Accuracy {best_acc_cord1:.4f}\")\n",
    "    else:\n",
    "        print(\"Multi-Model Mode\")\n",
    "        if val_acc > best_acc_cord1:\n",
    "            if model_list.index(model_name) == 0:\n",
    "                print(f'Validation accuracy have improved from {best_acc_cord1:.4f} to {val_acc:.4f},',\n",
    "                      f'saving the new {model_list[0]} model...')\n",
    "                best_acc_cord1 = val_acc\n",
    "                torch.save(state, pre_trained_weights)\n",
    "        print('Epoch Valid Loss: %.6f' % val_loss, \n",
    "              f\"| Best Global Accuracy {max(best_acc_cord1,best_acc_cord2):.4f}\",\n",
    "              f\"| Best {model_list[0]} Accuracy {best_acc_cord1:.4f}\")\n",
    "        if val_acc > best_acc_cord2:\n",
    "            if model_list.index(model_name) == 1:\n",
    "                print(f'Validation accuracy have improved from {best_acc_cord2:.4f} to {val_acc:.4f},',\n",
    "                      f'saving the new {model_list[1]} model...')\n",
    "                best_acc_cord2 = val_acc\n",
    "                torch.save(state, pre_trained_weights)\n",
    "        print('Epoch Valid Loss: %.6f' % val_loss, \n",
    "              f\"| Best Global Accuracy {max(best_acc_cord1,best_acc_cord2):.4f}\",\n",
    "              f\"| Best {model_list[1]} Accuracy {best_acc_cord2:.4f}\")\n",
    "                \n",
    "\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed per Epoch: {elapsed:.2f} min')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss, train_acc, train_loss_list, val_loss, val_acc, valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e34c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  143165576  ]\n",
      "\n",
      "Aggregated Epoch: 1\n",
      "\n",
      "Epoch: 1 on ResNet\n",
      "------Train------\n",
      "[1,   300] Train Loss: 2123.983156 |  Train Acc: 0.133% (51/38400)\n",
      "[1,   600] Train Loss: 4182.302071 |  Train Acc: 0.151% (116/76800)\n",
      "[1,   900] Train Loss: 6215.555280 |  Train Acc: 0.191% (220/115200)\n",
      "[1,  1200] Train Loss: 8227.197458 |  Train Acc: 0.242% (371/153600)\n",
      "[1,  1500] Train Loss: 10212.909453 |  Train Acc: 0.306% (587/192000)\n",
      "[1,  1800] Train Loss: 12171.533085 |  Train Acc: 0.383% (883/230400)\n",
      "[1,  2100] Train Loss: 14102.712073 |  Train Acc: 0.472% (1270/268800)\n",
      "[1,  2400] Train Loss: 16004.095046 |  Train Acc: 0.563% (1730/307200)\n",
      "[1,  2700] Train Loss: 17871.849343 |  Train Acc: 0.653% (2257/345600)\n",
      "[1,  3000] Train Loss: 19702.292276 |  Train Acc: 0.770% (2957/384000)\n",
      "[1,  3300] Train Loss: 21489.256155 |  Train Acc: 0.920% (3887/422400)\n",
      "[1,  3600] Train Loss: 23235.843031 |  Train Acc: 1.087% (5008/460800)\n",
      "[1,  3900] Train Loss: 24947.684061 |  Train Acc: 1.274% (6360/499200)\n",
      "[1,  4200] Train Loss: 26628.647999 |  Train Acc: 1.474% (7922/537600)\n",
      "[1,  4500] Train Loss: 28280.552988 |  Train Acc: 1.679% (9671/576000)\n",
      "[1,  4800] Train Loss: 29900.085544 |  Train Acc: 1.897% (11657/614400)\n",
      "[1,  5100] Train Loss: 31500.788542 |  Train Acc: 2.129% (13895/652800)\n",
      "[1,  5400] Train Loss: 33087.733246 |  Train Acc: 2.340% (16173/691200)\n",
      "[1,  5700] Train Loss: 34654.290530 |  Train Acc: 2.555% (18642/729600)\n",
      "[1,  6000] Train Loss: 36209.243589 |  Train Acc: 2.769% (21265/768000)\n",
      "[1,  6300] Train Loss: 37744.628840 |  Train Acc: 2.991% (24116/806400)\n",
      "[1,  6600] Train Loss: 39267.748755 |  Train Acc: 3.214% (27152/844800)\n",
      "[1,  6900] Train Loss: 40780.254350 |  Train Acc: 3.419% (30198/883200)\n",
      "[1,  7200] Train Loss: 42284.848516 |  Train Acc: 3.627% (33426/921600)\n",
      "[1,  7500] Train Loss: 43776.673087 |  Train Acc: 3.833% (36795/960000)\n",
      "[1,  7800] Train Loss: 45262.965715 |  Train Acc: 4.023% (40168/998400)\n",
      "[1,  8100] Train Loss: 46737.726435 |  Train Acc: 4.219% (43740/1036800)\n",
      "[1,  8400] Train Loss: 48202.044709 |  Train Acc: 4.411% (47426/1075200)\n",
      "[1,  8700] Train Loss: 49661.826899 |  Train Acc: 4.595% (51175/1113600)\n",
      "[1,  9000] Train Loss: 51112.591814 |  Train Acc: 4.782% (55093/1152000)\n",
      "Epoch Train Loss: 51156.518857\n",
      "------Validation------\n",
      "[1,   100] Val Loss: 496.475774 |  Val Acc: 8.562% (1096/12800)\n",
      "[1,   200] Val Loss: 994.092018 |  Val Acc: 8.660% (2217/25600)\n",
      "[1,   300] Val Loss: 1488.922601 |  Val Acc: 8.878% (3409/38400)\n",
      "[1,   400] Val Loss: 1985.642325 |  Val Acc: 8.932% (4573/51200)\n",
      "[1,   500] Val Loss: 2483.305267 |  Val Acc: 8.958% (5733/64000)\n",
      "[1,   600] Val Loss: 2978.796812 |  Val Acc: 8.996% (6909/76800)\n",
      "[1,   700] Val Loss: 3478.883958 |  Val Acc: 8.915% (7988/89600)\n",
      "[1,   800] Val Loss: 3974.921585 |  Val Acc: 8.935% (9149/102400)\n",
      "[1,   900] Val Loss: 4473.700171 |  Val Acc: 8.910% (10264/115200)\n",
      "[1,  1000] Val Loss: 4973.005648 |  Val Acc: 8.881% (11368/128000)\n",
      "\n",
      "\n",
      "Single-Model Mode\n",
      "Validation accuracy have improved from 0.0000 to 8.8818, saving the new ResNet model...\n",
      "Epoch Valid Loss: 4978.041795 | Best Global Accuracy 8.8818 | Best ResNet Accuracy 8.8818\n",
      "Time elapsed per Epoch: 38.22 min\n",
      "Adjusting learning rate of group 0 to 9.9975e-02.\n",
      "\n",
      "Aggregated Epoch: 2\n",
      "\n",
      "Epoch: 2 on ResNet\n",
      "------Train------\n",
      "[2,   300] Train Loss: 1440.103399 |  Train Acc: 10.318% (3962/38400)\n",
      "[2,   600] Train Loss: 2871.362207 |  Train Acc: 10.605% (8145/76800)\n",
      "[2,   900] Train Loss: 4300.983202 |  Train Acc: 10.563% (12169/115200)\n",
      "[2,  1200] Train Loss: 5717.894042 |  Train Acc: 10.701% (16437/153600)\n",
      "[2,  1500] Train Loss: 7132.611554 |  Train Acc: 10.803% (20742/192000)\n",
      "[2,  1800] Train Loss: 8538.448792 |  Train Acc: 10.913% (25144/230400)\n",
      "[2,  2100] Train Loss: 9944.083204 |  Train Acc: 11.036% (29666/268800)\n",
      "[2,  2400] Train Loss: 11337.909946 |  Train Acc: 11.171% (34316/307200)\n",
      "[2,  2700] Train Loss: 12725.032052 |  Train Acc: 11.253% (38889/345600)\n",
      "[2,  3000] Train Loss: 14109.507648 |  Train Acc: 11.378% (43693/384000)\n",
      "[2,  3300] Train Loss: 15490.261533 |  Train Acc: 11.487% (48522/422400)\n",
      "[2,  3600] Train Loss: 16865.244445 |  Train Acc: 11.589% (53404/460800)\n",
      "[2,  3900] Train Loss: 18241.675295 |  Train Acc: 11.691% (58360/499200)\n",
      "[2,  4200] Train Loss: 19612.178746 |  Train Acc: 11.773% (63294/537600)\n",
      "[2,  4500] Train Loss: 20976.372819 |  Train Acc: 11.868% (68358/576000)\n",
      "[2,  4800] Train Loss: 22335.914050 |  Train Acc: 11.956% (73460/614400)\n",
      "[2,  5100] Train Loss: 23693.163872 |  Train Acc: 12.044% (78623/652800)\n",
      "[2,  5400] Train Loss: 25054.018241 |  Train Acc: 12.100% (83634/691200)\n",
      "[2,  5700] Train Loss: 26403.935009 |  Train Acc: 12.187% (88913/729600)\n",
      "[2,  6000] Train Loss: 27751.730326 |  Train Acc: 12.266% (94205/768000)\n",
      "[2,  6300] Train Loss: 29094.535030 |  Train Acc: 12.362% (99687/806400)\n",
      "[2,  6600] Train Loss: 30436.102342 |  Train Acc: 12.440% (105094/844800)\n",
      "[2,  6900] Train Loss: 31776.042675 |  Train Acc: 12.511% (110498/883200)\n",
      "[2,  7200] Train Loss: 33115.723533 |  Train Acc: 12.570% (115843/921600)\n",
      "[2,  7500] Train Loss: 34450.628758 |  Train Acc: 12.634% (121282/960000)\n",
      "[2,  7800] Train Loss: 35779.202724 |  Train Acc: 12.703% (126831/998400)\n",
      "[2,  8100] Train Loss: 37111.268721 |  Train Acc: 12.766% (132354/1036800)\n",
      "[2,  8400] Train Loss: 38438.212073 |  Train Acc: 12.832% (137966/1075200)\n",
      "[2,  8700] Train Loss: 39759.691240 |  Train Acc: 12.904% (143697/1113600)\n",
      "[2,  9000] Train Loss: 41084.527991 |  Train Acc: 12.968% (149387/1152000)\n",
      "Epoch Train Loss: 41124.196296\n",
      "------Validation------\n",
      "[2,   100] Val Loss: 515.440779 |  Val Acc: 9.008% (1153/12800)\n",
      "[2,   200] Val Loss: 1026.351963 |  Val Acc: 9.121% (2335/25600)\n",
      "[2,   300] Val Loss: 1538.216670 |  Val Acc: 9.068% (3482/38400)\n",
      "[2,   400] Val Loss: 2045.075413 |  Val Acc: 9.162% (4691/51200)\n",
      "[2,   500] Val Loss: 2551.533287 |  Val Acc: 9.272% (5934/64000)\n",
      "[2,   600] Val Loss: 3063.289623 |  Val Acc: 9.145% (7023/76800)\n",
      "[2,   700] Val Loss: 3574.941785 |  Val Acc: 9.122% (8173/89600)\n",
      "[2,   800] Val Loss: 4085.303995 |  Val Acc: 9.104% (9322/102400)\n",
      "[2,   900] Val Loss: 4593.662865 |  Val Acc: 9.142% (10532/115200)\n",
      "[2,  1000] Val Loss: 5105.261104 |  Val Acc: 9.133% (11690/128000)\n",
      "\n",
      "\n",
      "Single-Model Mode\n",
      "Validation accuracy have improved from 8.8818 to 9.1339, saving the new ResNet model...\n",
      "Epoch Valid Loss: 5110.599751 | Best Global Accuracy 9.1339 | Best ResNet Accuracy 9.1339\n",
      "Time elapsed per Epoch: 38.23 min\n",
      "Adjusting learning rate of group 0 to 9.9901e-02.\n",
      "\n",
      "Aggregated Epoch: 3\n",
      "\n",
      "Epoch: 3 on ResNet\n",
      "------Train------\n",
      "[3,   300] Train Loss: 1323.053386 |  Train Acc: 14.656% (5628/38400)\n",
      "[3,   600] Train Loss: 2636.682280 |  Train Acc: 14.811% (11375/76800)\n",
      "[3,   900] Train Loss: 3953.413407 |  Train Acc: 14.844% (17100/115200)\n",
      "[3,  1200] Train Loss: 5264.611496 |  Train Acc: 14.872% (22843/153600)\n",
      "[3,  1500] Train Loss: 6576.333678 |  Train Acc: 14.949% (28703/192000)\n",
      "[3,  1800] Train Loss: 7887.854183 |  Train Acc: 14.995% (34549/230400)\n",
      "[3,  2100] Train Loss: 9196.927600 |  Train Acc: 15.055% (40467/268800)\n",
      "[3,  2400] Train Loss: 10501.644446 |  Train Acc: 15.101% (46391/307200)\n",
      "[3,  2700] Train Loss: 11802.631373 |  Train Acc: 15.167% (52418/345600)\n",
      "[3,  3000] Train Loss: 13105.446136 |  Train Acc: 15.196% (58354/384000)\n",
      "[3,  3300] Train Loss: 14402.463979 |  Train Acc: 15.252% (64423/422400)\n",
      "[3,  3600] Train Loss: 15705.224988 |  Train Acc: 15.272% (70375/460800)\n",
      "[3,  3900] Train Loss: 17006.783958 |  Train Acc: 15.310% (76427/499200)\n",
      "[3,  4200] Train Loss: 18305.211130 |  Train Acc: 15.351% (82525/537600)\n",
      "[3,  4500] Train Loss: 19603.901601 |  Train Acc: 15.388% (88636/576000)\n",
      "[3,  4800] Train Loss: 20905.034693 |  Train Acc: 15.398% (94606/614400)\n",
      "[3,  5100] Train Loss: 22198.061610 |  Train Acc: 15.420% (100659/652800)\n",
      "[3,  5400] Train Loss: 23493.066210 |  Train Acc: 15.434% (106679/691200)\n",
      "[3,  5700] Train Loss: 24788.842956 |  Train Acc: 15.441% (112654/729600)\n",
      "[3,  6000] Train Loss: 26083.474850 |  Train Acc: 15.466% (118779/768000)\n",
      "[3,  6300] Train Loss: 27374.397255 |  Train Acc: 15.490% (124914/806400)\n",
      "[3,  6600] Train Loss: 28663.262829 |  Train Acc: 15.520% (131117/844800)\n",
      "[3,  6900] Train Loss: 29954.679010 |  Train Acc: 15.543% (137280/883200)\n",
      "[3,  7200] Train Loss: 31237.664855 |  Train Acc: 15.582% (143603/921600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,  7500] Train Loss: 32525.010946 |  Train Acc: 15.596% (149719/960000)\n",
      "[3,  7800] Train Loss: 33814.272142 |  Train Acc: 15.617% (155917/998400)\n",
      "[3,  8100] Train Loss: 35099.845059 |  Train Acc: 15.644% (162194/1036800)\n",
      "[3,  8400] Train Loss: 36382.030806 |  Train Acc: 15.664% (168417/1075200)\n",
      "[3,  8700] Train Loss: 37662.397974 |  Train Acc: 15.695% (174777/1113600)\n",
      "[3,  9000] Train Loss: 38940.831187 |  Train Acc: 15.722% (181120/1152000)\n",
      "Epoch Train Loss: 38979.794506\n",
      "------Validation------\n",
      "[3,   100] Val Loss: 468.451157 |  Val Acc: 13.555% (1735/12800)\n",
      "[3,   200] Val Loss: 932.752264 |  Val Acc: 13.645% (3493/25600)\n",
      "[3,   300] Val Loss: 1397.512235 |  Val Acc: 13.573% (5212/38400)\n",
      "[3,   400] Val Loss: 1862.047996 |  Val Acc: 13.428% (6875/51200)\n",
      "[3,   500] Val Loss: 2331.980824 |  Val Acc: 13.373% (8559/64000)\n",
      "[3,   600] Val Loss: 2796.299761 |  Val Acc: 13.396% (10288/76800)\n",
      "[3,   700] Val Loss: 3263.556885 |  Val Acc: 13.357% (11968/89600)\n",
      "[3,   800] Val Loss: 3730.610107 |  Val Acc: 13.323% (13643/102400)\n",
      "[3,   900] Val Loss: 4191.726705 |  Val Acc: 13.413% (15452/115200)\n",
      "[3,  1000] Val Loss: 4661.861143 |  Val Acc: 13.349% (17087/128000)\n",
      "\n",
      "\n",
      "Single-Model Mode\n",
      "Validation accuracy have improved from 9.1339 to 13.3457, saving the new ResNet model...\n",
      "Epoch Valid Loss: 4666.910632 | Best Global Accuracy 13.3457 | Best ResNet Accuracy 13.3457\n",
      "Time elapsed per Epoch: 38.16 min\n",
      "Adjusting learning rate of group 0 to 9.9778e-02.\n",
      "\n",
      "Aggregated Epoch: 4\n",
      "\n",
      "Epoch: 4 on ResNet\n",
      "------Train------\n",
      "[4,   300] Train Loss: 1273.534316 |  Train Acc: 16.599% (6374/38400)\n",
      "[4,   600] Train Loss: 2549.100545 |  Train Acc: 16.470% (12649/76800)\n",
      "[4,   900] Train Loss: 3823.209391 |  Train Acc: 16.598% (19121/115200)\n",
      "[4,  1200] Train Loss: 5101.885389 |  Train Acc: 16.517% (25370/153600)\n",
      "[4,  1500] Train Loss: 6375.810015 |  Train Acc: 16.534% (31746/192000)\n",
      "[4,  1800] Train Loss: 7649.432520 |  Train Acc: 16.581% (38203/230400)\n",
      "[4,  2100] Train Loss: 8916.752374 |  Train Acc: 16.630% (44701/268800)\n",
      "[4,  2400] Train Loss: 10188.061409 |  Train Acc: 16.653% (51157/307200)\n",
      "[4,  2700] Train Loss: 11461.893044 |  Train Acc: 16.670% (57612/345600)\n",
      "[4,  3000] Train Loss: 12733.951419 |  Train Acc: 16.677% (64041/384000)\n",
      "[4,  3300] Train Loss: 14003.408241 |  Train Acc: 16.696% (70525/422400)\n",
      "[4,  3600] Train Loss: 15277.204911 |  Train Acc: 16.681% (76867/460800)\n",
      "[4,  3900] Train Loss: 16548.822663 |  Train Acc: 16.715% (83443/499200)\n",
      "[4,  4200] Train Loss: 17813.686993 |  Train Acc: 16.735% (89969/537600)\n",
      "[4,  4500] Train Loss: 19082.576510 |  Train Acc: 16.743% (96442/576000)\n",
      "[4,  4800] Train Loss: 20355.661360 |  Train Acc: 16.747% (102891/614400)\n",
      "[4,  5100] Train Loss: 21622.415004 |  Train Acc: 16.779% (109532/652800)\n",
      "[4,  5400] Train Loss: 22890.165364 |  Train Acc: 16.783% (116005/691200)\n",
      "[4,  5700] Train Loss: 24156.915043 |  Train Acc: 16.794% (122527/729600)\n",
      "[4,  6000] Train Loss: 25421.339940 |  Train Acc: 16.814% (129130/768000)\n",
      "[4,  6300] Train Loss: 26689.405484 |  Train Acc: 16.820% (135633/806400)\n",
      "[4,  6600] Train Loss: 27953.615885 |  Train Acc: 16.817% (142074/844800)\n",
      "[4,  6900] Train Loss: 29214.040230 |  Train Acc: 16.834% (148677/883200)\n",
      "[4,  7200] Train Loss: 30474.771526 |  Train Acc: 16.842% (155214/921600)\n",
      "[4,  7500] Train Loss: 31737.851633 |  Train Acc: 16.848% (161740/960000)\n",
      "[4,  7800] Train Loss: 33003.170841 |  Train Acc: 16.859% (168320/998400)\n",
      "[4,  8100] Train Loss: 34268.639941 |  Train Acc: 16.873% (174941/1036800)\n",
      "[4,  8400] Train Loss: 35527.201312 |  Train Acc: 16.883% (181522/1075200)\n",
      "[4,  8700] Train Loss: 36787.979303 |  Train Acc: 16.898% (188172/1113600)\n",
      "[4,  9000] Train Loss: 38055.737247 |  Train Acc: 16.898% (194660/1152000)\n",
      "Epoch Train Loss: 38093.380522\n",
      "------Validation------\n",
      "[4,   100] Val Loss: 454.522840 |  Val Acc: 14.281% (1828/12800)\n",
      "[4,   200] Val Loss: 907.701836 |  Val Acc: 14.164% (3626/25600)\n",
      "[4,   300] Val Loss: 1359.734322 |  Val Acc: 14.326% (5501/38400)\n",
      "[4,   400] Val Loss: 1812.383364 |  Val Acc: 14.551% (7450/51200)\n",
      "[4,   500] Val Loss: 2263.630642 |  Val Acc: 14.627% (9361/64000)\n",
      "[4,   600] Val Loss: 2713.784230 |  Val Acc: 14.633% (11238/76800)\n",
      "[4,   700] Val Loss: 3165.407974 |  Val Acc: 14.594% (13076/89600)\n",
      "[4,   800] Val Loss: 3616.792943 |  Val Acc: 14.589% (14939/102400)\n",
      "[4,   900] Val Loss: 4067.562569 |  Val Acc: 14.603% (16823/115200)\n",
      "[4,  1000] Val Loss: 4521.310719 |  Val Acc: 14.584% (18668/128000)\n",
      "\n",
      "\n",
      "Single-Model Mode\n",
      "Validation accuracy have improved from 13.3457 to 14.5829, saving the new ResNet model...\n",
      "Epoch Valid Loss: 4525.786114 | Best Global Accuracy 14.5829 | Best ResNet Accuracy 14.5829\n",
      "Time elapsed per Epoch: 38.20 min\n",
      "Adjusting learning rate of group 0 to 9.9606e-02.\n",
      "\n",
      "Aggregated Epoch: 5\n",
      "\n",
      "Epoch: 5 on ResNet\n",
      "------Train------\n",
      "[5,   300] Train Loss: 1252.856210 |  Train Acc: 17.482% (6713/38400)\n",
      "[5,   600] Train Loss: 2506.535534 |  Train Acc: 17.677% (13576/76800)\n",
      "[5,   900] Train Loss: 3759.157552 |  Train Acc: 17.688% (20377/115200)\n",
      "[5,  1200] Train Loss: 5013.681418 |  Train Acc: 17.609% (27048/153600)\n",
      "[5,  1500] Train Loss: 6272.369826 |  Train Acc: 17.606% (33803/192000)\n",
      "[5,  1800] Train Loss: 7532.972277 |  Train Acc: 17.548% (40430/230400)\n",
      "[5,  2100] Train Loss: 8788.860509 |  Train Acc: 17.529% (47119/268800)\n",
      "[5,  2400] Train Loss: 10048.350682 |  Train Acc: 17.532% (53857/307200)\n",
      "[5,  2700] Train Loss: 11299.066201 |  Train Acc: 17.542% (60624/345600)\n",
      "[5,  3000] Train Loss: 12553.749265 |  Train Acc: 17.543% (67365/384000)\n",
      "[5,  3300] Train Loss: 13805.757450 |  Train Acc: 17.528% (74040/422400)\n",
      "[5,  3600] Train Loss: 15056.670444 |  Train Acc: 17.543% (80840/460800)\n",
      "[5,  3900] Train Loss: 16313.318202 |  Train Acc: 17.532% (87519/499200)\n",
      "[5,  4200] Train Loss: 17563.817034 |  Train Acc: 17.538% (94285/537600)\n",
      "[5,  4500] Train Loss: 18817.053051 |  Train Acc: 17.546% (101067/576000)\n",
      "[5,  4800] Train Loss: 20069.950669 |  Train Acc: 17.568% (107939/614400)\n",
      "[5,  5100] Train Loss: 21321.756159 |  Train Acc: 17.575% (114732/652800)\n",
      "[5,  5400] Train Loss: 22572.089453 |  Train Acc: 17.593% (121600/691200)\n",
      "[5,  5700] Train Loss: 23826.139364 |  Train Acc: 17.576% (128237/729600)\n",
      "[5,  6000] Train Loss: 25078.207609 |  Train Acc: 17.584% (135046/768000)\n",
      "[5,  6300] Train Loss: 26326.664331 |  Train Acc: 17.590% (141843/806400)\n",
      "[5,  6600] Train Loss: 27577.383179 |  Train Acc: 17.593% (148628/844800)\n",
      "[5,  6900] Train Loss: 28828.013838 |  Train Acc: 17.605% (155485/883200)\n",
      "[5,  7200] Train Loss: 30076.774078 |  Train Acc: 17.613% (162317/921600)\n",
      "[5,  7500] Train Loss: 31329.554999 |  Train Acc: 17.611% (169066/960000)\n",
      "[5,  7800] Train Loss: 32581.541938 |  Train Acc: 17.604% (175754/998400)\n",
      "[5,  8100] Train Loss: 33827.776137 |  Train Acc: 17.619% (182676/1036800)\n",
      "[5,  8400] Train Loss: 35075.321915 |  Train Acc: 17.628% (189531/1075200)\n",
      "[5,  8700] Train Loss: 36324.504665 |  Train Acc: 17.632% (196350/1113600)\n",
      "[5,  9000] Train Loss: 37575.499792 |  Train Acc: 17.636% (203165/1152000)\n",
      "Epoch Train Loss: 37612.876375\n",
      "------Validation------\n",
      "[5,   100] Val Loss: 435.133418 |  Val Acc: 16.094% (2060/12800)\n",
      "[5,   200] Val Loss: 869.964381 |  Val Acc: 16.383% (4194/25600)\n",
      "[5,   300] Val Loss: 1300.384646 |  Val Acc: 16.479% (6328/38400)\n",
      "[5,   400] Val Loss: 1729.988395 |  Val Acc: 16.480% (8438/51200)\n",
      "[5,   500] Val Loss: 2164.361425 |  Val Acc: 16.438% (10520/64000)\n",
      "[5,   600] Val Loss: 2595.317295 |  Val Acc: 16.509% (12679/76800)\n",
      "[5,   700] Val Loss: 3028.681784 |  Val Acc: 16.483% (14769/89600)\n",
      "[5,   800] Val Loss: 3460.694195 |  Val Acc: 16.468% (16863/102400)\n",
      "[5,   900] Val Loss: 3894.229718 |  Val Acc: 16.463% (18965/115200)\n",
      "[5,  1000] Val Loss: 4329.618378 |  Val Acc: 16.461% (21070/128000)\n",
      "\n",
      "\n",
      "Single-Model Mode\n",
      "Validation accuracy have improved from 14.5829 to 16.4585, saving the new ResNet model...\n",
      "Epoch Valid Loss: 4334.596230 | Best Global Accuracy 16.4585 | Best ResNet Accuracy 16.4585\n",
      "Time elapsed per Epoch: 38.12 min\n",
      "Adjusting learning rate of group 0 to 9.9384e-02.\n",
      "\n",
      "Aggregated Epoch: 6\n",
      "\n",
      "Epoch: 6 on ResNet\n",
      "------Train------\n",
      "[6,   300] Train Loss: 1243.147875 |  Train Acc: 17.664% (6783/38400)\n",
      "[6,   600] Train Loss: 2486.448540 |  Train Acc: 17.887% (13737/76800)\n",
      "[6,   900] Train Loss: 3730.559719 |  Train Acc: 17.882% (20600/115200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,  1200] Train Loss: 4973.744289 |  Train Acc: 17.883% (27468/153600)\n",
      "[6,  1500] Train Loss: 6219.806604 |  Train Acc: 17.884% (34338/192000)\n",
      "[6,  1800] Train Loss: 7468.430518 |  Train Acc: 17.868% (41169/230400)\n",
      "[6,  2100] Train Loss: 8718.150114 |  Train Acc: 17.855% (47995/268800)\n",
      "[6,  2400] Train Loss: 9961.708293 |  Train Acc: 17.866% (54883/307200)\n",
      "[6,  2700] Train Loss: 11206.005030 |  Train Acc: 17.861% (61727/345600)\n",
      "[6,  3000] Train Loss: 12448.067912 |  Train Acc: 17.863% (68592/384000)\n",
      "[6,  3300] Train Loss: 13694.326426 |  Train Acc: 17.839% (75351/422400)\n",
      "[6,  3600] Train Loss: 14936.539602 |  Train Acc: 17.849% (82247/460800)\n",
      "[6,  3900] Train Loss: 16176.094074 |  Train Acc: 17.891% (89311/499200)\n",
      "[6,  4200] Train Loss: 17427.829747 |  Train Acc: 17.884% (96146/537600)\n",
      "[6,  4500] Train Loss: 18671.498603 |  Train Acc: 17.892% (103058/576000)\n",
      "[6,  4800] Train Loss: 19914.408027 |  Train Acc: 17.900% (109979/614400)\n",
      "[6,  5100] Train Loss: 21158.981443 |  Train Acc: 17.905% (116884/652800)\n",
      "[6,  5400] Train Loss: 22405.476706 |  Train Acc: 17.904% (123755/691200)\n",
      "[6,  5700] Train Loss: 23649.991230 |  Train Acc: 17.911% (130682/729600)\n",
      "[6,  6000] Train Loss: 24894.084344 |  Train Acc: 17.920% (137626/768000)\n",
      "[6,  6300] Train Loss: 26134.217037 |  Train Acc: 17.934% (144617/806400)\n",
      "[6,  6600] Train Loss: 27378.774136 |  Train Acc: 17.921% (151398/844800)\n",
      "[6,  6900] Train Loss: 28627.910415 |  Train Acc: 17.915% (158225/883200)\n",
      "[6,  7200] Train Loss: 29868.876913 |  Train Acc: 17.925% (165194/921600)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "start_epoch = 0\n",
    "epochs = 100\n",
    "\n",
    "exchange = 10 # network exchange threshold\n",
    "\n",
    "aggregated_epoch = 0\n",
    "\n",
    "deep_output, wide_output = [], []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    if epoch == 0: set_seeds(seed_factor, experiment_id)\n",
    "\n",
    "    aggregated_epoch+=1\n",
    "    print(\"\\nAggregated Epoch:\", aggregated_epoch)\n",
    "    deep_output.append(train(epoch,deep_net,deep_net_weights,train_loader,valid_loader,deep_optimizer,criterion,deep_scheduler,device))\n",
    "\n",
    "elapsed = (time.time() - start_time)/60\n",
    "model_name = f\"Wide ResNet20 Seed {experiment_id}\"\n",
    "print(f'Individual {model_name} Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581cec4",
   "metadata": {},
   "source": [
    "- Epoch Valid Loss: 43.205497 | Best Accuracy 77.38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d41cca",
   "metadata": {},
   "source": [
    "- Best Accuracy 92.64\n",
    "- Best Accuracy 92.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933afc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_net_weights_final = deep_net_weights.replace('.pt', '_final.pt')\n",
    "deep_net_weights_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_state = {\n",
    "    'net': deep_net.state_dict(),\n",
    "    'optimizer': deep_optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'output': deep_output,\n",
    "}\n",
    "\n",
    "torch.save(deep_state, deep_net_weights_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f0649e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_net_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1c77f675d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_net_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_net_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeep_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deep_net_weights' is not defined"
     ]
    }
   ],
   "source": [
    "print(deep_net_weights)\n",
    "checkpoint = torch.load(deep_net_weights)\n",
    "deep_net.load_state_dict(checkpoint['net'])\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomCrop((32, 32), padding = 4),\n",
    "    #transforms.RandomRotation(10),\n",
    "    #transforms.ColorJitter(),\n",
    "    #transforms.RandomAffine(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std),\n",
    "])\n",
    "\n",
    "test_data = datasets.CIFAR100('data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images[:4]\n",
    "labels = labels[:4]\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = deep_net(images.to(device))\n",
    "_, predicted = torch.max(outputs.cpu(), 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "deep_net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = deep_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = deep_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        if batch_size == c.shape[0]:\n",
    "            for i in range(batch_size):\n",
    "                class_correct[labels[i].item()] += c[i].item()\n",
    "                class_total[labels[i].item()] += 1\n",
    "        else:\n",
    "            for i in range(c.shape[0]):\n",
    "                class_correct[labels[i].item()] += c[i].item()\n",
    "                class_total[labels[i].item()] += 1\n",
    "\n",
    "each_class_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    class_acc = 100 * class_correct[i] / class_total[i]\n",
    "    each_class_list.append(class_acc)\n",
    "    print('Accuracy of %5s : %.2f%%' % (\n",
    "        classes[i], class_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd8641",
   "metadata": {},
   "source": [
    "- Accuracy of the network on the 10000 test images: 76.36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(class_total), np.var(each_class_list), np.std(each_class_list), np.mean(each_class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90beb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat,\n",
    "                          hide_spines=False,\n",
    "                          hide_ticks=False,\n",
    "                          figsize=None,\n",
    "                          cmap=None,\n",
    "                          colorbar=False,\n",
    "                          show_absolute=True,\n",
    "                          show_normed=False,\n",
    "                          class_names=None):\n",
    "\n",
    "    if not (show_absolute or show_normed):\n",
    "        raise AssertionError('Both show_absolute and show_normed are False')\n",
    "    if class_names is not None and len(class_names) != len(conf_mat):\n",
    "        raise AssertionError('len(class_names) should be equal to number of'\n",
    "                             'classes in the dataset')\n",
    "\n",
    "    total_samples = conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "    normed_conf_mat = conf_mat.astype('float') / total_samples\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid(False)\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.Blues\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (len(conf_mat)*1.25, len(conf_mat)*1.25)\n",
    "\n",
    "    if show_normed:\n",
    "        matshow = ax.matshow(normed_conf_mat, cmap=cmap)\n",
    "    else:\n",
    "        matshow = ax.matshow(conf_mat, cmap=cmap)\n",
    "\n",
    "    if colorbar:\n",
    "        fig.colorbar(matshow)\n",
    "\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        for j in range(conf_mat.shape[1]):\n",
    "            cell_text = \"\"\n",
    "            if show_absolute:\n",
    "                cell_text += format(conf_mat[i, j], 'd')\n",
    "                if show_normed:\n",
    "                    cell_text += \"\\n\" + '('\n",
    "                    cell_text += format(normed_conf_mat[i, j], '.2f') + ')'\n",
    "            else:\n",
    "                cell_text += format(normed_conf_mat[i, j], '.2f')\n",
    "            ax.text(x=j,\n",
    "                    y=i,\n",
    "                    s=cell_text,\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    color=\"white\" if normed_conf_mat[i, j] > 0.5 else \"black\")\n",
    "    \n",
    "    if class_names is not None:\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names, rotation=90)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        \n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if hide_ticks:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "    return fig, ax\n",
    "\n",
    "def compute_confusion_matrix(model, data_loader, device):\n",
    "\n",
    "    all_targets, all_predictions = [], []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            all_targets.extend(targets.to('cpu'))\n",
    "            all_predictions.extend(predicted_labels.to('cpu'))\n",
    "\n",
    "    all_predictions = all_predictions\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "        \n",
    "    class_labels = np.unique(np.concatenate((all_targets, all_predictions)))\n",
    "    if class_labels.shape[0] == 1:\n",
    "        if class_labels[0] != 0:\n",
    "            class_labels = np.array([0, class_labels[0]])\n",
    "        else:\n",
    "            class_labels = np.array([class_labels[0], 1])\n",
    "    n_labels = class_labels.shape[0]\n",
    "    lst = []\n",
    "    z = list(zip(all_targets, all_predictions))\n",
    "    for combi in product(class_labels, repeat=2):\n",
    "        lst.append(z.count(combi))\n",
    "    mat = np.asarray(lst)[:, None].reshape(n_labels, n_labels)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2class = {v: k for k, v in test_loader.dataset.class_to_idx.items()}\n",
    "idx2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = compute_confusion_matrix(model=deep_net, data_loader=test_loader, device=device)\n",
    "plot_confusion_matrix(mat, class_names=idx2class.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %psource train\n",
    "# train_loss, train_acc, train_loss_list, val_loss, val_acc, valid_loss_list\n",
    "print(deep_net_weights_final)\n",
    "checkpoint = torch.load(deep_net_weights_final)\n",
    "print(checkpoint.keys())\n",
    "deep_output = checkpoint['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21309ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, train_loss_list, val_loss, val_acc, valid_loss_list = [], [], [], [], [], []\n",
    "for idx, d in enumerate(deep_output):\n",
    "    # train_loss, train_acc, train_loss_list, val_loss, val_acc, valid_loss_list\n",
    "    train_loss.append(d[0])\n",
    "    train_acc.append(d[1])\n",
    "    train_loss_list.append(d[2])\n",
    "    val_loss.append(d[3])\n",
    "    val_acc.append(d[4])\n",
    "    valid_loss_list.append(d[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae36ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mini_loss_mean, val_mini_loss_mean = [], []\n",
    "for d, i in enumerate(train_loss_list):\n",
    "    #print(\"batch\", d+1,\":\", np.mean(i))\n",
    "    train_mini_loss_mean.append(np.mean(i))\n",
    "    #print(np.mean(i))\n",
    "for d, i in enumerate(valid_loss_list):\n",
    "    #print(\"batch\", d+1,\":\", np.mean(i))\n",
    "    val_mini_loss_mean.append(np.mean(i))\n",
    "    \n",
    "train_epoch_acc = train_acc\n",
    "valid_epoch_acc = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter( y=train_mini_loss_mean, name=\"Train Loss\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter( y=val_mini_loss_mean, name=\"Validation Loss\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter( y=train_epoch_acc, name=\"Train Accuracy\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter( y=valid_epoch_acc, name=\"Validation Accuracy\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Loss/Accuracy of WideResNet20\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Epoch\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Train/Validation</b> Loss\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Train/Validation</b> Accuracy\", secondary_y=True)\n",
    "fig.update_layout(yaxis_range=[-0.1,5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(train_mini_loss_mean), np.min(train_mini_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "\n",
    "for i in range(0,200):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    wide_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = wide_net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Prediction {i+1} Acc: {acc}\")\n",
    "    test_list.append(acc)\n",
    "\n",
    "def list_mean(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "print(\"Prediction Average\",round(list_mean(test_list),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2c827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torchvision.models.wide_resnet50_2(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879b3ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torchvision.models.resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet50(pretrained=False) # ResNet34().to(device) n = {3, 5, 7, 9}\n",
    "resnet = sum(p.numel() for p in resnet.parameters())\n",
    "print(\"resnet50 parameters:\",resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_resnet = torchvision.models.wide_resnet50_2(pretrained=False) # ResNet34().to(device) n = {3, 5, 7, 9}\n",
    "wide_resnet = sum(p.numel() for p in wide_resnet.parameters())\n",
    "print(\"wide_resnet50_2 parameters:\",wide_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662f266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
