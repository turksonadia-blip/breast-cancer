{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2fba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (IPython.notebook.kernel) {\n",
       "    IPython.notebook.kernel.execute('book_name = \"' + IPython.notebook.notebook_name+'\"')\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "if (IPython.notebook.kernel) {\n",
    "    IPython.notebook.kernel.execute('book_name = \"' + IPython.notebook.notebook_name+'\"')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dcec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bc6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_UNet_seed1.ipynb\n",
      "Experiment: 1\n"
     ]
    }
   ],
   "source": [
    "print(book_name)\n",
    "\n",
    "experiment_id = int(book_name[book_name.rfind('seed')+4:len(book_name)-6:1]) # Experiment 1 - ResNet replicate\n",
    "print(\"Experiment:\",experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09346ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np seed max 2**32 - 1\n",
    "# https://numpy.org/doc/stable/reference/random/legacy.html\n",
    "seed_factor = ((2**32 - 1)/60)\n",
    "\n",
    "def set_seeds(seed, experiment_id=None):\n",
    "    if not seed:\n",
    "        seed = 10\n",
    "        \n",
    "    seed = int(seed * experiment_id)\n",
    "\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20f72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from experiments.UNetExperiment import UNetExperiment\n",
    "from data_prep.HippocampusDatasetLoader import LoadHippocampusData\n",
    "\n",
    "\"\"\"\n",
    "This module represents a UNet experiment and contains a class that handles\n",
    "the experiment lifecycle\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_prep.SlicesDataset import SlicesDataset\n",
    "from utils.utils import log_to_tensorboard\n",
    "from utils.volume_stats import Dice3d, Jaccard3d, Sensitivity, Specificity #, F1_score\n",
    "# from networks.RecursiveUNet import UNet\n",
    "from inference.UNetInferenceAgent import UNetInferenceAgent\n",
    "from torch.nn import init\n",
    "import random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from scipy.ndimage import map_coordinates\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4760eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"basic_unet_\"+\"seed\"+str(experiment_id)\n",
    "        self.root_dir = r\"../../data\"\n",
    "        self.n_epochs = 100 # 10\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = 64\n",
    "        self.patch_size = 64 #64\n",
    "        self.test_results_dir = r\"out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77b884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train = 260\n",
      "Image dimensions:\n",
      "Train:\n",
      "1 : (33, 50, 35)\n",
      "2 : (35, 55, 41)\n",
      "3 : (34, 51, 38)\n",
      "4 : (35, 56, 28)\n",
      "5 : (36, 45, 39)\n",
      "6 : (38, 51, 37)\n",
      "7 : (37, 50, 38)\n",
      "8 : (33, 55, 29)\n",
      "9 : (36, 46, 43)\n",
      "10 : (35, 44, 41)\n",
      "11 : (38, 47, 37)\n",
      "12 : (35, 51, 36)\n",
      "13 : (35, 51, 36)\n",
      "14 : (36, 47, 39)\n",
      "15 : (34, 49, 36)\n",
      "16 : (38, 48, 33)\n",
      "17 : (34, 52, 35)\n",
      "18 : (34, 52, 40)\n",
      "19 : (39, 50, 40)\n",
      "20 : (36, 51, 31)\n",
      "21 : (37, 51, 35)\n",
      "22 : (38, 51, 35)\n",
      "23 : (31, 50, 36)\n",
      "24 : (32, 51, 28)\n",
      "25 : (35, 52, 34)\n",
      "26 : (36, 51, 35)\n",
      "27 : (37, 49, 34)\n",
      "28 : (37, 52, 34)\n",
      "29 : (36, 40, 43)\n",
      "30 : (34, 53, 36)\n",
      "31 : (34, 53, 34)\n",
      "32 : (37, 48, 37)\n",
      "33 : (35, 46, 42)\n",
      "34 : (37, 51, 35)\n",
      "35 : (33, 48, 38)\n",
      "36 : (35, 49, 34)\n",
      "37 : (35, 46, 39)\n",
      "38 : (39, 41, 42)\n",
      "39 : (38, 51, 37)\n",
      "40 : (35, 52, 38)\n",
      "41 : (34, 52, 38)\n",
      "42 : (33, 51, 34)\n",
      "43 : (35, 50, 36)\n",
      "44 : (32, 45, 38)\n",
      "45 : (36, 52, 37)\n",
      "46 : (36, 51, 34)\n",
      "47 : (34, 56, 31)\n",
      "48 : (37, 47, 32)\n",
      "49 : (35, 52, 34)\n",
      "50 : (37, 56, 36)\n",
      "51 : (33, 47, 34)\n",
      "52 : (40, 52, 35)\n",
      "53 : (36, 53, 37)\n",
      "54 : (36, 48, 37)\n",
      "55 : (34, 49, 39)\n",
      "56 : (33, 52, 27)\n",
      "57 : (37, 45, 36)\n",
      "58 : (33, 47, 42)\n",
      "59 : (32, 48, 34)\n",
      "60 : (38, 49, 28)\n",
      "61 : (34, 52, 37)\n",
      "62 : (36, 58, 33)\n",
      "63 : (33, 49, 40)\n",
      "64 : (36, 51, 35)\n",
      "65 : (35, 46, 38)\n",
      "66 : (37, 47, 34)\n",
      "67 : (35, 50, 33)\n",
      "68 : (38, 52, 36)\n",
      "69 : (35, 51, 36)\n",
      "70 : (36, 55, 32)\n",
      "71 : (38, 49, 38)\n",
      "72 : (34, 51, 31)\n",
      "73 : (33, 47, 37)\n",
      "74 : (36, 47, 41)\n",
      "75 : (37, 54, 36)\n",
      "76 : (34, 49, 37)\n",
      "77 : (33, 59, 29)\n",
      "78 : (36, 51, 37)\n",
      "79 : (35, 49, 33)\n",
      "80 : (33, 51, 37)\n",
      "81 : (32, 54, 34)\n",
      "82 : (36, 50, 36)\n",
      "83 : (36, 52, 36)\n",
      "84 : (33, 46, 38)\n",
      "85 : (33, 53, 26)\n",
      "86 : (33, 51, 32)\n",
      "87 : (38, 52, 34)\n",
      "88 : (37, 48, 34)\n",
      "89 : (35, 46, 42)\n",
      "90 : (41, 47, 42)\n",
      "91 : (37, 58, 35)\n",
      "92 : (36, 52, 32)\n",
      "93 : (36, 42, 41)\n",
      "94 : (34, 48, 35)\n",
      "95 : (37, 51, 33)\n",
      "96 : (35, 48, 35)\n",
      "97 : (34, 51, 26)\n",
      "98 : (35, 49, 33)\n",
      "99 : (35, 52, 37)\n",
      "100 : (32, 49, 38)\n",
      "101 : (35, 50, 34)\n",
      "102 : (38, 52, 29)\n",
      "103 : (32, 47, 41)\n",
      "104 : (35, 51, 35)\n",
      "105 : (36, 47, 36)\n",
      "106 : (38, 53, 30)\n",
      "107 : (36, 49, 36)\n",
      "108 : (34, 50, 34)\n",
      "109 : (32, 46, 42)\n",
      "110 : (35, 52, 33)\n",
      "111 : (35, 55, 33)\n",
      "112 : (38, 52, 33)\n",
      "113 : (36, 53, 33)\n",
      "114 : (33, 44, 40)\n",
      "115 : (36, 52, 32)\n",
      "116 : (32, 47, 32)\n",
      "117 : (36, 51, 32)\n",
      "118 : (35, 50, 36)\n",
      "119 : (35, 48, 42)\n",
      "120 : (38, 48, 39)\n",
      "121 : (34, 51, 30)\n",
      "122 : (38, 48, 40)\n",
      "123 : (35, 53, 36)\n",
      "124 : (34, 49, 35)\n",
      "125 : (33, 44, 41)\n",
      "126 : (34, 49, 38)\n",
      "127 : (37, 51, 33)\n",
      "128 : (33, 48, 34)\n",
      "129 : (38, 55, 31)\n",
      "130 : (37, 49, 37)\n",
      "131 : (36, 51, 29)\n",
      "132 : (32, 54, 34)\n",
      "133 : (35, 53, 39)\n",
      "134 : (37, 45, 40)\n",
      "135 : (39, 52, 37)\n",
      "136 : (34, 51, 33)\n",
      "137 : (35, 40, 40)\n",
      "138 : (35, 47, 37)\n",
      "139 : (33, 51, 28)\n",
      "140 : (36, 49, 40)\n",
      "141 : (35, 49, 35)\n",
      "142 : (34, 50, 32)\n",
      "143 : (35, 53, 35)\n",
      "144 : (37, 52, 26)\n",
      "145 : (35, 55, 32)\n",
      "146 : (35, 53, 33)\n",
      "147 : (36, 47, 44)\n",
      "148 : (33, 49, 32)\n",
      "149 : (38, 50, 39)\n",
      "150 : (37, 52, 32)\n",
      "151 : (43, 42, 39)\n",
      "152 : (33, 47, 38)\n",
      "153 : (37, 51, 33)\n",
      "154 : (38, 52, 33)\n",
      "155 : (36, 50, 31)\n",
      "156 : (34, 49, 37)\n",
      "157 : (32, 47, 41)\n",
      "158 : (35, 48, 40)\n",
      "159 : (34, 49, 41)\n",
      "160 : (35, 48, 32)\n",
      "161 : (37, 50, 33)\n",
      "162 : (38, 50, 42)\n",
      "163 : (35, 44, 44)\n",
      "164 : (34, 53, 37)\n",
      "165 : (36, 49, 38)\n",
      "166 : (32, 52, 34)\n",
      "167 : (35, 51, 40)\n",
      "168 : (34, 48, 40)\n",
      "169 : (38, 49, 36)\n",
      "170 : (35, 56, 34)\n",
      "171 : (34, 48, 40)\n",
      "172 : (37, 50, 40)\n",
      "173 : (34, 53, 24)\n",
      "174 : (37, 55, 26)\n",
      "175 : (41, 48, 47)\n",
      "176 : (38, 43, 41)\n",
      "177 : (34, 49, 30)\n",
      "178 : (36, 54, 27)\n",
      "179 : (38, 52, 35)\n",
      "180 : (38, 51, 31)\n",
      "181 : (36, 50, 34)\n",
      "182 : (32, 53, 38)\n",
      "183 : (35, 53, 30)\n",
      "184 : (36, 48, 39)\n",
      "185 : (33, 44, 42)\n",
      "186 : (32, 49, 36)\n",
      "187 : (37, 50, 39)\n",
      "188 : (32, 45, 41)\n",
      "189 : (33, 49, 37)\n",
      "190 : (35, 49, 36)\n",
      "191 : (37, 45, 39)\n",
      "192 : (38, 54, 30)\n",
      "193 : (34, 51, 32)\n",
      "194 : (35, 51, 35)\n",
      "195 : (39, 45, 40)\n",
      "196 : (34, 53, 35)\n",
      "197 : (34, 53, 37)\n",
      "198 : (34, 49, 29)\n",
      "199 : (35, 51, 35)\n",
      "200 : (35, 55, 37)\n",
      "201 : (36, 50, 32)\n",
      "202 : (37, 45, 46)\n",
      "203 : (34, 53, 32)\n",
      "204 : (35, 54, 35)\n",
      "205 : (34, 51, 37)\n",
      "206 : (31, 54, 34)\n",
      "207 : (36, 49, 31)\n",
      "208 : (32, 51, 31)\n",
      "209 : (34, 48, 32)\n",
      "210 : (35, 52, 33)\n",
      "211 : (35, 50, 30)\n",
      "212 : (39, 44, 43)\n",
      "213 : (38, 50, 38)\n",
      "214 : (36, 58, 28)\n",
      "215 : (34, 47, 39)\n",
      "216 : (37, 43, 43)\n",
      "217 : (34, 46, 34)\n",
      "218 : (36, 50, 40)\n",
      "219 : (33, 54, 39)\n",
      "220 : (35, 53, 29)\n",
      "221 : (38, 55, 40)\n",
      "222 : (37, 57, 35)\n",
      "223 : (38, 53, 27)\n",
      "224 : (36, 52, 38)\n",
      "225 : (35, 49, 40)\n",
      "226 : (34, 49, 32)\n",
      "227 : (36, 50, 33)\n",
      "228 : (36, 44, 43)\n",
      "229 : (32, 49, 30)\n",
      "230 : (33, 52, 37)\n",
      "231 : (34, 46, 38)\n",
      "232 : (36, 50, 38)\n",
      "233 : (37, 55, 34)\n",
      "234 : (35, 47, 45)\n",
      "235 : (35, 49, 40)\n",
      "236 : (36, 48, 38)\n",
      "237 : (34, 45, 43)\n",
      "238 : (37, 48, 36)\n",
      "239 : (35, 55, 34)\n",
      "240 : (33, 50, 29)\n",
      "241 : (37, 48, 34)\n",
      "242 : (33, 47, 35)\n",
      "243 : (34, 47, 43)\n",
      "244 : (37, 52, 30)\n",
      "245 : (37, 47, 42)\n",
      "246 : (34, 47, 40)\n",
      "247 : (35, 51, 34)\n",
      "248 : (36, 57, 37)\n",
      "249 : (36, 49, 41)\n",
      "250 : (42, 51, 28)\n",
      "251 : (35, 49, 37)\n",
      "252 : (36, 53, 37)\n",
      "253 : (39, 52, 31)\n",
      "254 : (33, 53, 28)\n",
      "255 : (38, 51, 33)\n",
      "256 : (34, 47, 36)\n",
      "257 : (36, 48, 40)\n",
      "258 : (37, 53, 33)\n",
      "259 : (35, 48, 38)\n",
      "260 : (35, 53, 32)\n",
      "Image Min-Max values: Image=1944.87841796875,0.0 and label=2.0,0.0\n",
      "Number of subclasses =  3\n"
     ]
    }
   ],
   "source": [
    "c = Config()\n",
    "\n",
    "from glob import glob\n",
    "print('num_train = {}'.format(len(glob(c.root_dir+\"/images/*\"))))\n",
    "print('Image dimensions:')\n",
    "print('Train:')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dimensions\n",
    "for idx, img_name in enumerate(glob(c.root_dir+\"/images/*\")):\n",
    "    img = nib.load(img_name)\n",
    "    print(idx+1,\":\",img.shape)\n",
    "\n",
    "# Find number of sub labels\n",
    "\n",
    "# Images\n",
    "img = nib.load(glob(c.root_dir+\"/images/*\")[0]).get_fdata()\n",
    "label = nib.load(glob(c.root_dir+\"/labels/*\")[0]).get_fdata()\n",
    "print('Image Min-Max values: Image={},{} and label={},{}'.format(img.max(), img.min(), label.max(), label.min()))\n",
    "print('Number of subclasses = ', int(label.max())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37947823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "[ Using Seed :  71582788  ]\n",
      "Processed 260 files, total 9198 image slices, total 9198 mask slices\n"
     ]
    }
   ],
   "source": [
    "c = Config()\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "\n",
    "# TASK: LoadHippocampusData is not complete. Go to the implementation and complete it. \n",
    "data = LoadHippocampusData(c.root_dir, y_shape = c.patch_size, z_shape = c.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd50d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  71582788  ]\n",
      "Train Size: 156 ; Validation Size: 52 ; Test Size: 52\n"
     ]
    }
   ],
   "source": [
    "split_idx = np.arange((len(data)))\n",
    "\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "# 60:20:20 split using train_test_split()\n",
    "train, test = train_test_split(split_idx, test_size=0.2, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.25, shuffle=True)\n",
    "\n",
    "print(\"Train Size:\", len(train), \"; Validation Size:\", len(val), \"; Test Size:\", len(test))    \n",
    "split=dict({'train': np.array(train),\n",
    "            'val': np.array(val),\n",
    "            'test': np.array(test)}\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c14fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_parameters(self):\n",
    "    \"\"\"\n",
    "    Saves model parameters to a file in results directory\n",
    "    \"\"\"\n",
    "    path = os.path.join(self.out_dir, \"model.pth\")\n",
    "\n",
    "    torch.save(self.model.state_dict(), path)\n",
    "\n",
    "def load_model_parameters(self, path=''):\n",
    "    \"\"\"\n",
    "    Loads model parameters from a supplied path or a\n",
    "    results directory\n",
    "    \"\"\"\n",
    "    if not path:\n",
    "        model_path = os.path.join(self.out_dir, \"model.pth\")\n",
    "    else:\n",
    "        model_path = path\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        raise Exception(f\"Could not find path {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8cacca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Status: True\n"
     ]
    }
   ],
   "source": [
    "# Do we have CUDA available?\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: No CUDA device is found. This may take significantly longer!\")\n",
    "else:\n",
    "    print(\"GPU Status:\",torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1b14ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out/basic_unet_seed1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = c.n_epochs\n",
    "split = split\n",
    "_time_start = \"\"\n",
    "_time_end = \"\"\n",
    "epoch = 0\n",
    "name = c.name\n",
    "\n",
    "# Create output folders\n",
    "# _{time.strftime(\"%Y_%m_%d_%H%M\", time.gmtime())}\n",
    "dirname = f'{c.name}'\n",
    "out_dir = os.path.join(c.test_results_dir, dirname)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d59f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/hayashimasa/UNet-PyTorch/blob/main/augmentation.py\n",
    "class DoubleHorizontalFlip:\n",
    "    \"\"\"Apply horizontal flips to both image and segmentation mask.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image, mask, weight=None):\n",
    "        p = random.random()\n",
    "        if p < self.p:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        if weight is None:\n",
    "            return image, mask\n",
    "        elif p > self.p:\n",
    "            weight = TF.hflip(weight)\n",
    "        return image, mask, weight\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(p={self.p})' \n",
    "\n",
    "class DoubleElasticTransform:\n",
    "    \"\"\"Based on implimentation on\n",
    "    https://gist.github.com/erniejunior/601cdf56d2b424757de5\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=250, sigma=10, p=0.5, seed=None, randinit=True):\n",
    "        if not seed:\n",
    "            seed = random.randint(1, 100)\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.p = p\n",
    "        self.randinit = randinit\n",
    "\n",
    "    def __call__(self, image, mask, weight=None):\n",
    "        if random.random() < self.p:\n",
    "            if self.randinit:\n",
    "                seed = random.randint(1, 100)\n",
    "                self.random_state = np.random.RandomState(seed)\n",
    "                self.alpha = random.uniform(100, 300)\n",
    "                self.sigma = random.uniform(10, 15)\n",
    "                # print(self.alpha)\n",
    "                # print(self.sigma)\n",
    "\n",
    "            dim = image.shape\n",
    "            dx = self.alpha * gaussian_filter(\n",
    "                (self.random_state.rand(*dim[1:]) * 2 - 1),\n",
    "                self.sigma,\n",
    "                mode=\"constant\",\n",
    "                cval=0\n",
    "            )\n",
    "            dy = self.alpha * gaussian_filter(\n",
    "                (self.random_state.rand(*dim[1:]) * 2 - 1),\n",
    "                self.sigma,\n",
    "                mode=\"constant\",\n",
    "                cval=0\n",
    "            )\n",
    "            image = image.view(*dim[1:]).numpy()\n",
    "            mask = mask.view(*dim[1:]).numpy()\n",
    "            x, y = np.meshgrid(np.arange(dim[1]), np.arange(dim[2]))\n",
    "            indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "            image = map_coordinates(image, indices, order=1)\n",
    "            mask = map_coordinates(mask, indices, order=1)\n",
    "            image, mask = image.reshape(dim), mask.reshape(dim)\n",
    "            image, mask = torch.Tensor(image), torch.Tensor(mask)\n",
    "            if weight is None:\n",
    "                return image, mask\n",
    "            weight = weight.view(*dim[1:]).numpy()\n",
    "            weight = map_coordinates(weight, indices, order=1)\n",
    "            weight = weight.reshape(dim)\n",
    "            weight = torch.Tensor(weight)\n",
    "\n",
    "        return (image, mask) if weight is None else (image, mask, weight)\n",
    "\n",
    "class DoubleCompose(transforms.Compose):\n",
    "\n",
    "    def __call__(self, image, mask, weight=None):\n",
    "        if weight is None:\n",
    "            for t in self.transforms:\n",
    "                image, mask = t(image, mask)\n",
    "            return image, mask\n",
    "        for t in self.transforms:\n",
    "            image, mask, weight = t(image, mask, weight)\n",
    "        return image, mask, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a885d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  71582788  ]\n"
     ]
    }
   ],
   "source": [
    "set_seeds(seed_factor, experiment_id)\n",
    "\n",
    "# reference: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "# https://github.com/hayashimasa/UNet-PyTorch\n",
    "# https://gist.github.com/ernestum/601cdf56d2b424757de5\n",
    "\n",
    "train_transform = DoubleCompose([\n",
    "    # DoubleElasticTransform(),\n",
    "    # DoubleHorizontalFlip(p = 0.25),\n",
    "])\n",
    "\n",
    "valid_transform = DoubleCompose([\n",
    "    # DoubleHorizontalFlip(p = 0.25),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9af1ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = copy.deepcopy(data[split[\"train\"]])\n",
    "val_df   = copy.deepcopy(data[split[\"val\"]])\n",
    "test_df  = copy.deepcopy(data[split[\"test\"]])\n",
    "\n",
    "train_loader = DataLoader(SlicesDataset(train_df, train_transform), \n",
    "                          batch_size=c.batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(SlicesDataset(val_df, valid_transform), \n",
    "                          batch_size=c.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# we will access volumes directly for testing\n",
    "val_data  = val_df\n",
    "test_data = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef88957",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = []\n",
    "val_dc_list = []\n",
    "val_dc_ap_mean_list = []\n",
    "val_jc_list = []\n",
    "val_jc_ap_mean_list = []\n",
    "\n",
    "def train(epoch, model):\n",
    "    \"\"\"\n",
    "    This method is executed once per epoch and takes \n",
    "    care of model weight update cycle\n",
    "    \"\"\"\n",
    "    global val_loss_list, val_dc_list, val_jc_list\n",
    "    \n",
    "    print(f\"\\nTraining epoch {epoch}...\")\n",
    "    model.train()\n",
    "\n",
    "    # Loop over our minibatches\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = batch['images'].to('cuda')\n",
    "        target = batch['segs'].to('cuda')\n",
    "\n",
    "        # prediction\n",
    "        prediction = model(data)\n",
    "\n",
    "        prediction_softmax = F.softmax(prediction, dim=1)\n",
    "        loss = loss_function(prediction, target[:, 0, :, :])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i % 10) == 0:\n",
    "            # Output to console on every 10th batch\n",
    "            print(f\"\\nEpoch: {epoch} Train loss: {loss}, {100*(i+1)/len(train_loader):.1f}% complete\")\n",
    "\n",
    "            counter = 100*epoch + 100*(i/len(train_loader))\n",
    "\n",
    "        print(\".\", end='')\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "    \n",
    "    print(f\"\\nValidating epoch {epoch}...\")\n",
    "\n",
    "    # Turn off gradient accumulation by switching model to \"eval\" mode\n",
    "    loss_list = []\n",
    "    dc_list = []\n",
    "    dc_mean_list = []\n",
    "    jc_list = []\n",
    "    jc_mean_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            data = batch['images'].to('cuda')\n",
    "            target = batch['segs'].to('cuda')\n",
    "\n",
    "            prediction = model(data)\n",
    "            prediction_softmax = F.softmax(prediction, dim=1)\n",
    "            loss = loss_function(prediction, target[:, 0, :, :])\n",
    "            # loss.requires_grad = True\n",
    "            # loss.backward()\n",
    "            print(f\"Batch {i}. Loss {loss}\") # Data shape {data.shape} \n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inference_agent = UNetInferenceAgent(model=model, device=device)\n",
    "        \n",
    "        for i, x in enumerate(val_data):\n",
    "\n",
    "            gt = x[\"seg\"]   # val image ground truth        \n",
    "            ti = x[\"image\"] # val image data\n",
    "\n",
    "            mask3d = np.zeros(ti.shape)\n",
    "            pred = inference_agent.single_volume_inference(ti)\n",
    "            mask3d = np.array(torch.argmax(pred, dim=1))\n",
    "\n",
    "            dc, dc_a, dc_p, _ = Dice3d(mask3d, gt)\n",
    "            dc_list.append(dc)\n",
    "            dc_mean_list.append((dc_a+dc_p)/2)\n",
    "\n",
    "            jc, jc_a, jc_p = Jaccard3d(mask3d, gt)\n",
    "            jc_list.append(jc)\n",
    "            jc_mean_list.append((jc_a+jc_p)/2)\n",
    "            \n",
    "    scheduler.step(np.mean(loss_list))\n",
    "    \n",
    "    #print(\"Epoch DC:\", np.sum(dc_list))\n",
    "    \n",
    "    val_loss_list.append(np.sum(loss_list))\n",
    "    val_dc_list.append(np.sum(dc_list))\n",
    "    val_dc_ap_mean_list.append(np.sum(dc_mean_list))\n",
    "    val_jc_list.append(np.sum(jc_list))\n",
    "    val_jc_ap_mean_list.append(np.sum(jc_mean_list))\n",
    "    \n",
    "    measure_ = \"dc_mean\" # loss\n",
    "    \n",
    "    if measure_ == \"loss\":\n",
    "        if epoch==0:\n",
    "            min_val_lose = np.min(val_loss_list)\n",
    "            max_val_dc = np.max(val_dc_list)\n",
    "        else:\n",
    "            min_val_lose = np.min(val_loss_list[:-1])\n",
    "            max_val_dc = np.max(val_dc_list[:-1])\n",
    "\n",
    "        current_val_loss = np.sum(loss_list)\n",
    "        current_val_dc = np.sum(dc_list)\n",
    "\n",
    "        print(f\"Current Validation Loss {round(current_val_loss,8)} | Minimum Validation Loss {round(min_val_lose,8)}\")\n",
    "        print(f\"Current Validation DC {round(current_val_dc,8)} | Maximum Validation DC {round(max_val_dc,8)}\")\n",
    "\n",
    "        if current_val_loss < min_val_lose:\n",
    "            print(f\"Current Validation Loss improved from {round(min_val_lose,8)} to {round(current_val_loss,8)}\")\n",
    "            path = os.path.join(out_dir, f\"model_epoch{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "    elif measure_ == \"dc\":\n",
    "        if epoch==0:\n",
    "            max_val_dc = np.max(val_dc_list)\n",
    "        else:\n",
    "            max_val_dc = np.max(val_dc_list[:-1])\n",
    "\n",
    "        current_val_dc = np.sum(dc_list)\n",
    "\n",
    "        print(f\"Current Validation DC {round(current_val_dc,8)} | Maximum Validation DC {round(max_val_dc,8)}\")\n",
    "\n",
    "        if current_val_dc > max_val_dc:\n",
    "            print(f\"Current Validation DC improved from {round(max_val_dc,8)} to {round(current_val_dc,8)}\")\n",
    "            path = os.path.join(out_dir, f\"model_epoch{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "    elif measure_ == \"dc_mean\":\n",
    "        if epoch==0:\n",
    "            max_val_dc = np.max(val_dc_ap_mean_list)\n",
    "            max_val_dc_all = np.max(val_dc_list)\n",
    "        else:\n",
    "            max_val_dc = np.max(val_dc_ap_mean_list[:-1])\n",
    "            max_val_dc_all = np.max(val_dc_list[:-1])\n",
    "\n",
    "        current_val_dc = np.sum(dc_mean_list)\n",
    "\n",
    "        print(f\"Current Validation AP Mean DC {round(current_val_dc,8)} | Maximum Validation AP Mean DC {round(max_val_dc,8)}\")\n",
    "        print(f\"Maximum Validation DC {round(max_val_dc_all,8)}\")\n",
    "        if current_val_dc > max_val_dc:\n",
    "            print(f\"Current Validation AP Mean DC improved from {round(max_val_dc,8)} to {round(current_val_dc,8)}\")\n",
    "            path = os.path.join(out_dir, f\"model_epoch{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "    elif measure_ == \"jc\":\n",
    "        if epoch==0:\n",
    "            max_val_jc = np.max(val_jc_list)\n",
    "        else:\n",
    "            max_val_jc = np.max(val_jc_list[:-1])\n",
    "\n",
    "        current_val_jc = np.sum(jc_list)\n",
    "\n",
    "        print(f\"Current Validation JC {round(current_val_jc,8)} | Maximum Validation JC {round(max_val_jc,8)}\")\n",
    "\n",
    "        if current_val_jc > max_val_jc:\n",
    "            print(f\"Current Validation JC improved from {round(max_val_jc,8)} to {round(current_val_jc,8)}\")\n",
    "            path = os.path.join(out_dir, f\"model_epoch{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "    print(\"\\nValidation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c7ba771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model):\n",
    "    \"\"\"\n",
    "    This runs test cycle on the test dataset.\n",
    "    Note that process and evaluations are quite different\n",
    "    Here we are computing a lot more metrics and returning\n",
    "    a dictionary that could later be persisted as JSON\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inference_agent = UNetInferenceAgent(model=model, device=device)\n",
    "\n",
    "    out_dict = {}\n",
    "    out_dict[\"volume_stats\"] = []\n",
    "    \n",
    "    dc_list = []\n",
    "    dc_anterior_list = []\n",
    "    dc_posterior_list = []\n",
    "    \n",
    "    jc_list = []\n",
    "    jc_anterior_list = []\n",
    "    jc_posterior_list = []\n",
    "    \n",
    "    sens_list = []\n",
    "    sens_anterior_list = []\n",
    "    sens_posterior_list = []\n",
    "    \n",
    "    spec_list = []\n",
    "    spec_anterior_list = []\n",
    "    spec_posterior_list = []\n",
    "    # f1_list = []\n",
    "\n",
    "    # for every in test set\n",
    "    for i, x in enumerate(test_data):\n",
    "\n",
    "        gt = x[\"seg\"]   # test image ground truth        \n",
    "        ti = x[\"image\"] # test image data\n",
    "        original_filename = x['filename'] # test image file name\n",
    "        pred_filename = 'predicted_'+x['filename'] # test image file name\n",
    "\n",
    "        file_path = os.path.join(\"../../data\",\"images\",original_filename)\n",
    "\n",
    "        original_images = nib.load(file_path)\n",
    "\n",
    "        mask3d = np.zeros(ti.shape)\n",
    "        pred = inference_agent.single_volume_inference(ti)\n",
    "        mask3d = np.array(torch.argmax(pred, dim=1))\n",
    "\n",
    "        # Save predicted labels to local environment for further verification \n",
    "        # with the original image NIFTI coordinate system\n",
    "        pred_coord = nib.Nifti1Image(mask3d, original_images.affine, dtype=np.int16)    \n",
    "\n",
    "        pred_out_path = os.path.join(\"\",\"preds\",\"seed\"+str(experiment_id))\n",
    "        pred_out_file = os.path.join(pred_out_path,pred_filename)\n",
    "\n",
    "        if not os.path.exists(pred_out_path):\n",
    "            os.makedirs(pred_out_path)\n",
    "\n",
    "        nib.save(pred_coord, pred_out_file)\n",
    "\n",
    "        dc, dc_a, dc_p, _ = Dice3d(mask3d, gt)\n",
    "        dc_list.append(dc)\n",
    "        dc_anterior_list.append(dc_a)\n",
    "        dc_posterior_list.append(dc_p)\n",
    "\n",
    "        jc, jc_a, jc_p = Jaccard3d(mask3d, gt)\n",
    "        jc_list.append(jc)\n",
    "        jc_anterior_list.append(jc_a)\n",
    "        jc_posterior_list.append(jc_p)\n",
    "\n",
    "        sens, sens_a, sens_p = Sensitivity(mask3d, gt)\n",
    "        sens_list.append(sens)\n",
    "        sens_anterior_list.append(sens_a)\n",
    "        sens_posterior_list.append(sens_p)\n",
    "\n",
    "        spec, spec_a, spec_p = Specificity(mask3d, gt)\n",
    "        spec_list.append(spec)\n",
    "        spec_anterior_list.append(spec_a)\n",
    "        spec_posterior_list.append(spec_p)\n",
    "\n",
    "        # f1 = F1_score(mask3d, gt)\n",
    "        # f1_list.append(f1)\n",
    "\n",
    "        out_dict[\"volume_stats\"].append({\n",
    "            \"filename\": x['filename'],\n",
    "            \n",
    "            \"dice\": dc,\n",
    "            \"dice_anterior\": dc_a,\n",
    "            \"dice_posterior\": dc_p,\n",
    "            \n",
    "            \"jaccard\": jc,\n",
    "            \"jaccard_anterior\": jc_a,\n",
    "            \"jaccard_posterior\": jc_p,\n",
    "            \n",
    "            \"sensitivity\": sens,\n",
    "            \"sensitivity_anterior\": sens_a,\n",
    "            \"sensitivity_posterior\": sens_p,\n",
    "            \n",
    "            \"specificity\": spec,\n",
    "            \"specificity_anterior\": spec_a,\n",
    "            \"specificity_posterior\": spec_p,\n",
    "            # \"f1\": f1,\n",
    "            })\n",
    "\n",
    "        print(f\"{x['filename']} Dice {dc:.4f}, Dice Anterior {dc_a:.4f}, Dice Posterior {dc_p:.4f}, Jaccard {jc:.4f}, Sensitivity {sens:.4f}, and Specificity {spec:.4f}. {100*(i+1)/len(test_data):.2f}% complete\")\n",
    "\n",
    "    avg_dc = np.mean(dc_list)\n",
    "    avg_dc_a = np.mean(dc_anterior_list)\n",
    "    avg_dc_p = np.mean(dc_posterior_list)\n",
    "\n",
    "    avg_jc = np.mean(jc_list)\n",
    "    avg_jc_a = np.mean(jc_anterior_list)\n",
    "    avg_jc_p = np.mean(jc_posterior_list)\n",
    "    \n",
    "    avg_sens = np.mean(sens_list)\n",
    "    avg_sens_a = np.mean(sens_anterior_list)\n",
    "    avg_sens_p = np.mean(sens_posterior_list)\n",
    "    \n",
    "    avg_spec = np.mean(spec_list)\n",
    "    avg_spec_a = np.mean(spec_anterior_list)\n",
    "    avg_spec_p = np.mean(spec_posterior_list)\n",
    "    \n",
    "    # avg_f1 = np.mean(f1_list)\n",
    "\n",
    "    out_dict[\"overall\"] = {\n",
    "        \"mean_dice\": avg_dc,\n",
    "        \"mean_dice_anterior\": avg_dc_a,\n",
    "        \"mean_dice_posterior\": avg_dc_p,\n",
    "        \"dice_ap_mean\": np.mean([avg_dc_a,avg_dc_p]),\n",
    "        \n",
    "        \"mean_jaccard\": avg_jc,\n",
    "        \"mean_jaccard_anterior\": avg_jc_a,\n",
    "        \"mean_jaccard_posterior\": avg_jc_p,\n",
    "        \n",
    "        \"mean_sensitivity\": avg_sens,\n",
    "        \"mean_sensitivity_anterior\": avg_sens_a,\n",
    "        \"mean_sensitivity_posterior\": avg_sens_p,\n",
    "        \n",
    "        \"mean_specificity\": avg_spec,\n",
    "        \"mean_specificity_anterior\": avg_spec_a,\n",
    "        \"mean_specificity_posterior\": avg_spec_p,\n",
    "        # \"mean_f1\": avg_f1,\n",
    "        }\n",
    "\n",
    "    print(\"\\nTesting complete.\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Average Dice {avg_dc:.4f}, Average Dice Anterior {avg_dc_a:.4f}, Average Dice Posterior {avg_dc_p:.4f}, Average Dice AP Mean {(avg_dc_a+avg_dc_p)/2:.4f}, Average Jaccard {avg_jc:.4f}, Average Sensitivity {avg_sens:.4f}, and Average Specificity {avg_spec:.4f}\")\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaada890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the submodule with skip connection.\n",
    "# X -------------------identity---------------------- X\n",
    "#   |-- downsampling -- |submodule| -- upsampling --|\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    def __init__(self, in_channels=None, out_channels=None, num_classes=1, kernel_size=3,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        # downconv\n",
    "        pool = nn.MaxPool2d(2, stride=2)\n",
    "        conv1 = self.contract(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "        conv2 = self.contract(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, norm_layer=norm_layer)\n",
    "\n",
    "        # upconv\n",
    "        conv3 = self.expand(in_channels=out_channels*2, out_channels=out_channels, kernel_size=kernel_size)\n",
    "        conv4 = self.expand(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "        if outermost:\n",
    "            final = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
    "            down = [conv1, conv2]\n",
    "            up = [conv3, conv4, final]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels,\n",
    "                                        kernel_size=2, stride=2)\n",
    "            model = [pool, conv1, conv2, upconv]\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(in_channels*2, in_channels, kernel_size=2, stride=2)\n",
    "\n",
    "            down = [pool, conv1, conv2]\n",
    "            up = [conv3, conv4, upconv]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    @staticmethod\n",
    "    def contract(in_channels, out_channels, kernel_size=3, norm_layer=nn.InstanceNorm2d):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            norm_layer(out_channels),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def expand(in_channels, out_channels, kernel_size=3):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(layer, target_width, target_height):\n",
    "        batch_size, n_channels, layer_width, layer_height = layer.size()\n",
    "        xy1 = (layer_width - target_width) // 2\n",
    "        xy2 = (layer_height - target_height) // 2\n",
    "        return layer[:, :, xy1:(xy1 + target_width), xy2:(xy2 + target_height)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            crop = self.center_crop(self.model(x), x.size()[2], x.size()[3])\n",
    "            return torch.cat([x, crop], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ebad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, in_channels=1, initial_filter_size=64, kernel_size=3, num_downs=4, norm_layer=nn.InstanceNorm2d):\n",
    "        # norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-1), out_channels=initial_filter_size * 2 ** num_downs,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, norm_layer=norm_layer, innermost=True)\n",
    "        for i in range(1, num_downs):\n",
    "            unet_block = UnetSkipConnectionBlock(in_channels=initial_filter_size * 2 ** (num_downs-(i+1)),\n",
    "                                                 out_channels=initial_filter_size * 2 ** (num_downs-i),\n",
    "                                                 num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(in_channels=in_channels, out_channels=initial_filter_size,\n",
    "                                             num_classes=num_classes, kernel_size=kernel_size, submodule=unet_block, norm_layer=norm_layer,\n",
    "                                             outermost=True)\n",
    "\n",
    "        self.model = unet_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da621c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  71582788  ]\n"
     ]
    }
   ],
   "source": [
    "set_seeds(seed_factor, experiment_id)\n",
    "model = UNet(num_classes=3, initial_filter_size=64, num_downs=4)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=c.learning_rate)\n",
    "# eps (float) â€“ Minimal decay applied to lr. \n",
    "# If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True, eps=1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "886bf880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31030723"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf8aaa8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  71582788  ]\n",
      "\n",
      "Training epoch 0...\n",
      "\n",
      "Epoch: 0 Train loss: 1.055749535560608, 1.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.16964256763458252, 12.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0869569256901741, 24.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.07537630200386047, 35.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06616166234016418, 47.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04276079311966896, 58.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.050687406212091446, 70.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04397191107273102, 81.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02751169539988041, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 0...\n",
      "Batch 0. Loss 0.03216904029250145\n",
      "Batch 1. Loss 0.03226707875728607\n",
      "Batch 2. Loss 0.03060026653110981\n",
      "Batch 3. Loss 0.040598802268505096\n",
      "Batch 4. Loss 0.03779638186097145\n",
      "Batch 5. Loss 0.03463990241289139\n",
      "Batch 6. Loss 0.03537403792142868\n",
      "Batch 7. Loss 0.037249695509672165\n",
      "Batch 8. Loss 0.039378877729177475\n",
      "Batch 9. Loss 0.034961964935064316\n",
      "Batch 10. Loss 0.03748370334506035\n",
      "Batch 11. Loss 0.03229594975709915\n",
      "Batch 12. Loss 0.031183302402496338\n",
      "Batch 13. Loss 0.03349883109331131\n",
      "Batch 14. Loss 0.03521949052810669\n",
      "Batch 15. Loss 0.038028549402952194\n",
      "Batch 16. Loss 0.029530640691518784\n",
      "Batch 17. Loss 0.03877955675125122\n",
      "Batch 18. Loss 0.04107741266489029\n",
      "Batch 19. Loss 0.03936085104942322\n",
      "Batch 20. Loss 0.03403615951538086\n",
      "Batch 21. Loss 0.033029843121767044\n",
      "Batch 22. Loss 0.03763274848461151\n",
      "Batch 23. Loss 0.0379793643951416\n",
      "Batch 24. Loss 0.036390192806720734\n",
      "Batch 25. Loss 0.03748287633061409\n",
      "Batch 26. Loss 0.04060141369700432\n",
      "Batch 27. Loss 0.03184502199292183\n",
      "Batch 28. Loss 0.03696181997656822\n",
      "Current Validation AP Mean DC 17.86948413 | Maximum Validation AP Mean DC 17.86948413\n",
      "Maximum Validation DC 38.59934766\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 1...\n",
      "\n",
      "Epoch: 1 Train loss: 0.03659716621041298, 1.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.0363418385386467, 12.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.03254241123795509, 24.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.026559818536043167, 35.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.03297334536910057, 47.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.03056984581053257, 58.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.021151015534996986, 70.1% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.02310490980744362, 81.6% complete\n",
      "..........\n",
      "Epoch: 1 Train loss: 0.024799445644021034, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 1...\n",
      "Batch 0. Loss 0.023242078721523285\n",
      "Batch 1. Loss 0.02411385253071785\n",
      "Batch 2. Loss 0.025507284328341484\n",
      "Batch 3. Loss 0.023853911086916924\n",
      "Batch 4. Loss 0.020699437707662582\n",
      "Batch 5. Loss 0.025196563452482224\n",
      "Batch 6. Loss 0.01741405390202999\n",
      "Batch 7. Loss 0.02224062755703926\n",
      "Batch 8. Loss 0.02477147988975048\n",
      "Batch 9. Loss 0.020915059372782707\n",
      "Batch 10. Loss 0.02970993146300316\n",
      "Batch 11. Loss 0.02421277016401291\n",
      "Batch 12. Loss 0.01871565729379654\n",
      "Batch 13. Loss 0.02657688967883587\n",
      "Batch 14. Loss 0.01970461755990982\n",
      "Batch 15. Loss 0.01910974271595478\n",
      "Batch 16. Loss 0.022887177765369415\n",
      "Batch 17. Loss 0.022705337032675743\n",
      "Batch 18. Loss 0.02064906805753708\n",
      "Batch 19. Loss 0.023169897496700287\n",
      "Batch 20. Loss 0.023397786542773247\n",
      "Batch 21. Loss 0.018811391666531563\n",
      "Batch 22. Loss 0.020682550966739655\n",
      "Batch 23. Loss 0.020651236176490784\n",
      "Batch 24. Loss 0.021595235913991928\n",
      "Batch 25. Loss 0.024254092946648598\n",
      "Batch 26. Loss 0.022119564935564995\n",
      "Batch 27. Loss 0.017452122643589973\n",
      "Batch 28. Loss 0.02499508112668991\n",
      "Current Validation AP Mean DC 39.979947 | Maximum Validation AP Mean DC 17.86948413\n",
      "Maximum Validation DC 38.59934766\n",
      "Current Validation AP Mean DC improved from 17.86948413 to 39.979947\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 2...\n",
      "\n",
      "Epoch: 2 Train loss: 0.020588688552379608, 1.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.02674802765250206, 12.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.018135465681552887, 24.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.016740748658776283, 35.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.011646449565887451, 47.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.018045784905552864, 58.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.017978686839342117, 70.1% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.015012991614639759, 81.6% complete\n",
      "..........\n",
      "Epoch: 2 Train loss: 0.014117307029664516, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 2...\n",
      "Batch 0. Loss 0.019830821081995964\n",
      "Batch 1. Loss 0.021454965695738792\n",
      "Batch 2. Loss 0.01819073222577572\n",
      "Batch 3. Loss 0.02296076901257038\n",
      "Batch 4. Loss 0.01975700818002224\n",
      "Batch 5. Loss 0.02343183010816574\n",
      "Batch 6. Loss 0.0205910075455904\n",
      "Batch 7. Loss 0.0184220839291811\n",
      "Batch 8. Loss 0.020861690863966942\n",
      "Batch 9. Loss 0.019259082153439522\n",
      "Batch 10. Loss 0.021900976076722145\n",
      "Batch 11. Loss 0.019102666527032852\n",
      "Batch 12. Loss 0.020977262407541275\n",
      "Batch 13. Loss 0.023398667573928833\n",
      "Batch 14. Loss 0.021787717938423157\n",
      "Batch 15. Loss 0.01986536756157875\n",
      "Batch 16. Loss 0.024639718234539032\n",
      "Batch 17. Loss 0.024309733882546425\n",
      "Batch 18. Loss 0.021730732172727585\n",
      "Batch 19. Loss 0.021186871454119682\n",
      "Batch 20. Loss 0.01762901432812214\n",
      "Batch 21. Loss 0.018414869904518127\n",
      "Batch 22. Loss 0.020431244745850563\n",
      "Batch 23. Loss 0.022203953936696053\n",
      "Batch 24. Loss 0.02295680344104767\n",
      "Batch 25. Loss 0.024128969758749008\n",
      "Batch 26. Loss 0.020726798102259636\n",
      "Batch 27. Loss 0.021912889555096626\n",
      "Batch 28. Loss 0.026534918695688248\n",
      "Current Validation AP Mean DC 40.17899967 | Maximum Validation AP Mean DC 39.979947\n",
      "Maximum Validation DC 43.63102892\n",
      "Current Validation AP Mean DC improved from 39.979947 to 40.17899967\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 3...\n",
      "\n",
      "Epoch: 3 Train loss: 0.021626310423016548, 1.1% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.01596684567630291, 12.6% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.013590488582849503, 24.1% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.013209694996476173, 35.6% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.016300182789564133, 47.1% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.014477068558335304, 58.6% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.012908397242426872, 70.1% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.014709552749991417, 81.6% complete\n",
      "..........\n",
      "Epoch: 3 Train loss: 0.016689369454979897, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 3...\n",
      "Batch 0. Loss 0.015438283793628216\n",
      "Batch 1. Loss 0.012943378649652004\n",
      "Batch 2. Loss 0.019521113485097885\n",
      "Batch 3. Loss 0.015369098633527756\n",
      "Batch 4. Loss 0.014607269316911697\n",
      "Batch 5. Loss 0.01597856543958187\n",
      "Batch 6. Loss 0.013637319207191467\n",
      "Batch 7. Loss 0.016168516129255295\n",
      "Batch 8. Loss 0.011729447171092033\n",
      "Batch 9. Loss 0.01770121417939663\n",
      "Batch 10. Loss 0.017032740637660027\n",
      "Batch 11. Loss 0.015805091708898544\n",
      "Batch 12. Loss 0.014496737159788609\n",
      "Batch 13. Loss 0.018941586837172508\n",
      "Batch 14. Loss 0.019485032185912132\n",
      "Batch 15. Loss 0.016258511692285538\n",
      "Batch 16. Loss 0.015001206658780575\n",
      "Batch 17. Loss 0.016785968095064163\n",
      "Batch 18. Loss 0.015227171592414379\n",
      "Batch 19. Loss 0.01884961873292923\n",
      "Batch 20. Loss 0.012466035783290863\n",
      "Batch 21. Loss 0.01917833462357521\n",
      "Batch 22. Loss 0.01756931096315384\n",
      "Batch 23. Loss 0.016491105780005455\n",
      "Batch 24. Loss 0.014854034408926964\n",
      "Batch 25. Loss 0.01845458894968033\n",
      "Batch 26. Loss 0.01730903796851635\n",
      "Batch 27. Loss 0.013551827520132065\n",
      "Batch 28. Loss 0.017267543822526932\n",
      "Current Validation AP Mean DC 43.43566504 | Maximum Validation AP Mean DC 40.17899967\n",
      "Maximum Validation DC 43.63102892\n",
      "Current Validation AP Mean DC improved from 40.17899967 to 43.43566504\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 4...\n",
      "\n",
      "Epoch: 4 Train loss: 0.01542649231851101, 1.1% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.014347624033689499, 12.6% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.013126224279403687, 24.1% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.010161961428821087, 35.6% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.012885353527963161, 47.1% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.01484990119934082, 58.6% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.014216127805411816, 70.1% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.015320087783038616, 81.6% complete\n",
      "..........\n",
      "Epoch: 4 Train loss: 0.011117824353277683, 93.1% complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 4...\n",
      "Batch 0. Loss 0.016492921859025955\n",
      "Batch 1. Loss 0.013751785270869732\n",
      "Batch 2. Loss 0.012152030132710934\n",
      "Batch 3. Loss 0.014509651809930801\n",
      "Batch 4. Loss 0.017340969294309616\n",
      "Batch 5. Loss 0.017274653539061546\n",
      "Batch 6. Loss 0.0162795502692461\n",
      "Batch 7. Loss 0.012872434221208096\n",
      "Batch 8. Loss 0.013331882655620575\n",
      "Batch 9. Loss 0.015448863618075848\n",
      "Batch 10. Loss 0.014994904398918152\n",
      "Batch 11. Loss 0.014806190505623817\n",
      "Batch 12. Loss 0.012778949923813343\n",
      "Batch 13. Loss 0.014600779861211777\n",
      "Batch 14. Loss 0.016192782670259476\n",
      "Batch 15. Loss 0.018945002928376198\n",
      "Batch 16. Loss 0.014050074853003025\n",
      "Batch 17. Loss 0.015744756907224655\n",
      "Batch 18. Loss 0.013200989924371243\n",
      "Batch 19. Loss 0.011862007901072502\n",
      "Batch 20. Loss 0.014370213262736797\n",
      "Batch 21. Loss 0.018234629184007645\n",
      "Batch 22. Loss 0.016084538772702217\n",
      "Batch 23. Loss 0.016106151044368744\n",
      "Batch 24. Loss 0.01580096036195755\n",
      "Batch 25. Loss 0.014223374426364899\n",
      "Batch 26. Loss 0.014361665584146976\n",
      "Batch 27. Loss 0.011947090737521648\n",
      "Batch 28. Loss 0.014849762432277203\n",
      "Current Validation AP Mean DC 44.39023561 | Maximum Validation AP Mean DC 43.43566504\n",
      "Maximum Validation DC 45.27275313\n",
      "Current Validation AP Mean DC improved from 43.43566504 to 44.39023561\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 5...\n",
      "\n",
      "Epoch: 5 Train loss: 0.014150789938867092, 1.1% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.01058224681764841, 12.6% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.015543321147561073, 24.1% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.013983219861984253, 35.6% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.013206693343818188, 47.1% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.012345871888101101, 58.6% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.01274094171822071, 70.1% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.01297913957387209, 81.6% complete\n",
      "..........\n",
      "Epoch: 5 Train loss: 0.014327351935207844, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 5...\n",
      "Batch 0. Loss 0.014289602637290955\n",
      "Batch 1. Loss 0.01308841910213232\n",
      "Batch 2. Loss 0.017339792102575302\n",
      "Batch 3. Loss 0.016549617052078247\n",
      "Batch 4. Loss 0.017280831933021545\n",
      "Batch 5. Loss 0.015325821004807949\n",
      "Batch 6. Loss 0.010958804748952389\n",
      "Batch 7. Loss 0.015152286738157272\n",
      "Batch 8. Loss 0.015358828008174896\n",
      "Batch 9. Loss 0.015367416664958\n",
      "Batch 10. Loss 0.013697801157832146\n",
      "Batch 11. Loss 0.014835790731012821\n",
      "Batch 12. Loss 0.015570104122161865\n",
      "Batch 13. Loss 0.014181028120219707\n",
      "Batch 14. Loss 0.013569782488048077\n",
      "Batch 15. Loss 0.015408352948725224\n",
      "Batch 16. Loss 0.01625044457614422\n",
      "Batch 17. Loss 0.01202347781509161\n",
      "Batch 18. Loss 0.014232391491532326\n",
      "Batch 19. Loss 0.015285682864487171\n",
      "Batch 20. Loss 0.012726887129247189\n",
      "Batch 21. Loss 0.020942093804478645\n",
      "Batch 22. Loss 0.014829806983470917\n",
      "Batch 23. Loss 0.014271182008087635\n",
      "Batch 24. Loss 0.01610499992966652\n",
      "Batch 25. Loss 0.01793583855032921\n",
      "Batch 26. Loss 0.014371989294886589\n",
      "Batch 27. Loss 0.01068618893623352\n",
      "Batch 28. Loss 0.011929105035960674\n",
      "Current Validation AP Mean DC 44.54839867 | Maximum Validation AP Mean DC 44.39023561\n",
      "Maximum Validation DC 45.76985617\n",
      "Current Validation AP Mean DC improved from 44.39023561 to 44.54839867\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 6...\n",
      "\n",
      "Epoch: 6 Train loss: 0.012842277996242046, 1.1% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.015964990481734276, 12.6% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.012014876119792461, 24.1% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.012800808995962143, 35.6% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.009845087304711342, 47.1% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.012255123816430569, 58.6% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.01048365980386734, 70.1% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.010331419296562672, 81.6% complete\n",
      "..........\n",
      "Epoch: 6 Train loss: 0.011869222857058048, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 6...\n",
      "Batch 0. Loss 0.01763947680592537\n",
      "Batch 1. Loss 0.015526962466537952\n",
      "Batch 2. Loss 0.012096328660845757\n",
      "Batch 3. Loss 0.014684893190860748\n",
      "Batch 4. Loss 0.015234547667205334\n",
      "Batch 5. Loss 0.0132607351988554\n",
      "Batch 6. Loss 0.017362482845783234\n",
      "Batch 7. Loss 0.014663731679320335\n",
      "Batch 8. Loss 0.015200099907815456\n",
      "Batch 9. Loss 0.015691706910729408\n",
      "Batch 10. Loss 0.013052009977400303\n",
      "Batch 11. Loss 0.015170554630458355\n",
      "Batch 12. Loss 0.01302359253168106\n",
      "Batch 13. Loss 0.010683106258511543\n",
      "Batch 14. Loss 0.013338425196707249\n",
      "Batch 15. Loss 0.01651391200721264\n",
      "Batch 16. Loss 0.014774961397051811\n",
      "Batch 17. Loss 0.013801970519125462\n",
      "Batch 18. Loss 0.01757965423166752\n",
      "Batch 19. Loss 0.014941033907234669\n",
      "Batch 20. Loss 0.01582736149430275\n",
      "Batch 21. Loss 0.014629795216023922\n",
      "Batch 22. Loss 0.017653627321124077\n",
      "Batch 23. Loss 0.018361488357186317\n",
      "Batch 24. Loss 0.01500929705798626\n",
      "Batch 25. Loss 0.01281106285750866\n",
      "Batch 26. Loss 0.01589338853955269\n",
      "Batch 27. Loss 0.017994191497564316\n",
      "Batch 28. Loss 0.01702266000211239\n",
      "Current Validation AP Mean DC 44.18173065 | Maximum Validation AP Mean DC 44.54839867\n",
      "Maximum Validation DC 45.8903696\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 7...\n",
      "\n",
      "Epoch: 7 Train loss: 0.012625640258193016, 1.1% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.011361498385667801, 12.6% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.012123537249863148, 24.1% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.011635195463895798, 35.6% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.012323839589953423, 47.1% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.01283695362508297, 58.6% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.010605186223983765, 70.1% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.00942312739789486, 81.6% complete\n",
      "..........\n",
      "Epoch: 7 Train loss: 0.011744852177798748, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 7...\n",
      "Batch 0. Loss 0.014454321004450321\n",
      "Batch 1. Loss 0.01715526543557644\n",
      "Batch 2. Loss 0.014821246266365051\n",
      "Batch 3. Loss 0.016146333888173103\n",
      "Batch 4. Loss 0.017326245084404945\n",
      "Batch 5. Loss 0.012275281362235546\n",
      "Batch 6. Loss 0.013707391917705536\n",
      "Batch 7. Loss 0.013156404718756676\n",
      "Batch 8. Loss 0.014617407694458961\n",
      "Batch 9. Loss 0.014603985473513603\n",
      "Batch 10. Loss 0.019336633384227753\n",
      "Batch 11. Loss 0.014408798888325691\n",
      "Batch 12. Loss 0.010509930551052094\n",
      "Batch 13. Loss 0.013227349147200584\n",
      "Batch 14. Loss 0.013270994648337364\n",
      "Batch 15. Loss 0.017137225717306137\n",
      "Batch 16. Loss 0.01510706078261137\n",
      "Batch 17. Loss 0.012756002135574818\n",
      "Batch 18. Loss 0.012937622144818306\n",
      "Batch 19. Loss 0.013638906180858612\n",
      "Batch 20. Loss 0.011516992002725601\n",
      "Batch 21. Loss 0.01495384331792593\n",
      "Batch 22. Loss 0.012150277383625507\n",
      "Batch 23. Loss 0.012911801226437092\n",
      "Batch 24. Loss 0.015148858539760113\n",
      "Batch 25. Loss 0.015259035862982273\n",
      "Batch 26. Loss 0.016308438032865524\n",
      "Batch 27. Loss 0.01231253333389759\n",
      "Batch 28. Loss 0.014585701748728752\n",
      "Current Validation AP Mean DC 44.73743788 | Maximum Validation AP Mean DC 44.54839867\n",
      "Maximum Validation DC 45.8903696\n",
      "Current Validation AP Mean DC improved from 44.54839867 to 44.73743788\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 8...\n",
      "\n",
      "Epoch: 8 Train loss: 0.010883034206926823, 1.1% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.009331384673714638, 12.6% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.00959585141390562, 24.1% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.011592390015721321, 35.6% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.008587217889726162, 47.1% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.011392087675631046, 58.6% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.010333290323615074, 70.1% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.009116682223975658, 81.6% complete\n",
      "..........\n",
      "Epoch: 8 Train loss: 0.009417367167770863, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 8...\n",
      "Batch 0. Loss 0.011002699844539165\n",
      "Batch 1. Loss 0.010509511455893517\n",
      "Batch 2. Loss 0.015091074630618095\n",
      "Batch 3. Loss 0.012835754081606865\n",
      "Batch 4. Loss 0.013180273585021496\n",
      "Batch 5. Loss 0.014277727343142033\n",
      "Batch 6. Loss 0.012884404510259628\n",
      "Batch 7. Loss 0.0135738430544734\n",
      "Batch 8. Loss 0.011645139195024967\n",
      "Batch 9. Loss 0.012120476923882961\n",
      "Batch 10. Loss 0.013346859253942966\n",
      "Batch 11. Loss 0.012320665642619133\n",
      "Batch 12. Loss 0.012661154381930828\n",
      "Batch 13. Loss 0.015465402975678444\n",
      "Batch 14. Loss 0.011836686171591282\n",
      "Batch 15. Loss 0.013008260168135166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16. Loss 0.01738051138818264\n",
      "Batch 17. Loss 0.016847608610987663\n",
      "Batch 18. Loss 0.017040885984897614\n",
      "Batch 19. Loss 0.01331520639359951\n",
      "Batch 20. Loss 0.013407624326646328\n",
      "Batch 21. Loss 0.014204456470906734\n",
      "Batch 22. Loss 0.014599418267607689\n",
      "Batch 23. Loss 0.01535065472126007\n",
      "Batch 24. Loss 0.014696854166686535\n",
      "Batch 25. Loss 0.016181712970137596\n",
      "Batch 26. Loss 0.011269218288362026\n",
      "Batch 27. Loss 0.011945679783821106\n",
      "Batch 28. Loss 0.012746275402605534\n",
      "Current Validation AP Mean DC 45.20473049 | Maximum Validation AP Mean DC 44.73743788\n",
      "Maximum Validation DC 46.21535485\n",
      "Current Validation AP Mean DC improved from 44.73743788 to 45.20473049\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 9...\n",
      "\n",
      "Epoch: 9 Train loss: 0.008464332669973373, 1.1% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.010789196006953716, 12.6% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.01072657946497202, 24.1% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.011522332206368446, 35.6% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.011128480546176434, 47.1% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.00946720875799656, 58.6% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.010333000682294369, 70.1% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.011446729302406311, 81.6% complete\n",
      "..........\n",
      "Epoch: 9 Train loss: 0.012691004201769829, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 9...\n",
      "Batch 0. Loss 0.012120175175368786\n",
      "Batch 1. Loss 0.012534363195300102\n",
      "Batch 2. Loss 0.01359578501433134\n",
      "Batch 3. Loss 0.016903560608625412\n",
      "Batch 4. Loss 0.013948783278465271\n",
      "Batch 5. Loss 0.012564373202621937\n",
      "Batch 6. Loss 0.014801464043557644\n",
      "Batch 7. Loss 0.01668626070022583\n",
      "Batch 8. Loss 0.01585230976343155\n",
      "Batch 9. Loss 0.016039220616221428\n",
      "Batch 10. Loss 0.014335842803120613\n",
      "Batch 11. Loss 0.018002428114414215\n",
      "Batch 12. Loss 0.011096533387899399\n",
      "Batch 13. Loss 0.01325210276991129\n",
      "Batch 14. Loss 0.01347501389682293\n",
      "Batch 15. Loss 0.011514700017869473\n",
      "Batch 16. Loss 0.01426548883318901\n",
      "Batch 17. Loss 0.014195432886481285\n",
      "Batch 18. Loss 0.013979202136397362\n",
      "Batch 19. Loss 0.01499201450496912\n",
      "Batch 20. Loss 0.014129838906228542\n",
      "Batch 21. Loss 0.015119141899049282\n",
      "Batch 22. Loss 0.01838139072060585\n",
      "Batch 23. Loss 0.017395874485373497\n",
      "Batch 24. Loss 0.01697249710559845\n",
      "Batch 25. Loss 0.013319519348442554\n",
      "Batch 26. Loss 0.013040758669376373\n",
      "Batch 27. Loss 0.012273945845663548\n",
      "Batch 28. Loss 0.01725226454436779\n",
      "Current Validation AP Mean DC 45.06710269 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.48588142\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 10...\n",
      "\n",
      "Epoch: 10 Train loss: 0.00857005175203085, 1.1% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.011251234449446201, 12.6% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.009508198127150536, 24.1% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.011800214648246765, 35.6% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.00966787151992321, 47.1% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.010325643233954906, 58.6% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.010154830291867256, 70.1% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.009236329235136509, 81.6% complete\n",
      "..........\n",
      "Epoch: 10 Train loss: 0.010249657556414604, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 10...\n",
      "Batch 0. Loss 0.01751008816063404\n",
      "Batch 1. Loss 0.016031313687562943\n",
      "Batch 2. Loss 0.018819956108927727\n",
      "Batch 3. Loss 0.015544258058071136\n",
      "Batch 4. Loss 0.014736507087945938\n",
      "Batch 5. Loss 0.015136901289224625\n",
      "Batch 6. Loss 0.017345167696475983\n",
      "Batch 7. Loss 0.013604340143501759\n",
      "Batch 8. Loss 0.015374585054814816\n",
      "Batch 9. Loss 0.016614221036434174\n",
      "Batch 10. Loss 0.014639146625995636\n",
      "Batch 11. Loss 0.015046012587845325\n",
      "Batch 12. Loss 0.01698135957121849\n",
      "Batch 13. Loss 0.0133946118876338\n",
      "Batch 14. Loss 0.011799920350313187\n",
      "Batch 15. Loss 0.015958471223711967\n",
      "Batch 16. Loss 0.0161890871822834\n",
      "Batch 17. Loss 0.013115310110151768\n",
      "Batch 18. Loss 0.014769496396183968\n",
      "Batch 19. Loss 0.013764562085270882\n",
      "Batch 20. Loss 0.01539256889373064\n",
      "Batch 21. Loss 0.01386010367423296\n",
      "Batch 22. Loss 0.013614683412015438\n",
      "Batch 23. Loss 0.013884825631976128\n",
      "Batch 24. Loss 0.016780627891421318\n",
      "Batch 25. Loss 0.01453033834695816\n",
      "Batch 26. Loss 0.014225512742996216\n",
      "Batch 27. Loss 0.012219329364597797\n",
      "Batch 28. Loss 0.013063089922070503\n",
      "Current Validation AP Mean DC 44.94468188 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.48588142\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 11...\n",
      "\n",
      "Epoch: 11 Train loss: 0.010257857851684093, 1.1% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.009788289666175842, 12.6% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.010272947140038013, 24.1% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.009253858588635921, 35.6% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.008218573406338692, 47.1% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.009287831373512745, 58.6% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.011371856555342674, 70.1% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.008996888995170593, 81.6% complete\n",
      "..........\n",
      "Epoch: 11 Train loss: 0.010023964568972588, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 11...\n",
      "Batch 0. Loss 0.016553452238440514\n",
      "Batch 1. Loss 0.016411658376455307\n",
      "Batch 2. Loss 0.015037176199257374\n",
      "Batch 3. Loss 0.014622881077229977\n",
      "Batch 4. Loss 0.016694650053977966\n",
      "Batch 5. Loss 0.01498095691204071\n",
      "Batch 6. Loss 0.013621586374938488\n",
      "Batch 7. Loss 0.016598429530858994\n",
      "Batch 8. Loss 0.018869340419769287\n",
      "Batch 9. Loss 0.014880424365401268\n",
      "Batch 10. Loss 0.014998320490121841\n",
      "Batch 11. Loss 0.012310170568525791\n",
      "Batch 12. Loss 0.01588370092213154\n",
      "Batch 13. Loss 0.013775055296719074\n",
      "Batch 14. Loss 0.016999445855617523\n",
      "Batch 15. Loss 0.011997045949101448\n",
      "Batch 16. Loss 0.015434149652719498\n",
      "Batch 17. Loss 0.015802230685949326\n",
      "Batch 18. Loss 0.014342793263494968\n",
      "Batch 19. Loss 0.014057019725441933\n",
      "Batch 20. Loss 0.01455321442335844\n",
      "Batch 21. Loss 0.014059486798942089\n",
      "Batch 22. Loss 0.010842242278158665\n",
      "Batch 23. Loss 0.015605870634317398\n",
      "Batch 24. Loss 0.015000010840594769\n",
      "Batch 25. Loss 0.01837487518787384\n",
      "Batch 26. Loss 0.013452298007905483\n",
      "Batch 27. Loss 0.014632824808359146\n",
      "Batch 28. Loss 0.009542260318994522\n",
      "Current Validation AP Mean DC 44.88746379 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.48588142\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 12...\n",
      "\n",
      "Epoch: 12 Train loss: 0.009823275730013847, 1.1% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.008812219835817814, 12.6% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.007549191825091839, 24.1% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.01015558559447527, 35.6% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.007891550660133362, 47.1% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.009474978782236576, 58.6% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.007955550216138363, 70.1% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.01042701955884695, 81.6% complete\n",
      "..........\n",
      "Epoch: 12 Train loss: 0.009294495917856693, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 12...\n",
      "Batch 0. Loss 0.016414854675531387\n",
      "Batch 1. Loss 0.011194015853106976\n",
      "Batch 2. Loss 0.011550206691026688\n",
      "Batch 3. Loss 0.01548954751342535\n",
      "Batch 4. Loss 0.014284869655966759\n",
      "Batch 5. Loss 0.01603325642645359\n",
      "Batch 6. Loss 0.013715293258428574\n",
      "Batch 7. Loss 0.016478242352604866\n",
      "Batch 8. Loss 0.015404227189719677\n",
      "Batch 9. Loss 0.011686545796692371\n",
      "Batch 10. Loss 0.014953390695154667\n",
      "Batch 11. Loss 0.015457889065146446\n",
      "Batch 12. Loss 0.014602215960621834\n",
      "Batch 13. Loss 0.012592015787959099\n",
      "Batch 14. Loss 0.012051455676555634\n",
      "Batch 15. Loss 0.013941697776317596\n",
      "Batch 16. Loss 0.016374172642827034\n",
      "Batch 17. Loss 0.012295602820813656\n",
      "Batch 18. Loss 0.015915311872959137\n",
      "Batch 19. Loss 0.018194926902651787\n",
      "Batch 20. Loss 0.014529320411384106\n",
      "Batch 21. Loss 0.014959810301661491\n",
      "Batch 22. Loss 0.012270030565559864\n",
      "Batch 23. Loss 0.013745762407779694\n",
      "Batch 24. Loss 0.016236845403909683\n",
      "Batch 25. Loss 0.011963039636611938\n",
      "Batch 26. Loss 0.016037043184041977\n",
      "Batch 27. Loss 0.013078389689326286\n",
      "Batch 28. Loss 0.012139780446887016\n",
      "Current Validation AP Mean DC 45.18063629 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.52165061\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 13...\n",
      "\n",
      "Epoch: 13 Train loss: 0.007369834464043379, 1.1% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.008065159432590008, 12.6% complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Epoch: 13 Train loss: 0.008431646972894669, 24.1% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.008686269633471966, 35.6% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.007753417827188969, 47.1% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.007835796102881432, 58.6% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.010843868367373943, 70.1% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.008174520917236805, 81.6% complete\n",
      "..........\n",
      "Epoch: 13 Train loss: 0.010309431701898575, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 13...\n",
      "Batch 0. Loss 0.014743686653673649\n",
      "Batch 1. Loss 0.014961124397814274\n",
      "Batch 2. Loss 0.014322347939014435\n",
      "Batch 3. Loss 0.017053136602044106\n",
      "Batch 4. Loss 0.015568814240396023\n",
      "Batch 5. Loss 0.015975836664438248\n",
      "Batch 6. Loss 0.015324709936976433\n",
      "Batch 7. Loss 0.013157888315618038\n",
      "Batch 8. Loss 0.012589134275913239\n",
      "Batch 9. Loss 0.01405167393386364\n",
      "Batch 10. Loss 0.013699720613658428\n",
      "Batch 11. Loss 0.011730986647307873\n",
      "Batch 12. Loss 0.01329169049859047\n",
      "Batch 13. Loss 0.013654369860887527\n",
      "Batch 14. Loss 0.009275405667722225\n",
      "Batch 15. Loss 0.017151497304439545\n",
      "Batch 16. Loss 0.01434344332665205\n",
      "Batch 17. Loss 0.015040547586977482\n",
      "Batch 18. Loss 0.010825563222169876\n",
      "Batch 19. Loss 0.017594492062926292\n",
      "Batch 20. Loss 0.01495804637670517\n",
      "Batch 21. Loss 0.01488199271261692\n",
      "Batch 22. Loss 0.017414238303899765\n",
      "Batch 23. Loss 0.013019071891903877\n",
      "Batch 24. Loss 0.01529539842158556\n",
      "Batch 25. Loss 0.01322085503488779\n",
      "Batch 26. Loss 0.01999598927795887\n",
      "Batch 27. Loss 0.016290152445435524\n",
      "Batch 28. Loss 0.017055362462997437\n",
      "Current Validation AP Mean DC 45.17248549 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 14...\n",
      "\n",
      "Epoch: 14 Train loss: 0.006912069860845804, 1.1% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.008985922671854496, 12.6% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.008376014418900013, 24.1% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.008038545027375221, 35.6% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.00794858019798994, 47.1% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.0069632758386433125, 58.6% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.009476570412516594, 70.1% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.008936983533203602, 81.6% complete\n",
      "..........\n",
      "Epoch: 14 Train loss: 0.008423138409852982, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 14...\n",
      "Batch 0. Loss 0.013363235630095005\n",
      "Batch 1. Loss 0.013786598108708858\n",
      "Batch 2. Loss 0.0150983314961195\n",
      "Batch 3. Loss 0.01704842410981655\n",
      "Batch 4. Loss 0.01755458116531372\n",
      "Batch 5. Loss 0.017581336200237274\n",
      "Batch 6. Loss 0.018197251483798027\n",
      "Batch 7. Loss 0.013530950993299484\n",
      "Batch 8. Loss 0.015135128051042557\n",
      "Batch 9. Loss 0.01484628301113844\n",
      "Batch 10. Loss 0.012450683861970901\n",
      "Batch 11. Loss 0.015240231528878212\n",
      "Batch 12. Loss 0.012286164797842503\n",
      "Batch 13. Loss 0.012833811342716217\n",
      "Batch 14. Loss 0.017016952857375145\n",
      "Batch 15. Loss 0.014584628865122795\n",
      "Batch 16. Loss 0.015031930059194565\n",
      "Batch 17. Loss 0.013297325000166893\n",
      "Batch 18. Loss 0.01734060049057007\n",
      "Batch 19. Loss 0.015091201290488243\n",
      "Batch 20. Loss 0.0143159544095397\n",
      "Batch 21. Loss 0.012695016339421272\n",
      "Batch 22. Loss 0.011739865876734257\n",
      "Batch 23. Loss 0.016053326427936554\n",
      "Batch 24. Loss 0.01256164163351059\n",
      "Batch 25. Loss 0.011832006275653839\n",
      "Batch 26. Loss 0.01314251497387886\n",
      "Batch 27. Loss 0.015555918216705322\n",
      "Batch 28. Loss 0.012808299623429775\n",
      "Epoch 00015: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Current Validation AP Mean DC 45.21170978 | Maximum Validation AP Mean DC 45.20473049\n",
      "Maximum Validation DC 46.67858658\n",
      "Current Validation AP Mean DC improved from 45.20473049 to 45.21170978\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 15...\n",
      "\n",
      "Epoch: 15 Train loss: 0.008348815143108368, 1.1% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.0074122752994298935, 12.6% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.007281762547791004, 24.1% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.007648965809494257, 35.6% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.006999331060796976, 47.1% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.0069769443944096565, 58.6% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.00725137535482645, 70.1% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.008329890668392181, 81.6% complete\n",
      "..........\n",
      "Epoch: 15 Train loss: 0.006507792044430971, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 15...\n",
      "Batch 0. Loss 0.01852220855653286\n",
      "Batch 1. Loss 0.012077554129064083\n",
      "Batch 2. Loss 0.018248608335852623\n",
      "Batch 3. Loss 0.01563858799636364\n",
      "Batch 4. Loss 0.011811899952590466\n",
      "Batch 5. Loss 0.014128795824944973\n",
      "Batch 6. Loss 0.017505409196019173\n",
      "Batch 7. Loss 0.013613336719572544\n",
      "Batch 8. Loss 0.013298538513481617\n",
      "Batch 9. Loss 0.01601693592965603\n",
      "Batch 10. Loss 0.01619221270084381\n",
      "Batch 11. Loss 0.016297319903969765\n",
      "Batch 12. Loss 0.015942418947815895\n",
      "Batch 13. Loss 0.014410249888896942\n",
      "Batch 14. Loss 0.014965420588850975\n",
      "Batch 15. Loss 0.014781870879232883\n",
      "Batch 16. Loss 0.01574849896132946\n",
      "Batch 17. Loss 0.012334303930401802\n",
      "Batch 18. Loss 0.0140218585729599\n",
      "Batch 19. Loss 0.016987480223178864\n",
      "Batch 20. Loss 0.01352921687066555\n",
      "Batch 21. Loss 0.015314997173845768\n",
      "Batch 22. Loss 0.01886908896267414\n",
      "Batch 23. Loss 0.017638705670833588\n",
      "Batch 24. Loss 0.015590720809996128\n",
      "Batch 25. Loss 0.010173121467232704\n",
      "Batch 26. Loss 0.012017914094030857\n",
      "Batch 27. Loss 0.015859542414546013\n",
      "Batch 28. Loss 0.0145465899258852\n",
      "Current Validation AP Mean DC 45.28226554 | Maximum Validation AP Mean DC 45.21170978\n",
      "Maximum Validation DC 46.67858658\n",
      "Current Validation AP Mean DC improved from 45.21170978 to 45.28226554\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 16...\n",
      "\n",
      "Epoch: 16 Train loss: 0.007812377065420151, 1.1% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.00690667936578393, 12.6% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.0069184438325464725, 24.1% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.008233400993049145, 35.6% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.008428972214460373, 47.1% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.007376453839242458, 58.6% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.007074781693518162, 70.1% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.006613391917198896, 81.6% complete\n",
      "..........\n",
      "Epoch: 16 Train loss: 0.007194153033196926, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 16...\n",
      "Batch 0. Loss 0.01257763896137476\n",
      "Batch 1. Loss 0.014558477327227592\n",
      "Batch 2. Loss 0.01616380363702774\n",
      "Batch 3. Loss 0.016833141446113586\n",
      "Batch 4. Loss 0.013024324551224709\n",
      "Batch 5. Loss 0.014324123039841652\n",
      "Batch 6. Loss 0.015808269381523132\n",
      "Batch 7. Loss 0.014274267479777336\n",
      "Batch 8. Loss 0.013312911614775658\n",
      "Batch 9. Loss 0.016088800504803658\n",
      "Batch 10. Loss 0.016376446932554245\n",
      "Batch 11. Loss 0.01694844849407673\n",
      "Batch 12. Loss 0.017979804426431656\n",
      "Batch 13. Loss 0.016591127961874008\n",
      "Batch 14. Loss 0.014096787199378014\n",
      "Batch 15. Loss 0.01249262597411871\n",
      "Batch 16. Loss 0.018227990716695786\n",
      "Batch 17. Loss 0.017737796530127525\n",
      "Batch 18. Loss 0.014671905897557735\n",
      "Batch 19. Loss 0.016217920929193497\n",
      "Batch 20. Loss 0.013002771884202957\n",
      "Batch 21. Loss 0.016431517899036407\n",
      "Batch 22. Loss 0.014296132139861584\n",
      "Batch 23. Loss 0.01560123823583126\n",
      "Batch 24. Loss 0.018170202150940895\n",
      "Batch 25. Loss 0.018406346440315247\n",
      "Batch 26. Loss 0.015018383972346783\n",
      "Batch 27. Loss 0.016056975349783897\n",
      "Batch 28. Loss 0.012013707309961319\n",
      "Current Validation AP Mean DC 45.1704958 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 17...\n",
      "\n",
      "Epoch: 17 Train loss: 0.007604152895510197, 1.1% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.006692844443023205, 12.6% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007072924170643091, 24.1% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007113391999155283, 35.6% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007174914702773094, 47.1% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007174680475145578, 58.6% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007922989316284657, 70.1% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.007067959289997816, 81.6% complete\n",
      "..........\n",
      "Epoch: 17 Train loss: 0.006913412362337112, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 17...\n",
      "Batch 0. Loss 0.014141510240733624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1. Loss 0.013317720964550972\n",
      "Batch 2. Loss 0.014977630227804184\n",
      "Batch 3. Loss 0.016823239624500275\n",
      "Batch 4. Loss 0.02063438668847084\n",
      "Batch 5. Loss 0.015344977378845215\n",
      "Batch 6. Loss 0.015413099899888039\n",
      "Batch 7. Loss 0.014477328397333622\n",
      "Batch 8. Loss 0.012898465618491173\n",
      "Batch 9. Loss 0.015698393806815147\n",
      "Batch 10. Loss 0.018572421744465828\n",
      "Batch 11. Loss 0.016285177320241928\n",
      "Batch 12. Loss 0.015180273912847042\n",
      "Batch 13. Loss 0.014955602586269379\n",
      "Batch 14. Loss 0.013392829336225986\n",
      "Batch 15. Loss 0.01619706302881241\n",
      "Batch 16. Loss 0.018608063459396362\n",
      "Batch 17. Loss 0.01267162524163723\n",
      "Batch 18. Loss 0.014594155363738537\n",
      "Batch 19. Loss 0.015031712129712105\n",
      "Batch 20. Loss 0.01421604584902525\n",
      "Batch 21. Loss 0.018775662407279015\n",
      "Batch 22. Loss 0.01506762858480215\n",
      "Batch 23. Loss 0.012578858993947506\n",
      "Batch 24. Loss 0.019046630710363388\n",
      "Batch 25. Loss 0.01766008324921131\n",
      "Batch 26. Loss 0.020952625200152397\n",
      "Batch 27. Loss 0.014937667176127434\n",
      "Batch 28. Loss 0.012178434990346432\n",
      "Current Validation AP Mean DC 45.22755073 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 18...\n",
      "\n",
      "Epoch: 18 Train loss: 0.006126584019511938, 1.1% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.006311400327831507, 12.6% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.006796465255320072, 24.1% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.00745040038600564, 35.6% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.008301488123834133, 47.1% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.007761411368846893, 58.6% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.006325617432594299, 70.1% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.006759595591574907, 81.6% complete\n",
      "..........\n",
      "Epoch: 18 Train loss: 0.007973226718604565, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 18...\n",
      "Batch 0. Loss 0.01575601100921631\n",
      "Batch 1. Loss 0.014611955732107162\n",
      "Batch 2. Loss 0.015898263081908226\n",
      "Batch 3. Loss 0.016192929819226265\n",
      "Batch 4. Loss 0.017806584015488625\n",
      "Batch 5. Loss 0.017960160970687866\n",
      "Batch 6. Loss 0.017364390194416046\n",
      "Batch 7. Loss 0.018084699288010597\n",
      "Batch 8. Loss 0.014840478077530861\n",
      "Batch 9. Loss 0.014170454815030098\n",
      "Batch 10. Loss 0.020087547600269318\n",
      "Batch 11. Loss 0.014827865175902843\n",
      "Batch 12. Loss 0.01687103882431984\n",
      "Batch 13. Loss 0.01533498615026474\n",
      "Batch 14. Loss 0.01727912575006485\n",
      "Batch 15. Loss 0.01886703632771969\n",
      "Batch 16. Loss 0.016584794968366623\n",
      "Batch 17. Loss 0.014509372413158417\n",
      "Batch 18. Loss 0.015272706747055054\n",
      "Batch 19. Loss 0.011079146526753902\n",
      "Batch 20. Loss 0.01995238848030567\n",
      "Batch 21. Loss 0.01604025438427925\n",
      "Batch 22. Loss 0.01599063351750374\n",
      "Batch 23. Loss 0.012878208421170712\n",
      "Batch 24. Loss 0.017708253115415573\n",
      "Batch 25. Loss 0.013731686398386955\n",
      "Batch 26. Loss 0.013121208176016808\n",
      "Batch 27. Loss 0.016481176018714905\n",
      "Batch 28. Loss 0.019397197291254997\n",
      "Current Validation AP Mean DC 45.12131637 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 19...\n",
      "\n",
      "Epoch: 19 Train loss: 0.006054067052900791, 1.1% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.007014023140072823, 12.6% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.0067400382831692696, 24.1% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.006948505062609911, 35.6% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.006746208295226097, 47.1% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.006959191057831049, 58.6% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.006470822729170322, 70.1% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.006707699969410896, 81.6% complete\n",
      "..........\n",
      "Epoch: 19 Train loss: 0.007492546923458576, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 19...\n",
      "Batch 0. Loss 0.017844703048467636\n",
      "Batch 1. Loss 0.020195897668600082\n",
      "Batch 2. Loss 0.0137846814468503\n",
      "Batch 3. Loss 0.016186915338039398\n",
      "Batch 4. Loss 0.018328657373785973\n",
      "Batch 5. Loss 0.012132616713643074\n",
      "Batch 6. Loss 0.017180483788251877\n",
      "Batch 7. Loss 0.014566675759851933\n",
      "Batch 8. Loss 0.016014909371733665\n",
      "Batch 9. Loss 0.016992347314953804\n",
      "Batch 10. Loss 0.017555272206664085\n",
      "Batch 11. Loss 0.014424275606870651\n",
      "Batch 12. Loss 0.014584634453058243\n",
      "Batch 13. Loss 0.017231736332178116\n",
      "Batch 14. Loss 0.016678234562277794\n",
      "Batch 15. Loss 0.016802500933408737\n",
      "Batch 16. Loss 0.013803763315081596\n",
      "Batch 17. Loss 0.0217053834348917\n",
      "Batch 18. Loss 0.016040056943893433\n",
      "Batch 19. Loss 0.017074113711714745\n",
      "Batch 20. Loss 0.012353907339274883\n",
      "Batch 21. Loss 0.017567554488778114\n",
      "Batch 22. Loss 0.016433486714959145\n",
      "Batch 23. Loss 0.012958021834492683\n",
      "Batch 24. Loss 0.01782119832932949\n",
      "Batch 25. Loss 0.015511500649154186\n",
      "Batch 26. Loss 0.01716083660721779\n",
      "Batch 27. Loss 0.015975505113601685\n",
      "Batch 28. Loss 0.020148156210780144\n",
      "Current Validation AP Mean DC 45.1116518 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 20...\n",
      "\n",
      "Epoch: 20 Train loss: 0.006063447799533606, 1.1% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.006113933399319649, 12.6% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.0057626692578196526, 24.1% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.006809870712459087, 35.6% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.005275133531540632, 47.1% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.007085319142788649, 58.6% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.006799905560910702, 70.1% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.006028979551047087, 81.6% complete\n",
      "..........\n",
      "Epoch: 20 Train loss: 0.007002201396971941, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 20...\n",
      "Batch 0. Loss 0.011836912482976913\n",
      "Batch 1. Loss 0.01732630468904972\n",
      "Batch 2. Loss 0.019231002777814865\n",
      "Batch 3. Loss 0.021046988666057587\n",
      "Batch 4. Loss 0.01238935999572277\n",
      "Batch 5. Loss 0.013427252881228924\n",
      "Batch 6. Loss 0.018078098073601723\n",
      "Batch 7. Loss 0.014791433699429035\n",
      "Batch 8. Loss 0.016701487824320793\n",
      "Batch 9. Loss 0.02092624269425869\n",
      "Batch 10. Loss 0.018925998359918594\n",
      "Batch 11. Loss 0.013908539898693562\n",
      "Batch 12. Loss 0.01636044681072235\n",
      "Batch 13. Loss 0.01808140054345131\n",
      "Batch 14. Loss 0.018229208886623383\n",
      "Batch 15. Loss 0.018618375062942505\n",
      "Batch 16. Loss 0.018561966717243195\n",
      "Batch 17. Loss 0.01562894694507122\n",
      "Batch 18. Loss 0.019286980852484703\n",
      "Batch 19. Loss 0.019911570474505424\n",
      "Batch 20. Loss 0.014817142859101295\n",
      "Batch 21. Loss 0.013218146748840809\n",
      "Batch 22. Loss 0.017561163753271103\n",
      "Batch 23. Loss 0.01427589263767004\n",
      "Batch 24. Loss 0.014570657163858414\n",
      "Batch 25. Loss 0.014463799074292183\n",
      "Batch 26. Loss 0.015282954089343548\n",
      "Batch 27. Loss 0.015234127640724182\n",
      "Batch 28. Loss 0.016637345775961876\n",
      "Epoch 00021: reducing learning rate of group 0 to 2.0000e-06.\n",
      "Current Validation AP Mean DC 45.11734263 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 21...\n",
      "\n",
      "Epoch: 21 Train loss: 0.007178298197686672, 1.1% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.008556184358894825, 12.6% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.005876383278518915, 24.1% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.007781374733895063, 35.6% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.006317957304418087, 47.1% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.0075029851868748665, 58.6% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.006942702457308769, 70.1% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.006466415245085955, 81.6% complete\n",
      "..........\n",
      "Epoch: 21 Train loss: 0.007695411331951618, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 21...\n",
      "Batch 0. Loss 0.014706217683851719\n",
      "Batch 1. Loss 0.01658053509891033\n",
      "Batch 2. Loss 0.02017930895090103\n",
      "Batch 3. Loss 0.015154238790273666\n",
      "Batch 4. Loss 0.01719019189476967\n",
      "Batch 5. Loss 0.017541438341140747\n",
      "Batch 6. Loss 0.016568858176469803\n",
      "Batch 7. Loss 0.02195669896900654\n",
      "Batch 8. Loss 0.015832489356398582\n",
      "Batch 9. Loss 0.013589073903858662\n",
      "Batch 10. Loss 0.01532343216240406\n",
      "Batch 11. Loss 0.01447928324341774\n",
      "Batch 12. Loss 0.014060457237064838\n",
      "Batch 13. Loss 0.017347102984786034\n",
      "Batch 14. Loss 0.0167416799813509\n",
      "Batch 15. Loss 0.014818280935287476\n",
      "Batch 16. Loss 0.01488529983907938\n",
      "Batch 17. Loss 0.01856634020805359\n",
      "Batch 18. Loss 0.016699999570846558\n",
      "Batch 19. Loss 0.01752992533147335\n",
      "Batch 20. Loss 0.015198214910924435\n",
      "Batch 21. Loss 0.017204947769641876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22. Loss 0.014443938620388508\n",
      "Batch 23. Loss 0.016700461506843567\n",
      "Batch 24. Loss 0.018118131905794144\n",
      "Batch 25. Loss 0.017872439697384834\n",
      "Batch 26. Loss 0.01888856664299965\n",
      "Batch 27. Loss 0.022059623152017593\n",
      "Batch 28. Loss 0.012083862908184528\n",
      "Current Validation AP Mean DC 45.05146321 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 22...\n",
      "\n",
      "Epoch: 22 Train loss: 0.006635845638811588, 1.1% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.006146200932562351, 12.6% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.006315212696790695, 24.1% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.007590541150420904, 35.6% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.006443270482122898, 47.1% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.0046949381940066814, 58.6% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.006877305917441845, 70.1% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.004903652239590883, 81.6% complete\n",
      "..........\n",
      "Epoch: 22 Train loss: 0.0064368657767772675, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 22...\n",
      "Batch 0. Loss 0.016640832647681236\n",
      "Batch 1. Loss 0.015833929181098938\n",
      "Batch 2. Loss 0.01736178994178772\n",
      "Batch 3. Loss 0.01486196555197239\n",
      "Batch 4. Loss 0.01448096800595522\n",
      "Batch 5. Loss 0.01841926760971546\n",
      "Batch 6. Loss 0.01599358581006527\n",
      "Batch 7. Loss 0.019263625144958496\n",
      "Batch 8. Loss 0.019556596875190735\n",
      "Batch 9. Loss 0.018358848989009857\n",
      "Batch 10. Loss 0.01806364580988884\n",
      "Batch 11. Loss 0.016368616372346878\n",
      "Batch 12. Loss 0.01287555880844593\n",
      "Batch 13. Loss 0.017946261912584305\n",
      "Batch 14. Loss 0.01741558499634266\n",
      "Batch 15. Loss 0.015639062970876694\n",
      "Batch 16. Loss 0.01696733757853508\n",
      "Batch 17. Loss 0.019246146082878113\n",
      "Batch 18. Loss 0.017707427963614464\n",
      "Batch 19. Loss 0.013250387273728848\n",
      "Batch 20. Loss 0.02012781985104084\n",
      "Batch 21. Loss 0.015244248323142529\n",
      "Batch 22. Loss 0.017175620421767235\n",
      "Batch 23. Loss 0.018501736223697662\n",
      "Batch 24. Loss 0.018090303987264633\n",
      "Batch 25. Loss 0.015020562335848808\n",
      "Batch 26. Loss 0.013177096843719482\n",
      "Batch 27. Loss 0.01631814055144787\n",
      "Batch 28. Loss 0.017801014706492424\n",
      "Current Validation AP Mean DC 45.04001092 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 23...\n",
      "\n",
      "Epoch: 23 Train loss: 0.006589700002223253, 1.1% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.006778844632208347, 12.6% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.0067394813522696495, 24.1% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.005730596836656332, 35.6% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.00643913634121418, 47.1% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.006921866443008184, 58.6% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.006618788931518793, 70.1% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.006640551146119833, 81.6% complete\n",
      "..........\n",
      "Epoch: 23 Train loss: 0.0067488038912415504, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 23...\n",
      "Batch 0. Loss 0.022259246557950974\n",
      "Batch 1. Loss 0.020041024312376976\n",
      "Batch 2. Loss 0.01747771166265011\n",
      "Batch 3. Loss 0.015398959629237652\n",
      "Batch 4. Loss 0.01873871684074402\n",
      "Batch 5. Loss 0.01529140304774046\n",
      "Batch 6. Loss 0.01719149574637413\n",
      "Batch 7. Loss 0.013871774077415466\n",
      "Batch 8. Loss 0.018418962135910988\n",
      "Batch 9. Loss 0.01652681641280651\n",
      "Batch 10. Loss 0.017789360135793686\n",
      "Batch 11. Loss 0.018552573397755623\n",
      "Batch 12. Loss 0.013459988869726658\n",
      "Batch 13. Loss 0.015489340759813786\n",
      "Batch 14. Loss 0.015199866145849228\n",
      "Batch 15. Loss 0.015052015893161297\n",
      "Batch 16. Loss 0.018849531188607216\n",
      "Batch 17. Loss 0.01863163895905018\n",
      "Batch 18. Loss 0.017425816506147385\n",
      "Batch 19. Loss 0.013937960378825665\n",
      "Batch 20. Loss 0.015370155684649944\n",
      "Batch 21. Loss 0.018128057941794395\n",
      "Batch 22. Loss 0.016042811796069145\n",
      "Batch 23. Loss 0.018586892634630203\n",
      "Batch 24. Loss 0.013847186230123043\n",
      "Batch 25. Loss 0.016763752326369286\n",
      "Batch 26. Loss 0.015239416621625423\n",
      "Batch 27. Loss 0.01662871241569519\n",
      "Batch 28. Loss 0.017431339249014854\n",
      "Current Validation AP Mean DC 45.05059299 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 24...\n",
      "\n",
      "Epoch: 24 Train loss: 0.006909099407494068, 1.1% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.0069526853039860725, 12.6% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.004803563002496958, 24.1% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.0067422655411064625, 35.6% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.005175589118152857, 47.1% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.00711783766746521, 58.6% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.00778990238904953, 70.1% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.006349221803247929, 81.6% complete\n",
      "..........\n",
      "Epoch: 24 Train loss: 0.006072051357477903, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 24...\n",
      "Batch 0. Loss 0.015435230918228626\n",
      "Batch 1. Loss 0.013494130223989487\n",
      "Batch 2. Loss 0.01710762456059456\n",
      "Batch 3. Loss 0.01913338154554367\n",
      "Batch 4. Loss 0.016617417335510254\n",
      "Batch 5. Loss 0.01413054671138525\n",
      "Batch 6. Loss 0.015607774257659912\n",
      "Batch 7. Loss 0.018562229350209236\n",
      "Batch 8. Loss 0.016153529286384583\n",
      "Batch 9. Loss 0.013641530647873878\n",
      "Batch 10. Loss 0.01706003211438656\n",
      "Batch 11. Loss 0.020762410014867783\n",
      "Batch 12. Loss 0.016278551891446114\n",
      "Batch 13. Loss 0.018123146146535873\n",
      "Batch 14. Loss 0.01768708974123001\n",
      "Batch 15. Loss 0.020099585875868797\n",
      "Batch 16. Loss 0.016165221109986305\n",
      "Batch 17. Loss 0.021284446120262146\n",
      "Batch 18. Loss 0.02164873294532299\n",
      "Batch 19. Loss 0.017947955057024956\n",
      "Batch 20. Loss 0.01611158810555935\n",
      "Batch 21. Loss 0.022039562463760376\n",
      "Batch 22. Loss 0.013403832912445068\n",
      "Batch 23. Loss 0.016812603920698166\n",
      "Batch 24. Loss 0.015178506262600422\n",
      "Batch 25. Loss 0.015137922950088978\n",
      "Batch 26. Loss 0.013553031720221043\n",
      "Batch 27. Loss 0.015451976098120213\n",
      "Batch 28. Loss 0.016858067363500595\n",
      "Current Validation AP Mean DC 45.01755872 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 25...\n",
      "\n",
      "Epoch: 25 Train loss: 0.0057182773016393185, 1.1% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.006531126797199249, 12.6% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.005906843114644289, 24.1% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.0055622500367462635, 35.6% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.006015654653310776, 47.1% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.005304197780787945, 58.6% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.006348843686282635, 70.1% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.006621997803449631, 81.6% complete\n",
      "..........\n",
      "Epoch: 25 Train loss: 0.006205912213772535, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 25...\n",
      "Batch 0. Loss 0.016502857208251953\n",
      "Batch 1. Loss 0.015508905053138733\n",
      "Batch 2. Loss 0.014663828536868095\n",
      "Batch 3. Loss 0.017797475680708885\n",
      "Batch 4. Loss 0.015950733795762062\n",
      "Batch 5. Loss 0.01688973791897297\n",
      "Batch 6. Loss 0.015375048853456974\n",
      "Batch 7. Loss 0.014860137365758419\n",
      "Batch 8. Loss 0.021018538624048233\n",
      "Batch 9. Loss 0.01754213124513626\n",
      "Batch 10. Loss 0.017147505655884743\n",
      "Batch 11. Loss 0.019072357565164566\n",
      "Batch 12. Loss 0.016644123941659927\n",
      "Batch 13. Loss 0.015638167038559914\n",
      "Batch 14. Loss 0.0177064910531044\n",
      "Batch 15. Loss 0.017722465097904205\n",
      "Batch 16. Loss 0.020948220044374466\n",
      "Batch 17. Loss 0.0191964041441679\n",
      "Batch 18. Loss 0.01743815839290619\n",
      "Batch 19. Loss 0.014384466223418713\n",
      "Batch 20. Loss 0.015122306533157825\n",
      "Batch 21. Loss 0.0195440873503685\n",
      "Batch 22. Loss 0.014836639165878296\n",
      "Batch 23. Loss 0.017774133011698723\n",
      "Batch 24. Loss 0.01575080305337906\n",
      "Batch 25. Loss 0.017992088571190834\n",
      "Batch 26. Loss 0.016315462067723274\n",
      "Batch 27. Loss 0.01526645477861166\n",
      "Batch 28. Loss 0.019205300137400627\n",
      "Current Validation AP Mean DC 45.01185359 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 26...\n",
      "\n",
      "Epoch: 26 Train loss: 0.00746510224416852, 1.1% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.006889644544571638, 12.6% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.006051928270608187, 24.1% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.0067164096981287, 35.6% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.00643074419349432, 47.1% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.005775116849690676, 58.6% complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Epoch: 26 Train loss: 0.006790658459067345, 70.1% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.0070107365027070045, 81.6% complete\n",
      "..........\n",
      "Epoch: 26 Train loss: 0.005967090371996164, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 26...\n",
      "Batch 0. Loss 0.01649881713092327\n",
      "Batch 1. Loss 0.016465242952108383\n",
      "Batch 2. Loss 0.016941005364060402\n",
      "Batch 3. Loss 0.017211973667144775\n",
      "Batch 4. Loss 0.0158708319067955\n",
      "Batch 5. Loss 0.016232576221227646\n",
      "Batch 6. Loss 0.017269087955355644\n",
      "Batch 7. Loss 0.017080850899219513\n",
      "Batch 8. Loss 0.015066277235746384\n",
      "Batch 9. Loss 0.019804853945970535\n",
      "Batch 10. Loss 0.01779145933687687\n",
      "Batch 11. Loss 0.01888125017285347\n",
      "Batch 12. Loss 0.014147325418889523\n",
      "Batch 13. Loss 0.013752075843513012\n",
      "Batch 14. Loss 0.019256072118878365\n",
      "Batch 15. Loss 0.018923470750451088\n",
      "Batch 16. Loss 0.01657470129430294\n",
      "Batch 17. Loss 0.015846816822886467\n",
      "Batch 18. Loss 0.013376295566558838\n",
      "Batch 19. Loss 0.0173171479254961\n",
      "Batch 20. Loss 0.020395971834659576\n",
      "Batch 21. Loss 0.017076462507247925\n",
      "Batch 22. Loss 0.01455327495932579\n",
      "Batch 23. Loss 0.015950610861182213\n",
      "Batch 24. Loss 0.015618867240846157\n",
      "Batch 25. Loss 0.020641988143324852\n",
      "Batch 26. Loss 0.01710641384124756\n",
      "Batch 27. Loss 0.017615845426917076\n",
      "Batch 28. Loss 0.020856408402323723\n",
      "Epoch 00027: reducing learning rate of group 0 to 2.0000e-07.\n",
      "Current Validation AP Mean DC 45.02362603 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 27...\n",
      "\n",
      "Epoch: 27 Train loss: 0.005999973509460688, 1.1% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.006414392497390509, 12.6% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.007534535601735115, 24.1% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.007088892627507448, 35.6% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.006958821788430214, 47.1% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.006101415958255529, 58.6% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.007283117156475782, 70.1% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.007705972995609045, 81.6% complete\n",
      "..........\n",
      "Epoch: 27 Train loss: 0.005642960779368877, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 27...\n",
      "Batch 0. Loss 0.012295925989747047\n",
      "Batch 1. Loss 0.018035678192973137\n",
      "Batch 2. Loss 0.01670362800359726\n",
      "Batch 3. Loss 0.015716373920440674\n",
      "Batch 4. Loss 0.015209848061203957\n",
      "Batch 5. Loss 0.017542162910103798\n",
      "Batch 6. Loss 0.015573776327073574\n",
      "Batch 7. Loss 0.01957717165350914\n",
      "Batch 8. Loss 0.017184626311063766\n",
      "Batch 9. Loss 0.015705034136772156\n",
      "Batch 10. Loss 0.022314831614494324\n",
      "Batch 11. Loss 0.020429592579603195\n",
      "Batch 12. Loss 0.018267668783664703\n",
      "Batch 13. Loss 0.01608862541615963\n",
      "Batch 14. Loss 0.016560787335038185\n",
      "Batch 15. Loss 0.017822038382291794\n",
      "Batch 16. Loss 0.017841555178165436\n",
      "Batch 17. Loss 0.017755385488271713\n",
      "Batch 18. Loss 0.01657821238040924\n",
      "Batch 19. Loss 0.01594894379377365\n",
      "Batch 20. Loss 0.01558771263808012\n",
      "Batch 21. Loss 0.018984898924827576\n",
      "Batch 22. Loss 0.019343657419085503\n",
      "Batch 23. Loss 0.01507041696459055\n",
      "Batch 24. Loss 0.017384307458996773\n",
      "Batch 25. Loss 0.018300840631127357\n",
      "Batch 26. Loss 0.012856673449277878\n",
      "Batch 27. Loss 0.015154698863625526\n",
      "Batch 28. Loss 0.016090746968984604\n",
      "Current Validation AP Mean DC 45.02752876 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 28...\n",
      "\n",
      "Epoch: 28 Train loss: 0.00709820631891489, 1.1% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.005801839753985405, 12.6% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.007499421946704388, 24.1% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.005806795787066221, 35.6% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.0061028385534882545, 47.1% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.006018597166985273, 58.6% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.006782132666558027, 70.1% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.006436117924749851, 81.6% complete\n",
      "..........\n",
      "Epoch: 28 Train loss: 0.004939895588904619, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 28...\n",
      "Batch 0. Loss 0.013117469847202301\n",
      "Batch 1. Loss 0.018244700506329536\n",
      "Batch 2. Loss 0.017830245196819305\n",
      "Batch 3. Loss 0.01920916885137558\n",
      "Batch 4. Loss 0.01634906977415085\n",
      "Batch 5. Loss 0.015897708013653755\n",
      "Batch 6. Loss 0.01849919743835926\n",
      "Batch 7. Loss 0.01775449328124523\n",
      "Batch 8. Loss 0.014555574394762516\n",
      "Batch 9. Loss 0.02013806253671646\n",
      "Batch 10. Loss 0.018132483586668968\n",
      "Batch 11. Loss 0.01364224124699831\n",
      "Batch 12. Loss 0.019813939929008484\n",
      "Batch 13. Loss 0.013654451817274094\n",
      "Batch 14. Loss 0.017130641266703606\n",
      "Batch 15. Loss 0.01934255287051201\n",
      "Batch 16. Loss 0.01826617680490017\n",
      "Batch 17. Loss 0.01703932136297226\n",
      "Batch 18. Loss 0.014299890026450157\n",
      "Batch 19. Loss 0.017622072249650955\n",
      "Batch 20. Loss 0.019242331385612488\n",
      "Batch 21. Loss 0.013231300748884678\n",
      "Batch 22. Loss 0.015156666748225689\n",
      "Batch 23. Loss 0.01882542110979557\n",
      "Batch 24. Loss 0.020982887595891953\n",
      "Batch 25. Loss 0.017218248918652534\n",
      "Batch 26. Loss 0.013915399089455605\n",
      "Batch 27. Loss 0.014723006635904312\n",
      "Batch 28. Loss 0.021882448345422745\n",
      "Current Validation AP Mean DC 45.02199063 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 29...\n",
      "\n",
      "Epoch: 29 Train loss: 0.00653862627223134, 1.1% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.006663129199296236, 12.6% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.006587941199541092, 24.1% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.007997089996933937, 35.6% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.00663021020591259, 47.1% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.006630664225667715, 58.6% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.007738455198705196, 70.1% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.005571306217461824, 81.6% complete\n",
      "..........\n",
      "Epoch: 29 Train loss: 0.0060034943744540215, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 29...\n",
      "Batch 0. Loss 0.018214479088783264\n",
      "Batch 1. Loss 0.018335137516260147\n",
      "Batch 2. Loss 0.01526738703250885\n",
      "Batch 3. Loss 0.016793178394436836\n",
      "Batch 4. Loss 0.01494520902633667\n",
      "Batch 5. Loss 0.01730537973344326\n",
      "Batch 6. Loss 0.015111073851585388\n",
      "Batch 7. Loss 0.0163964182138443\n",
      "Batch 8. Loss 0.015813346952199936\n",
      "Batch 9. Loss 0.015845241025090218\n",
      "Batch 10. Loss 0.01865260675549507\n",
      "Batch 11. Loss 0.019687646999955177\n",
      "Batch 12. Loss 0.01976439356803894\n",
      "Batch 13. Loss 0.017882490530610085\n",
      "Batch 14. Loss 0.0168362595140934\n",
      "Batch 15. Loss 0.014712486416101456\n",
      "Batch 16. Loss 0.018822992220520973\n",
      "Batch 17. Loss 0.020448755472898483\n",
      "Batch 18. Loss 0.01611575484275818\n",
      "Batch 19. Loss 0.016936274245381355\n",
      "Batch 20. Loss 0.016590172424912453\n",
      "Batch 21. Loss 0.01628687232732773\n",
      "Batch 22. Loss 0.015613924711942673\n",
      "Batch 23. Loss 0.017105821520090103\n",
      "Batch 24. Loss 0.01720428839325905\n",
      "Batch 25. Loss 0.016285473480820656\n",
      "Batch 26. Loss 0.016008993610739708\n",
      "Batch 27. Loss 0.01687912829220295\n",
      "Batch 28. Loss 0.017673296853899956\n",
      "Current Validation AP Mean DC 45.02256107 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 30...\n",
      "\n",
      "Epoch: 30 Train loss: 0.006522235460579395, 1.1% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.0055599394254386425, 12.6% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.006818735506385565, 24.1% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.006151692941784859, 35.6% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.008564031682908535, 47.1% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.005467499606311321, 58.6% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.006088886875659227, 70.1% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.007084330078214407, 81.6% complete\n",
      "..........\n",
      "Epoch: 30 Train loss: 0.007028468418866396, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 30...\n",
      "Batch 0. Loss 0.009812897071242332\n",
      "Batch 1. Loss 0.017572790384292603\n",
      "Batch 2. Loss 0.01670362614095211\n",
      "Batch 3. Loss 0.015946362167596817\n",
      "Batch 4. Loss 0.016517367213964462\n",
      "Batch 5. Loss 0.014568749815225601\n",
      "Batch 6. Loss 0.01667604222893715\n",
      "Batch 7. Loss 0.01990671455860138\n",
      "Batch 8. Loss 0.018117157742381096\n",
      "Batch 9. Loss 0.017081452533602715\n",
      "Batch 10. Loss 0.015575959347188473\n",
      "Batch 11. Loss 0.01442553661763668\n",
      "Batch 12. Loss 0.01269511692225933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13. Loss 0.0157126747071743\n",
      "Batch 14. Loss 0.01712738163769245\n",
      "Batch 15. Loss 0.018360471352934837\n",
      "Batch 16. Loss 0.01626560091972351\n",
      "Batch 17. Loss 0.019205622375011444\n",
      "Batch 18. Loss 0.014142856933176517\n",
      "Batch 19. Loss 0.01716024987399578\n",
      "Batch 20. Loss 0.015022937208414078\n",
      "Batch 21. Loss 0.019270554184913635\n",
      "Batch 22. Loss 0.017750713974237442\n",
      "Batch 23. Loss 0.01709337718784809\n",
      "Batch 24. Loss 0.020222589373588562\n",
      "Batch 25. Loss 0.02328125387430191\n",
      "Batch 26. Loss 0.02152642048895359\n",
      "Batch 27. Loss 0.019916148856282234\n",
      "Batch 28. Loss 0.014527395367622375\n",
      "Current Validation AP Mean DC 45.02439341 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 31...\n",
      "\n",
      "Epoch: 31 Train loss: 0.006574876140803099, 1.1% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.006564803887158632, 12.6% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.006637916900217533, 24.1% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.00688869459554553, 35.6% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.007077629677951336, 47.1% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.007144385017454624, 58.6% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.007635490968823433, 70.1% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.006364088971167803, 81.6% complete\n",
      "..........\n",
      "Epoch: 31 Train loss: 0.006837903056293726, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 31...\n",
      "Batch 0. Loss 0.021305326372385025\n",
      "Batch 1. Loss 0.013435853645205498\n",
      "Batch 2. Loss 0.01873057335615158\n",
      "Batch 3. Loss 0.015533256344497204\n",
      "Batch 4. Loss 0.018162239342927933\n",
      "Batch 5. Loss 0.019673839211463928\n",
      "Batch 6. Loss 0.016185030341148376\n",
      "Batch 7. Loss 0.01614254340529442\n",
      "Batch 8. Loss 0.0152414096519351\n",
      "Batch 9. Loss 0.021252164617180824\n",
      "Batch 10. Loss 0.019728899002075195\n",
      "Batch 11. Loss 0.01752607338130474\n",
      "Batch 12. Loss 0.01918010041117668\n",
      "Batch 13. Loss 0.018720021471381187\n",
      "Batch 14. Loss 0.013803588226437569\n",
      "Batch 15. Loss 0.01598130166530609\n",
      "Batch 16. Loss 0.017271237447857857\n",
      "Batch 17. Loss 0.014795917086303234\n",
      "Batch 18. Loss 0.016116993501782417\n",
      "Batch 19. Loss 0.017273204401135445\n",
      "Batch 20. Loss 0.01767800562083721\n",
      "Batch 21. Loss 0.016486993059515953\n",
      "Batch 22. Loss 0.015832427889108658\n",
      "Batch 23. Loss 0.0178438238799572\n",
      "Batch 24. Loss 0.017169104889035225\n",
      "Batch 25. Loss 0.012646840885281563\n",
      "Batch 26. Loss 0.013804580084979534\n",
      "Batch 27. Loss 0.01689697615802288\n",
      "Batch 28. Loss 0.02115817926824093\n",
      "Current Validation AP Mean DC 45.02660852 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 32...\n",
      "\n",
      "Epoch: 32 Train loss: 0.005796036683022976, 1.1% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.006284728646278381, 12.6% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.005979199428111315, 24.1% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.007465482223778963, 35.6% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.005479971878230572, 47.1% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.005669938866049051, 58.6% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.006062492728233337, 70.1% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.005425856914371252, 81.6% complete\n",
      "..........\n",
      "Epoch: 32 Train loss: 0.005979906301945448, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 32...\n",
      "Batch 0. Loss 0.01422959566116333\n",
      "Batch 1. Loss 0.014117184095084667\n",
      "Batch 2. Loss 0.01864897646009922\n",
      "Batch 3. Loss 0.016909640282392502\n",
      "Batch 4. Loss 0.01727108471095562\n",
      "Batch 5. Loss 0.014727410860359669\n",
      "Batch 6. Loss 0.018189840018749237\n",
      "Batch 7. Loss 0.01743798889219761\n",
      "Batch 8. Loss 0.02072307839989662\n",
      "Batch 9. Loss 0.01695225201547146\n",
      "Batch 10. Loss 0.014043108560144901\n",
      "Batch 11. Loss 0.014987798407673836\n",
      "Batch 12. Loss 0.01551904622465372\n",
      "Batch 13. Loss 0.020168328657746315\n",
      "Batch 14. Loss 0.016630716621875763\n",
      "Batch 15. Loss 0.017787933349609375\n",
      "Batch 16. Loss 0.015205314382910728\n",
      "Batch 17. Loss 0.012118488550186157\n",
      "Batch 18. Loss 0.01765626296401024\n",
      "Batch 19. Loss 0.01660604402422905\n",
      "Batch 20. Loss 0.01811140403151512\n",
      "Batch 21. Loss 0.020127467811107635\n",
      "Batch 22. Loss 0.019062936305999756\n",
      "Batch 23. Loss 0.01635645516216755\n",
      "Batch 24. Loss 0.0172302033752203\n",
      "Batch 25. Loss 0.01622900739312172\n",
      "Batch 26. Loss 0.017383160069584846\n",
      "Batch 27. Loss 0.019293975085020065\n",
      "Batch 28. Loss 0.023959940299391747\n",
      "Epoch 00033: reducing learning rate of group 0 to 2.0000e-08.\n",
      "Current Validation AP Mean DC 45.02052201 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 33...\n",
      "\n",
      "Epoch: 33 Train loss: 0.006255958694964647, 1.1% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.007294024806469679, 12.6% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.006204226054251194, 24.1% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.006793007254600525, 35.6% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.005463262554258108, 47.1% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.005285368766635656, 58.6% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.007414261344820261, 70.1% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.007001481484621763, 81.6% complete\n",
      "..........\n",
      "Epoch: 33 Train loss: 0.007824539206922054, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 33...\n",
      "Batch 0. Loss 0.012281185947358608\n",
      "Batch 1. Loss 0.01781962811946869\n",
      "Batch 2. Loss 0.01936924085021019\n",
      "Batch 3. Loss 0.01351239811629057\n",
      "Batch 4. Loss 0.016922758892178535\n",
      "Batch 5. Loss 0.014807977713644505\n",
      "Batch 6. Loss 0.01997249945998192\n",
      "Batch 7. Loss 0.018946021795272827\n",
      "Batch 8. Loss 0.017502643167972565\n",
      "Batch 9. Loss 0.023833608254790306\n",
      "Batch 10. Loss 0.01345872413367033\n",
      "Batch 11. Loss 0.01609930396080017\n",
      "Batch 12. Loss 0.017113037407398224\n",
      "Batch 13. Loss 0.018274135887622833\n",
      "Batch 14. Loss 0.01956583559513092\n",
      "Batch 15. Loss 0.015731312334537506\n",
      "Batch 16. Loss 0.017254766076803207\n",
      "Batch 17. Loss 0.01983393169939518\n",
      "Batch 18. Loss 0.018389590084552765\n",
      "Batch 19. Loss 0.016223641112446785\n",
      "Batch 20. Loss 0.017258288338780403\n",
      "Batch 21. Loss 0.015801722183823586\n",
      "Batch 22. Loss 0.014066283591091633\n",
      "Batch 23. Loss 0.017029525712132454\n",
      "Batch 24. Loss 0.016403276473283768\n",
      "Batch 25. Loss 0.014081628993153572\n",
      "Batch 26. Loss 0.01727179065346718\n",
      "Batch 27. Loss 0.016773950308561325\n",
      "Batch 28. Loss 0.019896885380148888\n",
      "Current Validation AP Mean DC 45.0207139 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 34...\n",
      "\n",
      "Epoch: 34 Train loss: 0.0074968403205275536, 1.1% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.007607640698552132, 12.6% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.006418068427592516, 24.1% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.006693671457469463, 35.6% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.00800725445151329, 47.1% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.005587588995695114, 58.6% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.005916284397244453, 70.1% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.006228313315659761, 81.6% complete\n",
      "..........\n",
      "Epoch: 34 Train loss: 0.006266735959798098, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 34...\n",
      "Batch 0. Loss 0.012055606581270695\n",
      "Batch 1. Loss 0.01575326733291149\n",
      "Batch 2. Loss 0.01579362340271473\n",
      "Batch 3. Loss 0.019778557121753693\n",
      "Batch 4. Loss 0.017644762992858887\n",
      "Batch 5. Loss 0.015219911001622677\n",
      "Batch 6. Loss 0.0172041654586792\n",
      "Batch 7. Loss 0.017270293086767197\n",
      "Batch 8. Loss 0.016018642112612724\n",
      "Batch 9. Loss 0.01869313418865204\n",
      "Batch 10. Loss 0.013340088538825512\n",
      "Batch 11. Loss 0.01692296750843525\n",
      "Batch 12. Loss 0.0162630844861269\n",
      "Batch 13. Loss 0.014704205095767975\n",
      "Batch 14. Loss 0.015204750001430511\n",
      "Batch 15. Loss 0.02386939339339733\n",
      "Batch 16. Loss 0.017290914431214333\n",
      "Batch 17. Loss 0.017340131103992462\n",
      "Batch 18. Loss 0.018411030992865562\n",
      "Batch 19. Loss 0.017002034932374954\n",
      "Batch 20. Loss 0.015918049961328506\n",
      "Batch 21. Loss 0.016959160566329956\n",
      "Batch 22. Loss 0.015548096038401127\n",
      "Batch 23. Loss 0.020005330443382263\n",
      "Batch 24. Loss 0.013498138636350632\n",
      "Batch 25. Loss 0.01971173658967018\n",
      "Batch 26. Loss 0.01995937153697014\n",
      "Batch 27. Loss 0.018692905083298683\n",
      "Batch 28. Loss 0.01893032342195511\n",
      "Current Validation AP Mean DC 45.0198836 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 35...\n",
      "\n",
      "Epoch: 35 Train loss: 0.006418221164494753, 1.1% complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Epoch: 35 Train loss: 0.005598034244030714, 12.6% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.006181231699883938, 24.1% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.006694058421999216, 35.6% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.006951282266527414, 47.1% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.006942233070731163, 58.6% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.00666380301117897, 70.1% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.0069564939476549625, 81.6% complete\n",
      "..........\n",
      "Epoch: 35 Train loss: 0.007180798798799515, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 35...\n",
      "Batch 0. Loss 0.018354106694459915\n",
      "Batch 1. Loss 0.012289020232856274\n",
      "Batch 2. Loss 0.02116401493549347\n",
      "Batch 3. Loss 0.022917116060853004\n",
      "Batch 4. Loss 0.01538698747754097\n",
      "Batch 5. Loss 0.012789459899067879\n",
      "Batch 6. Loss 0.018497906625270844\n",
      "Batch 7. Loss 0.01855531521141529\n",
      "Batch 8. Loss 0.018161233514547348\n",
      "Batch 9. Loss 0.017990950495004654\n",
      "Batch 10. Loss 0.014321698807179928\n",
      "Batch 11. Loss 0.013848003931343555\n",
      "Batch 12. Loss 0.015813028439879417\n",
      "Batch 13. Loss 0.017258942127227783\n",
      "Batch 14. Loss 0.017302436754107475\n",
      "Batch 15. Loss 0.017888614907860756\n",
      "Batch 16. Loss 0.014630147255957127\n",
      "Batch 17. Loss 0.019129086285829544\n",
      "Batch 18. Loss 0.020461272448301315\n",
      "Batch 19. Loss 0.016988076269626617\n",
      "Batch 20. Loss 0.017005356028676033\n",
      "Batch 21. Loss 0.017085298895835876\n",
      "Batch 22. Loss 0.01851607859134674\n",
      "Batch 23. Loss 0.017753593623638153\n",
      "Batch 24. Loss 0.01474058162420988\n",
      "Batch 25. Loss 0.01708514243364334\n",
      "Batch 26. Loss 0.01575293019413948\n",
      "Batch 27. Loss 0.016219938173890114\n",
      "Batch 28. Loss 0.014942157082259655\n",
      "Current Validation AP Mean DC 45.02056462 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 36...\n",
      "\n",
      "Epoch: 36 Train loss: 0.007402285002171993, 1.1% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.006579640321433544, 12.6% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.0061868890188634396, 24.1% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.006005724426358938, 35.6% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.0055338721722364426, 47.1% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.0055239032953977585, 58.6% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.007170575205236673, 70.1% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.0066736978478729725, 81.6% complete\n",
      "..........\n",
      "Epoch: 36 Train loss: 0.006787447724491358, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 36...\n",
      "Batch 0. Loss 0.020194560289382935\n",
      "Batch 1. Loss 0.016217701137065887\n",
      "Batch 2. Loss 0.013034604489803314\n",
      "Batch 3. Loss 0.02243485115468502\n",
      "Batch 4. Loss 0.0189508143812418\n",
      "Batch 5. Loss 0.020576773211359978\n",
      "Batch 6. Loss 0.01761295273900032\n",
      "Batch 7. Loss 0.016591954976320267\n",
      "Batch 8. Loss 0.015929004177451134\n",
      "Batch 9. Loss 0.018164509907364845\n",
      "Batch 10. Loss 0.020607685670256615\n",
      "Batch 11. Loss 0.02011382207274437\n",
      "Batch 12. Loss 0.015342985279858112\n",
      "Batch 13. Loss 0.015745142474770546\n",
      "Batch 14. Loss 0.01739591173827648\n",
      "Batch 15. Loss 0.013314737938344479\n",
      "Batch 16. Loss 0.0234487596899271\n",
      "Batch 17. Loss 0.016332851722836494\n",
      "Batch 18. Loss 0.013121608644723892\n",
      "Batch 19. Loss 0.013743892312049866\n",
      "Batch 20. Loss 0.017850445583462715\n",
      "Batch 21. Loss 0.014171711169183254\n",
      "Batch 22. Loss 0.012303555384278297\n",
      "Batch 23. Loss 0.014434575103223324\n",
      "Batch 24. Loss 0.016373582184314728\n",
      "Batch 25. Loss 0.016550617292523384\n",
      "Batch 26. Loss 0.017212161794304848\n",
      "Batch 27. Loss 0.017997749149799347\n",
      "Batch 28. Loss 0.019523194059729576\n",
      "Current Validation AP Mean DC 45.02026066 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 37...\n",
      "\n",
      "Epoch: 37 Train loss: 0.006970781832933426, 1.1% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.005581109784543514, 12.6% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.006765514146536589, 24.1% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.006112885661423206, 35.6% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.006925808731466532, 47.1% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.005456306505948305, 58.6% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.006805991753935814, 70.1% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.005577359814196825, 81.6% complete\n",
      "..........\n",
      "Epoch: 37 Train loss: 0.008065791800618172, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 37...\n",
      "Batch 0. Loss 0.017965711653232574\n",
      "Batch 1. Loss 0.014515168033540249\n",
      "Batch 2. Loss 0.015414823777973652\n",
      "Batch 3. Loss 0.014453396201133728\n",
      "Batch 4. Loss 0.01742422580718994\n",
      "Batch 5. Loss 0.019575100392103195\n",
      "Batch 6. Loss 0.020215049386024475\n",
      "Batch 7. Loss 0.0156848207116127\n",
      "Batch 8. Loss 0.019577015191316605\n",
      "Batch 9. Loss 0.01886449009180069\n",
      "Batch 10. Loss 0.015428136102855206\n",
      "Batch 11. Loss 0.01451740600168705\n",
      "Batch 12. Loss 0.01737544685602188\n",
      "Batch 13. Loss 0.016910137608647346\n",
      "Batch 14. Loss 0.014120145700871944\n",
      "Batch 15. Loss 0.01627025380730629\n",
      "Batch 16. Loss 0.01660512201488018\n",
      "Batch 17. Loss 0.020745012909173965\n",
      "Batch 18. Loss 0.01946391724050045\n",
      "Batch 19. Loss 0.018423719331622124\n",
      "Batch 20. Loss 0.01389810536056757\n",
      "Batch 21. Loss 0.019011693075299263\n",
      "Batch 22. Loss 0.015023339539766312\n",
      "Batch 23. Loss 0.01733359694480896\n",
      "Batch 24. Loss 0.014879035763442516\n",
      "Batch 25. Loss 0.019458450376987457\n",
      "Batch 26. Loss 0.01722128316760063\n",
      "Batch 27. Loss 0.016850098967552185\n",
      "Batch 28. Loss 0.016435813158750534\n",
      "Current Validation AP Mean DC 45.02044607 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 38...\n",
      "\n",
      "Epoch: 38 Train loss: 0.00570677500218153, 1.1% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.006317058112472296, 12.6% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.006111855152994394, 24.1% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.006196487229317427, 35.6% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.008184224367141724, 47.1% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.008004693314433098, 58.6% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.0071107009425759315, 70.1% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.0060445996932685375, 81.6% complete\n",
      "..........\n",
      "Epoch: 38 Train loss: 0.007176268380135298, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 38...\n",
      "Batch 0. Loss 0.014638742431998253\n",
      "Batch 1. Loss 0.01896926574409008\n",
      "Batch 2. Loss 0.01971534639596939\n",
      "Batch 3. Loss 0.01930001564323902\n",
      "Batch 4. Loss 0.017319206148386\n",
      "Batch 5. Loss 0.015376449562609196\n",
      "Batch 6. Loss 0.017272768542170525\n",
      "Batch 7. Loss 0.01957964338362217\n",
      "Batch 8. Loss 0.017635978758335114\n",
      "Batch 9. Loss 0.014569001272320747\n",
      "Batch 10. Loss 0.019349340349435806\n",
      "Batch 11. Loss 0.020981352776288986\n",
      "Batch 12. Loss 0.014016090892255306\n",
      "Batch 13. Loss 0.016408033668994904\n",
      "Batch 14. Loss 0.01559616532176733\n",
      "Batch 15. Loss 0.014546656981110573\n",
      "Batch 16. Loss 0.014947142452001572\n",
      "Batch 17. Loss 0.0195096917450428\n",
      "Batch 18. Loss 0.01748393289744854\n",
      "Batch 19. Loss 0.01726970076560974\n",
      "Batch 20. Loss 0.016382655128836632\n",
      "Batch 21. Loss 0.015588013455271721\n",
      "Batch 22. Loss 0.01886351965367794\n",
      "Batch 23. Loss 0.013985924422740936\n",
      "Batch 24. Loss 0.019678087905049324\n",
      "Batch 25. Loss 0.016454162076115608\n",
      "Batch 26. Loss 0.015840930864214897\n",
      "Batch 27. Loss 0.014911952428519726\n",
      "Batch 28. Loss 0.018666038289666176\n",
      "Epoch 00039: reducing learning rate of group 0 to 2.0000e-09.\n",
      "Current Validation AP Mean DC 45.0201389 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 39...\n",
      "\n",
      "Epoch: 39 Train loss: 0.006487562321126461, 1.1% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.006301150657236576, 12.6% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.005298276897519827, 24.1% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.006986901629716158, 35.6% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.006611264776438475, 47.1% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.006445526611059904, 58.6% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.005297123920172453, 70.1% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.0070024519227445126, 81.6% complete\n",
      "..........\n",
      "Epoch: 39 Train loss: 0.006648608949035406, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 39...\n",
      "Batch 0. Loss 0.01692921668291092\n",
      "Batch 1. Loss 0.013552156277000904\n",
      "Batch 2. Loss 0.016781151294708252\n",
      "Batch 3. Loss 0.015148431994020939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4. Loss 0.015978269279003143\n",
      "Batch 5. Loss 0.01734272390604019\n",
      "Batch 6. Loss 0.01702059991657734\n",
      "Batch 7. Loss 0.015983060002326965\n",
      "Batch 8. Loss 0.018035979941487312\n",
      "Batch 9. Loss 0.016940472647547722\n",
      "Batch 10. Loss 0.015885332599282265\n",
      "Batch 11. Loss 0.01746520958840847\n",
      "Batch 12. Loss 0.01768764853477478\n",
      "Batch 13. Loss 0.016749363392591476\n",
      "Batch 14. Loss 0.018423564732074738\n",
      "Batch 15. Loss 0.01599244773387909\n",
      "Batch 16. Loss 0.017256606370210648\n",
      "Batch 17. Loss 0.015952015295624733\n",
      "Batch 18. Loss 0.016334695741534233\n",
      "Batch 19. Loss 0.018865151330828667\n",
      "Batch 20. Loss 0.017978642135858536\n",
      "Batch 21. Loss 0.020965764299035072\n",
      "Batch 22. Loss 0.018160579726099968\n",
      "Batch 23. Loss 0.017856301739811897\n",
      "Batch 24. Loss 0.0156431682407856\n",
      "Batch 25. Loss 0.01747976243495941\n",
      "Batch 26. Loss 0.019026516005396843\n",
      "Batch 27. Loss 0.01447012834250927\n",
      "Batch 28. Loss 0.01927766390144825\n",
      "Current Validation AP Mean DC 45.02013713 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 40...\n",
      "\n",
      "Epoch: 40 Train loss: 0.006196398753672838, 1.1% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.007259065750986338, 12.6% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.0056161400862038136, 24.1% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.0047686356119811535, 35.6% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.0052071562968194485, 47.1% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.005744844675064087, 58.6% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.006372994743287563, 70.1% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.0065039522014558315, 81.6% complete\n",
      "..........\n",
      "Epoch: 40 Train loss: 0.006369581446051598, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 40...\n",
      "Batch 0. Loss 0.013218792155385017\n",
      "Batch 1. Loss 0.01604469120502472\n",
      "Batch 2. Loss 0.017168952152132988\n",
      "Batch 3. Loss 0.017520321533083916\n",
      "Batch 4. Loss 0.015834450721740723\n",
      "Batch 5. Loss 0.016385788097977638\n",
      "Batch 6. Loss 0.017704831436276436\n",
      "Batch 7. Loss 0.016593048349022865\n",
      "Batch 8. Loss 0.016506880521774292\n",
      "Batch 9. Loss 0.015825629234313965\n",
      "Batch 10. Loss 0.019225554540753365\n",
      "Batch 11. Loss 0.016853906214237213\n",
      "Batch 12. Loss 0.017439324408769608\n",
      "Batch 13. Loss 0.016551418229937553\n",
      "Batch 14. Loss 0.01889297179877758\n",
      "Batch 15. Loss 0.01660836860537529\n",
      "Batch 16. Loss 0.017393728718161583\n",
      "Batch 17. Loss 0.018916863948106766\n",
      "Batch 18. Loss 0.012826880440115929\n",
      "Batch 19. Loss 0.020848948508501053\n",
      "Batch 20. Loss 0.017338473349809647\n",
      "Batch 21. Loss 0.020525839179754257\n",
      "Batch 22. Loss 0.01892167516052723\n",
      "Batch 23. Loss 0.01917843520641327\n",
      "Batch 24. Loss 0.01671263575553894\n",
      "Batch 25. Loss 0.014886709861457348\n",
      "Batch 26. Loss 0.013996278867125511\n",
      "Batch 27. Loss 0.017110055312514305\n",
      "Batch 28. Loss 0.01686273328959942\n",
      "Current Validation AP Mean DC 45.01913979 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 41...\n",
      "\n",
      "Epoch: 41 Train loss: 0.006699807941913605, 1.1% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.007267433684319258, 12.6% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.0063324375078082085, 24.1% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.007048814091831446, 35.6% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.0054877884685993195, 47.1% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.006677683442831039, 58.6% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.005707853939384222, 70.1% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.005958206951618195, 81.6% complete\n",
      "..........\n",
      "Epoch: 41 Train loss: 0.00555057916790247, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 41...\n",
      "Batch 0. Loss 0.012831739149987698\n",
      "Batch 1. Loss 0.01750648021697998\n",
      "Batch 2. Loss 0.015488442964851856\n",
      "Batch 3. Loss 0.012814466841518879\n",
      "Batch 4. Loss 0.018469544127583504\n",
      "Batch 5. Loss 0.01848972775042057\n",
      "Batch 6. Loss 0.01852414384484291\n",
      "Batch 7. Loss 0.016529396176338196\n",
      "Batch 8. Loss 0.017048917710781097\n",
      "Batch 9. Loss 0.018154656514525414\n",
      "Batch 10. Loss 0.014439825899899006\n",
      "Batch 11. Loss 0.017815152183175087\n",
      "Batch 12. Loss 0.016858208924531937\n",
      "Batch 13. Loss 0.017031552270054817\n",
      "Batch 14. Loss 0.01708928309381008\n",
      "Batch 15. Loss 0.01960223913192749\n",
      "Batch 16. Loss 0.019291114062070847\n",
      "Batch 17. Loss 0.015395197086036205\n",
      "Batch 18. Loss 0.015711579471826553\n",
      "Batch 19. Loss 0.0176467914134264\n",
      "Batch 20. Loss 0.01596452109515667\n",
      "Batch 21. Loss 0.013285333290696144\n",
      "Batch 22. Loss 0.01990034058690071\n",
      "Batch 23. Loss 0.018705176189541817\n",
      "Batch 24. Loss 0.015746185556054115\n",
      "Batch 25. Loss 0.018056325614452362\n",
      "Batch 26. Loss 0.01684008352458477\n",
      "Batch 27. Loss 0.018554994836449623\n",
      "Batch 28. Loss 0.023771267384290695\n",
      "Current Validation AP Mean DC 45.02057854 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 42...\n",
      "\n",
      "Epoch: 42 Train loss: 0.005410884507000446, 1.1% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.0064691416919231415, 12.6% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.005891137756407261, 24.1% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.006682833191007376, 35.6% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.005986322183161974, 47.1% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.00608496880158782, 58.6% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.006790476385504007, 70.1% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.006566836964339018, 81.6% complete\n",
      "..........\n",
      "Epoch: 42 Train loss: 0.008415703661739826, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 42...\n",
      "Batch 0. Loss 0.01445735152810812\n",
      "Batch 1. Loss 0.020029166713356972\n",
      "Batch 2. Loss 0.015024062246084213\n",
      "Batch 3. Loss 0.018498314544558525\n",
      "Batch 4. Loss 0.016867373138666153\n",
      "Batch 5. Loss 0.014577528461813927\n",
      "Batch 6. Loss 0.020044906064867973\n",
      "Batch 7. Loss 0.019208408892154694\n",
      "Batch 8. Loss 0.01806442253291607\n",
      "Batch 9. Loss 0.013245020061731339\n",
      "Batch 10. Loss 0.01807653158903122\n",
      "Batch 11. Loss 0.020357193425297737\n",
      "Batch 12. Loss 0.01334089133888483\n",
      "Batch 13. Loss 0.01611870341002941\n",
      "Batch 14. Loss 0.015417639166116714\n",
      "Batch 15. Loss 0.016706641763448715\n",
      "Batch 16. Loss 0.015701573342084885\n",
      "Batch 17. Loss 0.015563840977847576\n",
      "Batch 18. Loss 0.015596039593219757\n",
      "Batch 19. Loss 0.015134554356336594\n",
      "Batch 20. Loss 0.02130698785185814\n",
      "Batch 21. Loss 0.014805801212787628\n",
      "Batch 22. Loss 0.017471028491854668\n",
      "Batch 23. Loss 0.02234167791903019\n",
      "Batch 24. Loss 0.014572368934750557\n",
      "Batch 25. Loss 0.018154973164200783\n",
      "Batch 26. Loss 0.021578477695584297\n",
      "Batch 27. Loss 0.015585973858833313\n",
      "Batch 28. Loss 0.015128317289054394\n",
      "Current Validation AP Mean DC 45.0192143 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 43...\n",
      "\n",
      "Epoch: 43 Train loss: 0.007668414153158665, 1.1% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.005207330919802189, 12.6% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.007133175618946552, 24.1% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.005851922556757927, 35.6% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.00737876258790493, 47.1% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.0063417875207960606, 58.6% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.006424230523407459, 70.1% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.005860797595232725, 81.6% complete\n",
      "..........\n",
      "Epoch: 43 Train loss: 0.006421910598874092, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 43...\n",
      "Batch 0. Loss 0.0180576853454113\n",
      "Batch 1. Loss 0.01586904376745224\n",
      "Batch 2. Loss 0.014742114581167698\n",
      "Batch 3. Loss 0.018291883170604706\n",
      "Batch 4. Loss 0.014291088096797466\n",
      "Batch 5. Loss 0.019710112363100052\n",
      "Batch 6. Loss 0.02098746784031391\n",
      "Batch 7. Loss 0.016529547050595284\n",
      "Batch 8. Loss 0.017390400171279907\n",
      "Batch 9. Loss 0.019935816526412964\n",
      "Batch 10. Loss 0.01614726334810257\n",
      "Batch 11. Loss 0.017998499795794487\n",
      "Batch 12. Loss 0.012975306250154972\n",
      "Batch 13. Loss 0.01496853120625019\n",
      "Batch 14. Loss 0.01944529265165329\n",
      "Batch 15. Loss 0.014373990707099438\n",
      "Batch 16. Loss 0.016149897128343582\n",
      "Batch 17. Loss 0.015310170128941536\n",
      "Batch 18. Loss 0.016852859407663345\n",
      "Batch 19. Loss 0.01940113864839077\n",
      "Batch 20. Loss 0.016128376126289368\n",
      "Batch 21. Loss 0.015916191041469574\n",
      "Batch 22. Loss 0.022429177537560463\n",
      "Batch 23. Loss 0.013879796490073204\n",
      "Batch 24. Loss 0.016806140542030334\n",
      "Batch 25. Loss 0.020526761189103127\n",
      "Batch 26. Loss 0.01426194328814745\n",
      "Batch 27. Loss 0.01902288943529129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28. Loss 0.013961070217192173\n",
      "Current Validation AP Mean DC 45.01944718 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 44...\n",
      "\n",
      "Epoch: 44 Train loss: 0.006223106756806374, 1.1% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.007634714711457491, 12.6% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.006018831394612789, 24.1% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.007811223156750202, 35.6% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.006596884690225124, 47.1% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.0069845207035541534, 58.6% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.006002056412398815, 70.1% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.006698923651129007, 81.6% complete\n",
      "..........\n",
      "Epoch: 44 Train loss: 0.0056399148888885975, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 44...\n",
      "Batch 0. Loss 0.015684084966778755\n",
      "Batch 1. Loss 0.01954255998134613\n",
      "Batch 2. Loss 0.016407761722803116\n",
      "Batch 3. Loss 0.018732866272330284\n",
      "Batch 4. Loss 0.01855270005762577\n",
      "Batch 5. Loss 0.017455946654081345\n",
      "Batch 6. Loss 0.015013933181762695\n",
      "Batch 7. Loss 0.019927507266402245\n",
      "Batch 8. Loss 0.01809409260749817\n",
      "Batch 9. Loss 0.012947785668075085\n",
      "Batch 10. Loss 0.014716875739395618\n",
      "Batch 11. Loss 0.016559721902012825\n",
      "Batch 12. Loss 0.020042533054947853\n",
      "Batch 13. Loss 0.01764143444597721\n",
      "Batch 14. Loss 0.018349673599004745\n",
      "Batch 15. Loss 0.01903712935745716\n",
      "Batch 16. Loss 0.01371661014854908\n",
      "Batch 17. Loss 0.01551800686866045\n",
      "Batch 18. Loss 0.019265154376626015\n",
      "Batch 19. Loss 0.01718381606042385\n",
      "Batch 20. Loss 0.020363586023449898\n",
      "Batch 21. Loss 0.014011163264513016\n",
      "Batch 22. Loss 0.015193053521215916\n",
      "Batch 23. Loss 0.013331159017980099\n",
      "Batch 24. Loss 0.021191611886024475\n",
      "Batch 25. Loss 0.017430849373340607\n",
      "Batch 26. Loss 0.01479832548648119\n",
      "Batch 27. Loss 0.01624204032123089\n",
      "Batch 28. Loss 0.017053566873073578\n",
      "Epoch 00045: reducing learning rate of group 0 to 2.0000e-10.\n",
      "Current Validation AP Mean DC 45.02002852 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 45...\n",
      "\n",
      "Epoch: 45 Train loss: 0.006470080465078354, 1.1% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.00542836170643568, 12.6% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.005285137332975864, 24.1% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.007157777901738882, 35.6% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.0051127769984304905, 47.1% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.005035989917814732, 58.6% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.006142923142760992, 70.1% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.006909038405865431, 81.6% complete\n",
      "..........\n",
      "Epoch: 45 Train loss: 0.005415739957243204, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 45...\n",
      "Batch 0. Loss 0.012932073324918747\n",
      "Batch 1. Loss 0.01992042362689972\n",
      "Batch 2. Loss 0.015846025198698044\n",
      "Batch 3. Loss 0.013678724877536297\n",
      "Batch 4. Loss 0.014886369928717613\n",
      "Batch 5. Loss 0.018074631690979004\n",
      "Batch 6. Loss 0.015422724187374115\n",
      "Batch 7. Loss 0.017110908403992653\n",
      "Batch 8. Loss 0.015189495868980885\n",
      "Batch 9. Loss 0.01523048710078001\n",
      "Batch 10. Loss 0.01980426535010338\n",
      "Batch 11. Loss 0.018592234700918198\n",
      "Batch 12. Loss 0.016241448000073433\n",
      "Batch 13. Loss 0.019883710891008377\n",
      "Batch 14. Loss 0.020707644522190094\n",
      "Batch 15. Loss 0.016382327303290367\n",
      "Batch 16. Loss 0.017232483252882957\n",
      "Batch 17. Loss 0.01651780866086483\n",
      "Batch 18. Loss 0.018889104947447777\n",
      "Batch 19. Loss 0.016750525683164597\n",
      "Batch 20. Loss 0.01608138345181942\n",
      "Batch 21. Loss 0.013617921620607376\n",
      "Batch 22. Loss 0.017845576629042625\n",
      "Batch 23. Loss 0.018073054030537605\n",
      "Batch 24. Loss 0.014691106043756008\n",
      "Batch 25. Loss 0.017023401334881783\n",
      "Batch 26. Loss 0.018997032195329666\n",
      "Batch 27. Loss 0.024486159905791283\n",
      "Batch 28. Loss 0.010315115563571453\n",
      "Current Validation AP Mean DC 45.02032806 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 46...\n",
      "\n",
      "Epoch: 46 Train loss: 0.007322162389755249, 1.1% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.007135964930057526, 12.6% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.006511211395263672, 24.1% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.006376439705491066, 35.6% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.006858320906758308, 47.1% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.005935695953667164, 58.6% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.005085973534733057, 70.1% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.006682358682155609, 81.6% complete\n",
      "..........\n",
      "Epoch: 46 Train loss: 0.0053042760118842125, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 46...\n",
      "Batch 0. Loss 0.017878251150250435\n",
      "Batch 1. Loss 0.013559586368501186\n",
      "Batch 2. Loss 0.014321357011795044\n",
      "Batch 3. Loss 0.018026059493422508\n",
      "Batch 4. Loss 0.01688222400844097\n",
      "Batch 5. Loss 0.022141695022583008\n",
      "Batch 6. Loss 0.015771595761179924\n",
      "Batch 7. Loss 0.015408338978886604\n",
      "Batch 8. Loss 0.016264742240309715\n",
      "Batch 9. Loss 0.015938758850097656\n",
      "Batch 10. Loss 0.016849536448717117\n",
      "Batch 11. Loss 0.01556326448917389\n",
      "Batch 12. Loss 0.01613711565732956\n",
      "Batch 13. Loss 0.021785106509923935\n",
      "Batch 14. Loss 0.01922640949487686\n",
      "Batch 15. Loss 0.014506637118756771\n",
      "Batch 16. Loss 0.013783236034214497\n",
      "Batch 17. Loss 0.015488075092434883\n",
      "Batch 18. Loss 0.016383491456508636\n",
      "Batch 19. Loss 0.015902290120720863\n",
      "Batch 20. Loss 0.016284404322504997\n",
      "Batch 21. Loss 0.018091006204485893\n",
      "Batch 22. Loss 0.015950219705700874\n",
      "Batch 23. Loss 0.016833636909723282\n",
      "Batch 24. Loss 0.02340218797326088\n",
      "Batch 25. Loss 0.019695613533258438\n",
      "Batch 26. Loss 0.018087148666381836\n",
      "Batch 27. Loss 0.01632864773273468\n",
      "Batch 28. Loss 0.01803571917116642\n",
      "Current Validation AP Mean DC 45.02030709 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 47...\n",
      "\n",
      "Epoch: 47 Train loss: 0.005559293087571859, 1.1% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.0065110367722809315, 12.6% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.006993759889155626, 24.1% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.0072951773181557655, 35.6% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.005412539467215538, 47.1% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.005818186327815056, 58.6% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.00765960430726409, 70.1% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.006690599489957094, 81.6% complete\n",
      "..........\n",
      "Epoch: 47 Train loss: 0.007104477379471064, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 47...\n",
      "Batch 0. Loss 0.013905394822359085\n",
      "Batch 1. Loss 0.014801316894590855\n",
      "Batch 2. Loss 0.013728830963373184\n",
      "Batch 3. Loss 0.01873065158724785\n",
      "Batch 4. Loss 0.01448750589042902\n",
      "Batch 5. Loss 0.019210675731301308\n",
      "Batch 6. Loss 0.019419556483626366\n",
      "Batch 7. Loss 0.015616298653185368\n",
      "Batch 8. Loss 0.01807965151965618\n",
      "Batch 9. Loss 0.017939288169145584\n",
      "Batch 10. Loss 0.014050109311938286\n",
      "Batch 11. Loss 0.01804300956428051\n",
      "Batch 12. Loss 0.01670827530324459\n",
      "Batch 13. Loss 0.015507622621953487\n",
      "Batch 14. Loss 0.018466565757989883\n",
      "Batch 15. Loss 0.016863103955984116\n",
      "Batch 16. Loss 0.014869865030050278\n",
      "Batch 17. Loss 0.01588830165565014\n",
      "Batch 18. Loss 0.01684597134590149\n",
      "Batch 19. Loss 0.013503513298928738\n",
      "Batch 20. Loss 0.022305011749267578\n",
      "Batch 21. Loss 0.023190664127469063\n",
      "Batch 22. Loss 0.01746574603021145\n",
      "Batch 23. Loss 0.015975715592503548\n",
      "Batch 24. Loss 0.021673761308193207\n",
      "Batch 25. Loss 0.014263887889683247\n",
      "Batch 26. Loss 0.015507664531469345\n",
      "Batch 27. Loss 0.018093295395374298\n",
      "Batch 28. Loss 0.02091786451637745\n",
      "Current Validation AP Mean DC 45.01989637 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 48...\n",
      "\n",
      "Epoch: 48 Train loss: 0.006821656599640846, 1.1% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.0051350062713027, 12.6% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.006025016773492098, 24.1% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.006859411019831896, 35.6% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.006180725991725922, 47.1% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.007146002724766731, 58.6% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.00662360480055213, 70.1% complete\n",
      "..........\n",
      "Epoch: 48 Train loss: 0.00607338547706604, 81.6% complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "Epoch: 48 Train loss: 0.006096189375966787, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 48...\n",
      "Batch 0. Loss 0.016181794926524162\n",
      "Batch 1. Loss 0.018568024039268494\n",
      "Batch 2. Loss 0.014603356830775738\n",
      "Batch 3. Loss 0.018802287057042122\n",
      "Batch 4. Loss 0.019340602681040764\n",
      "Batch 5. Loss 0.016261694952845573\n",
      "Batch 6. Loss 0.014216084964573383\n",
      "Batch 7. Loss 0.01545658614486456\n",
      "Batch 8. Loss 0.018109487369656563\n",
      "Batch 9. Loss 0.018974803388118744\n",
      "Batch 10. Loss 0.016574107110500336\n",
      "Batch 11. Loss 0.013351657427847385\n",
      "Batch 12. Loss 0.017027894034981728\n",
      "Batch 13. Loss 0.01688573695719242\n",
      "Batch 14. Loss 0.017778653651475906\n",
      "Batch 15. Loss 0.017447181046009064\n",
      "Batch 16. Loss 0.022184208035469055\n",
      "Batch 17. Loss 0.015528670512139797\n",
      "Batch 18. Loss 0.01690077781677246\n",
      "Batch 19. Loss 0.020200518891215324\n",
      "Batch 20. Loss 0.015293719246983528\n",
      "Batch 21. Loss 0.021165132522583008\n",
      "Batch 22. Loss 0.01477949321269989\n",
      "Batch 23. Loss 0.01830805279314518\n",
      "Batch 24. Loss 0.01702262833714485\n",
      "Batch 25. Loss 0.017060471698641777\n",
      "Batch 26. Loss 0.015151833184063435\n",
      "Batch 27. Loss 0.014663957990705967\n",
      "Batch 28. Loss 0.015164598822593689\n",
      "Current Validation AP Mean DC 45.02037931 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "\n",
      "Training epoch 49...\n",
      "\n",
      "Epoch: 49 Train loss: 0.00678899185732007, 1.1% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.007867472246289253, 12.6% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.00554742943495512, 24.1% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.006307460833340883, 35.6% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.006395342759788036, 47.1% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.005596447736024857, 58.6% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.005500282160937786, 70.1% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.006435081362724304, 81.6% complete\n",
      "..........\n",
      "Epoch: 49 Train loss: 0.005696761887520552, 93.1% complete\n",
      ".......\n",
      "Training complete\n",
      "\n",
      "Validating epoch 49...\n",
      "Batch 0. Loss 0.01633109711110592\n",
      "Batch 1. Loss 0.01663965731859207\n",
      "Batch 2. Loss 0.017601551488041878\n",
      "Batch 3. Loss 0.016853109002113342\n",
      "Batch 4. Loss 0.014759481884539127\n",
      "Batch 5. Loss 0.012616075575351715\n",
      "Batch 6. Loss 0.01585337519645691\n",
      "Batch 7. Loss 0.017732955515384674\n",
      "Batch 8. Loss 0.020547213032841682\n",
      "Batch 9. Loss 0.016539184376597404\n",
      "Batch 10. Loss 0.01713542267680168\n",
      "Batch 11. Loss 0.018943607807159424\n",
      "Batch 12. Loss 0.01858018897473812\n",
      "Batch 13. Loss 0.017145982012152672\n",
      "Batch 14. Loss 0.02114606276154518\n",
      "Batch 15. Loss 0.017467297613620758\n",
      "Batch 16. Loss 0.01884845644235611\n",
      "Batch 17. Loss 0.014948926866054535\n",
      "Batch 18. Loss 0.02041250839829445\n",
      "Batch 19. Loss 0.014867720194160938\n",
      "Batch 20. Loss 0.016047142446041107\n",
      "Batch 21. Loss 0.01596313714981079\n",
      "Batch 22. Loss 0.013066543266177177\n",
      "Batch 23. Loss 0.015717431902885437\n",
      "Batch 24. Loss 0.017619876191020012\n",
      "Batch 25. Loss 0.01625964418053627\n",
      "Batch 26. Loss 0.019243696704506874\n",
      "Batch 27. Loss 0.017584579065442085\n",
      "Batch 28. Loss 0.018077906221151352\n",
      "Current Validation AP Mean DC 45.02017739 | Maximum Validation AP Mean DC 45.28226554\n",
      "Maximum Validation DC 46.67858658\n",
      "\n",
      "Validation complete\n",
      "Run complete. Total time: 00:18:41\n"
     ]
    }
   ],
   "source": [
    "_time_start = time.time()\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "for i in range(50):\n",
    "    train(epoch=i, model=model)\n",
    "\n",
    "_time_end = time.time()\n",
    "print(f\"Run complete. Total time: {time.strftime('%H:%M:%S', time.gmtime(_time_end - _time_start))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cc6b433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['out/basic_unet_seed1/model_epoch16.pth',\n",
       " 'out/basic_unet_seed1/model_epoch15.pth',\n",
       " 'out/basic_unet_seed1/model_epoch9.pth',\n",
       " 'out/basic_unet_seed1/model_epoch8.pth',\n",
       " 'out/basic_unet_seed1/model_epoch6.pth',\n",
       " 'out/basic_unet_seed1/model_epoch5.pth',\n",
       " 'out/basic_unet_seed1/model_epoch4.pth',\n",
       " 'out/basic_unet_seed1/model_epoch3.pth',\n",
       " 'out/basic_unet_seed1/model_epoch2.pth',\n",
       " 'out/basic_unet_seed1/model_epoch13.pth',\n",
       " 'out/basic_unet_seed1/model_epoch7.pth',\n",
       " 'out/basic_unet_seed1/model_epoch11.pth',\n",
       " 'out/basic_unet_seed1/model_epoch14.pth',\n",
       " 'out/basic_unet_seed1/model_epoch12.pth']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_list = glob(f\"out/{dirname}/*\")\n",
    "weight_list.sort(reverse=True, key=os.path.getmtime)\n",
    "weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c08ff751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out/basic_unet_seed1/model_epoch16.pth\n",
      "[ Using Seed :  71582788  ]\n",
      "hippocampus_260.nii.gz Dice 0.8978, Dice Anterior 0.8556, Dice Posterior 0.8749, Jaccard 0.8145, Sensitivity 0.8821, and Specificity 0.9982. 1.92% complete\n",
      "hippocampus_378.nii.gz Dice 0.8930, Dice Anterior 0.8589, Dice Posterior 0.8760, Jaccard 0.8067, Sensitivity 0.9039, and Specificity 0.9976. 3.85% complete\n",
      "hippocampus_321.nii.gz Dice 0.8905, Dice Anterior 0.8435, Dice Posterior 0.8420, Jaccard 0.8025, Sensitivity 0.8997, and Specificity 0.9974. 5.77% complete\n",
      "hippocampus_319.nii.gz Dice 0.8985, Dice Anterior 0.9102, Dice Posterior 0.8556, Jaccard 0.8156, Sensitivity 0.9096, and Specificity 0.9979. 7.69% complete\n",
      "hippocampus_041.nii.gz Dice 0.8362, Dice Anterior 0.8270, Dice Posterior 0.8290, Jaccard 0.7185, Sensitivity 0.8220, and Specificity 0.9962. 9.62% complete\n",
      "hippocampus_088.nii.gz Dice 0.9149, Dice Anterior 0.9128, Dice Posterior 0.8896, Jaccard 0.8431, Sensitivity 0.9162, and Specificity 0.9979. 11.54% complete\n",
      "hippocampus_087.nii.gz Dice 0.9228, Dice Anterior 0.9148, Dice Posterior 0.9177, Jaccard 0.8566, Sensitivity 0.9460, and Specificity 0.9972. 13.46% complete\n",
      "hippocampus_355.nii.gz Dice 0.8977, Dice Anterior 0.8864, Dice Posterior 0.8707, Jaccard 0.8144, Sensitivity 0.9304, and Specificity 0.9964. 15.38% complete\n",
      "hippocampus_225.nii.gz Dice 0.9060, Dice Anterior 0.8676, Dice Posterior 0.8284, Jaccard 0.8281, Sensitivity 0.8897, and Specificity 0.9986. 17.31% complete\n",
      "hippocampus_236.nii.gz Dice 0.8842, Dice Anterior 0.8842, Dice Posterior 0.8522, Jaccard 0.7924, Sensitivity 0.8540, and Specificity 0.9984. 19.23% complete\n",
      "hippocampus_084.nii.gz Dice 0.9158, Dice Anterior 0.8591, Dice Posterior 0.8672, Jaccard 0.8447, Sensitivity 0.8756, and Specificity 0.9992. 21.15% complete\n",
      "hippocampus_380.nii.gz Dice 0.9236, Dice Anterior 0.9105, Dice Posterior 0.9100, Jaccard 0.8580, Sensitivity 0.9109, and Specificity 0.9985. 23.08% complete\n",
      "hippocampus_305.nii.gz Dice 0.8209, Dice Anterior 0.7908, Dice Posterior 0.7647, Jaccard 0.6962, Sensitivity 0.8019, and Specificity 0.9966. 25.00% complete\n",
      "hippocampus_370.nii.gz Dice 0.9011, Dice Anterior 0.9071, Dice Posterior 0.8771, Jaccard 0.8199, Sensitivity 0.8952, and Specificity 0.9978. 26.92% complete\n",
      "hippocampus_165.nii.gz Dice 0.9251, Dice Anterior 0.9032, Dice Posterior 0.8999, Jaccard 0.8607, Sensitivity 0.9219, and Specificity 0.9984. 28.85% complete\n",
      "hippocampus_356.nii.gz Dice 0.8868, Dice Anterior 0.8780, Dice Posterior 0.8623, Jaccard 0.7966, Sensitivity 0.8696, and Specificity 0.9978. 30.77% complete\n",
      "hippocampus_385.nii.gz Dice 0.8996, Dice Anterior 0.8251, Dice Posterior 0.8015, Jaccard 0.8175, Sensitivity 0.8758, and Specificity 0.9983. 32.69% complete\n",
      "hippocampus_252.nii.gz Dice 0.8848, Dice Anterior 0.8875, Dice Posterior 0.8402, Jaccard 0.7934, Sensitivity 0.8442, and Specificity 0.9985. 34.62% complete\n",
      "hippocampus_350.nii.gz Dice 0.8854, Dice Anterior 0.8687, Dice Posterior 0.8742, Jaccard 0.7944, Sensitivity 0.8654, and Specificity 0.9981. 36.54% complete\n",
      "hippocampus_174.nii.gz Dice 0.9049, Dice Anterior 0.8838, Dice Posterior 0.8630, Jaccard 0.8264, Sensitivity 0.8645, and Specificity 0.9988. 38.46% complete\n",
      "hippocampus_215.nii.gz Dice 0.9063, Dice Anterior 0.9205, Dice Posterior 0.8819, Jaccard 0.8286, Sensitivity 0.8818, and Specificity 0.9985. 40.38% complete\n",
      "hippocampus_074.nii.gz Dice 0.8862, Dice Anterior 0.7783, Dice Posterior 0.8432, Jaccard 0.7957, Sensitivity 0.9153, and Specificity 0.9970. 42.31% complete\n",
      "hippocampus_224.nii.gz Dice 0.8929, Dice Anterior 0.8938, Dice Posterior 0.8790, Jaccard 0.8066, Sensitivity 0.8767, and Specificity 0.9978. 44.23% complete\n",
      "hippocampus_280.nii.gz Dice 0.8162, Dice Anterior 0.8070, Dice Posterior 0.8162, Jaccard 0.6895, Sensitivity 0.8021, and Specificity 0.9971. 46.15% complete\n",
      "hippocampus_101.nii.gz Dice 0.9066, Dice Anterior 0.8754, Dice Posterior 0.8740, Jaccard 0.8292, Sensitivity 0.8767, and Specificity 0.9986. 48.08% complete\n",
      "hippocampus_279.nii.gz Dice 0.8780, Dice Anterior 0.8194, Dice Posterior 0.8686, Jaccard 0.7826, Sensitivity 0.9051, and Specificity 0.9973. 50.00% complete\n",
      "hippocampus_109.nii.gz Dice 0.9225, Dice Anterior 0.8819, Dice Posterior 0.9094, Jaccard 0.8561, Sensitivity 0.8984, and Specificity 0.9989. 51.92% complete\n",
      "hippocampus_164.nii.gz Dice 0.9134, Dice Anterior 0.8346, Dice Posterior 0.8285, Jaccard 0.8406, Sensitivity 0.8912, and Specificity 0.9986. 53.85% complete\n",
      "hippocampus_340.nii.gz Dice 0.8926, Dice Anterior 0.8793, Dice Posterior 0.8455, Jaccard 0.8061, Sensitivity 0.8713, and Specificity 0.9981. 55.77% complete\n",
      "hippocampus_376.nii.gz Dice 0.8766, Dice Anterior 0.7857, Dice Posterior 0.8055, Jaccard 0.7803, Sensitivity 0.8648, and Specificity 0.9972. 57.69% complete\n",
      "hippocampus_233.nii.gz Dice 0.8952, Dice Anterior 0.8660, Dice Posterior 0.8571, Jaccard 0.8103, Sensitivity 0.9062, and Specificity 0.9972. 59.62% complete\n",
      "hippocampus_367.nii.gz Dice 0.8971, Dice Anterior 0.8806, Dice Posterior 0.8736, Jaccard 0.8133, Sensitivity 0.9048, and Specificity 0.9967. 61.54% complete\n",
      "hippocampus_097.nii.gz Dice 0.9059, Dice Anterior 0.9082, Dice Posterior 0.8672, Jaccard 0.8279, Sensitivity 0.8565, and Specificity 0.9994. 63.46% complete\n",
      "hippocampus_050.nii.gz Dice 0.9184, Dice Anterior 0.8990, Dice Posterior 0.9042, Jaccard 0.8492, Sensitivity 0.9199, and Specificity 0.9979. 65.38% complete\n",
      "hippocampus_363.nii.gz Dice 0.9161, Dice Anterior 0.8672, Dice Posterior 0.8550, Jaccard 0.8452, Sensitivity 0.9281, and Specificity 0.9977. 67.31% complete\n",
      "hippocampus_197.nii.gz Dice 0.9137, Dice Anterior 0.8508, Dice Posterior 0.8693, Jaccard 0.8411, Sensitivity 0.9032, and Specificity 0.9984. 69.23% complete\n",
      "hippocampus_102.nii.gz Dice 0.9069, Dice Anterior 0.8558, Dice Posterior 0.8715, Jaccard 0.8297, Sensitivity 0.8756, and Specificity 0.9985. 71.15% complete\n",
      "hippocampus_325.nii.gz Dice 0.8649, Dice Anterior 0.8357, Dice Posterior 0.8736, Jaccard 0.7620, Sensitivity 0.8683, and Specificity 0.9962. 73.08% complete\n",
      "hippocampus_251.nii.gz Dice 0.8827, Dice Anterior 0.8728, Dice Posterior 0.8166, Jaccard 0.7900, Sensitivity 0.8366, and Specificity 0.9985. 75.00% complete\n",
      "hippocampus_106.nii.gz Dice 0.8919, Dice Anterior 0.8629, Dice Posterior 0.8822, Jaccard 0.8048, Sensitivity 0.8723, and Specificity 0.9981. 76.92% complete\n",
      "hippocampus_294.nii.gz Dice 0.8916, Dice Anterior 0.8540, Dice Posterior 0.8261, Jaccard 0.8044, Sensitivity 0.8531, and Specificity 0.9986. 78.85% complete\n",
      "hippocampus_372.nii.gz Dice 0.9070, Dice Anterior 0.8923, Dice Posterior 0.8583, Jaccard 0.8297, Sensitivity 0.9291, and Specificity 0.9971. 80.77% complete\n",
      "hippocampus_222.nii.gz Dice 0.8459, Dice Anterior 0.8019, Dice Posterior 0.8134, Jaccard 0.7330, Sensitivity 0.8193, and Specificity 0.9977. 82.69% complete\n",
      "hippocampus_361.nii.gz Dice 0.8837, Dice Anterior 0.8670, Dice Posterior 0.8380, Jaccard 0.7917, Sensitivity 0.8824, and Specificity 0.9975. 84.62% complete\n",
      "hippocampus_105.nii.gz Dice 0.9067, Dice Anterior 0.8807, Dice Posterior 0.8976, Jaccard 0.8293, Sensitivity 0.8886, and Specificity 0.9983. 86.54% complete\n",
      "hippocampus_178.nii.gz Dice 0.9240, Dice Anterior 0.9071, Dice Posterior 0.9156, Jaccard 0.8587, Sensitivity 0.8909, and Specificity 0.9993. 88.46% complete\n",
      "hippocampus_234.nii.gz Dice 0.8839, Dice Anterior 0.8619, Dice Posterior 0.8704, Jaccard 0.7919, Sensitivity 0.8731, and Specificity 0.9978. 90.38% complete\n",
      "hippocampus_075.nii.gz Dice 0.9159, Dice Anterior 0.8706, Dice Posterior 0.8861, Jaccard 0.8449, Sensitivity 0.9022, and Specificity 0.9984. 92.31% complete\n",
      "hippocampus_039.nii.gz Dice 0.9156, Dice Anterior 0.9167, Dice Posterior 0.8965, Jaccard 0.8443, Sensitivity 0.9030, and Specificity 0.9981. 94.23% complete\n",
      "hippocampus_077.nii.gz Dice 0.8870, Dice Anterior 0.8451, Dice Posterior 0.8633, Jaccard 0.7969, Sensitivity 0.8265, and Specificity 0.9990. 96.15% complete\n",
      "hippocampus_189.nii.gz Dice 0.9232, Dice Anterior 0.9184, Dice Posterior 0.9176, Jaccard 0.8574, Sensitivity 0.9224, and Specificity 0.9981. 98.08% complete\n",
      "hippocampus_207.nii.gz Dice 0.9121, Dice Anterior 0.9116, Dice Posterior 0.8909, Jaccard 0.8384, Sensitivity 0.8913, and Specificity 0.9982. 100.00% complete\n",
      "\n",
      "Testing complete.\n",
      "------------------------------\n",
      "Average Dice 0.8956, Average Dice Anterior 0.8688, Average Dice Posterior 0.8633, Average Dice AP Mean 0.8661, Average Jaccard 0.8117, Average Sensitivity 0.8830, and Average Specificity 0.9979\n"
     ]
    }
   ],
   "source": [
    "weight_list = glob(f\"out/{dirname}/*\")\n",
    "weight_list.sort(reverse=True, key=os.path.getmtime)\n",
    "best_weight = weight_list[0]\n",
    "print(best_weight)\n",
    "model.load_state_dict(torch.load(best_weight))\n",
    "set_seeds(seed_factor, experiment_id)\n",
    "test_out = run_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e1643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del test; del train; del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc552951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8660525620157116"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_dice = []\n",
    "for i in test_out['volume_stats']:\n",
    "    mean_dice.append(i['dice_anterior'])\n",
    "    mean_dice.append(i['dice_posterior'])\n",
    "\n",
    "np.mean(mean_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60469b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/test_out_Train_UNet_seed1.txt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out_path = 'test/test_out_'+book_name.replace(\"ipynb\",'txt')\n",
    "test_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c36f9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_out_path, 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbc3d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# reading the data from the file\n",
    "with open(test_out_path) as f:\n",
    "    data = f.read()\n",
    "\n",
    "data_js = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d4b5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_dice': 0.8955788039180423,\n",
       " 'mean_dice_anterior': 0.8687932304178995,\n",
       " 'mean_dice_posterior': 0.8633118936135236,\n",
       " 'dice_ap_mean': 0.8660525620157116,\n",
       " 'mean_jaccard': 0.8117259549545448,\n",
       " 'mean_jaccard_anterior': 0.7697578409115746,\n",
       " 'mean_jaccard_posterior': 0.7608864530712249,\n",
       " 'mean_sensitivity': 0.8829941936590842,\n",
       " 'mean_sensitivity_anterior': 0.8601237510859008,\n",
       " 'mean_sensitivity_posterior': 0.8518153621023841,\n",
       " 'mean_specificity': 0.9979490919754727,\n",
       " 'mean_specificity_anterior': 0.9986252232556727,\n",
       " 'mean_specificity_posterior': 0.9986942574434164}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "data_js['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6beb4bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7653221469913998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_jac = []\n",
    "for i in test_out['volume_stats']:\n",
    "    mean_jac.append(i['jaccard_anterior'])\n",
    "    mean_jac.append(i['jaccard_posterior'])\n",
    "\n",
    "np.mean(mean_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb630a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
